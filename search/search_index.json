{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Conductor is a Workflow Orchestration engine that runs in the cloud. Motivation \u00b6 Conductor was built to help Netflix orchestrate microservices based process flows with the following features: \u00b6 A distributed server ecosystem, which stores workflow state information efficiently. Allow creation of process / business flows in which each individual task can be implemented by the same / different microservices. A DAG (Directed Acyclic Graph) based workflow definition. Workflow definitions are decoupled from the service implementations. Provide visibility and traceability into these process flows. Simple interface to connect workers, which execute the tasks in workflows. Workers are language agnostic, allowing each microservice to be written in the language most suited for the service. Full operational control over workflows with the ability to pause, resume, restart, retry and terminate. Allow greater reuse of existing microservices providing an easier path for onboarding. User interface to visualize, replay and search the process flows. Ability to scale to millions of concurrently running process flows. Backed by a queuing service abstracted from the clients. Be able to operate on HTTP or other transports e.g. gRPC. Event handlers to control workflows via external actions. Client implementations in Java, Python and other languages. Various configurable properties with sensible defaults to fine tune workflow and task executions like rate limiting, concurrent execution limits etc. Why not peer to peer choreography? \u00b6 With peer to peer task choreography, we found it was harder to scale with growing business needs and complexities. Pub/sub model worked for simplest of the flows, but quickly highlighted some of the issues associated with the approach: Process flows are \u201cembedded\u201d within the code of multiple application. Often, there is tight coupling and assumptions around input/output, SLAs etc, making it harder to adapt to changing needs. Almost no way to systematically answer \u201cHow much are we done with process X\u201d?","title":"Introduction"},{"location":"#motivation","text":"","title":"Motivation"},{"location":"#conductor-was-built-to-help-netflix-orchestrate-microservices-based-process-flows-with-the-following-features","text":"A distributed server ecosystem, which stores workflow state information efficiently. Allow creation of process / business flows in which each individual task can be implemented by the same / different microservices. A DAG (Directed Acyclic Graph) based workflow definition. Workflow definitions are decoupled from the service implementations. Provide visibility and traceability into these process flows. Simple interface to connect workers, which execute the tasks in workflows. Workers are language agnostic, allowing each microservice to be written in the language most suited for the service. Full operational control over workflows with the ability to pause, resume, restart, retry and terminate. Allow greater reuse of existing microservices providing an easier path for onboarding. User interface to visualize, replay and search the process flows. Ability to scale to millions of concurrently running process flows. Backed by a queuing service abstracted from the clients. Be able to operate on HTTP or other transports e.g. gRPC. Event handlers to control workflows via external actions. Client implementations in Java, Python and other languages. Various configurable properties with sensible defaults to fine tune workflow and task executions like rate limiting, concurrent execution limits etc.","title":"Conductor was built to help Netflix orchestrate microservices based process flows with the following features:"},{"location":"#why-not-peer-to-peer-choreography","text":"With peer to peer task choreography, we found it was harder to scale with growing business needs and complexities. Pub/sub model worked for simplest of the flows, but quickly highlighted some of the issues associated with the approach: Process flows are \u201cembedded\u201d within the code of multiple application. Often, there is tight coupling and assumptions around input/output, SLAs etc, making it harder to adapt to changing needs. Almost no way to systematically answer \u201cHow much are we done with process X\u201d?","title":"Why not peer to peer choreography?"},{"location":"apispec/","text":"Task & Workflow Metadata \u00b6 Endpoint Description Input GET /metadata/taskdefs Get all the task definitions n/a GET /metadata/taskdefs/{taskType} Retrieve task definition Task Name POST /metadata/taskdefs Register new task definitions List of Task Definitions PUT /metadata/taskdefs Update a task definition A Task Definition DELETE /metadata/taskdefs/{taskType} Delete a task definition Task Name GET /metadata/workflow Get all the workflow definitions n/a POST /metadata/workflow Register new workflow Workflow Definition PUT /metadata/workflow Register/Update new workflows List of Workflow Definition GET /metadata/workflow/{name}?version= Get the workflow definitions workflow name, version (optional) Start A Workflow \u00b6 With Input only \u00b6 See Start Workflow Request . Output \u00b6 Id of the workflow (GUID) With Input and Task Domains \u00b6 POST /workflow { //JSON payload for Start workflow request } Start workflow request \u00b6 JSON for start workflow request { \"name\": \"myWorkflow\", // Name of the workflow \"version\": 1, // Version \u201ccorrelationId\u201d: \u201ccorr1\u201d, // correlation Id \"priority\": 1, // Priority \"input\": { // Input map. }, \"taskToDomain\": { // Task to domain map } } Output \u00b6 Id of the workflow (GUID) Retrieve Workflows \u00b6 Endpoint Description GET /workflow/{workflowId}?includeTasks=true | false Get Workflow State by workflow Id. If includeTasks is set, then also includes all the tasks executed and scheduled. GET /workflow/running/{name} Get all the running workflows of a given type GET /workflow/running/{name}/correlated/{correlationId}?includeClosed=true | false&includeTasks=true |false Get all the running workflows filtered by correlation Id. If includeClosed is set, also includes workflows that have completed running. GET /workflow/search Search for workflows. See Below. Search for Workflows \u00b6 Conductor uses Elasticsearch for indexing workflow execution and is used by search APIs. GET /workflow/search?start=&size=&sort=&freeText=&query= Parameter Description start Page number. Defaults to 0 size Number of results to return sort Sorting. Format is: ASC:<fieldname> or DESC:<fieldname> to sort in ascending or descending order by a field freeText Elasticsearch supported query. e.g. workflowType:\"name_of_workflow\" query SQL like where clause. e.g. workflowType = 'name_of_workflow'. Optional if freeText is provided. Output \u00b6 Search result as described below: { \"totalHits\": 0, \"results\": [ { \"workflowType\": \"string\", \"version\": 0, \"workflowId\": \"string\", \"correlationId\": \"string\", \"startTime\": \"string\", \"updateTime\": \"string\", \"endTime\": \"string\", \"status\": \"RUNNING\", \"input\": \"string\", \"output\": \"string\", \"reasonForIncompletion\": \"string\", \"executionTime\": 0, \"event\": \"string\" } ] } Manage Workflows \u00b6 Endpoint Description PUT /workflow/{workflowId}/pause Pause. No further tasks will be scheduled until resumed. Currently running tasks are not paused. PUT /workflow/{workflowId}/resume Resume normal operations after a pause. POST /workflow/{workflowId}/rerun See Below. POST /workflow/{workflowId}/restart Restart workflow execution from the start. Current execution history is wiped out. POST /workflow/{workflowId}/retry Retry the last failed task. PUT /workflow/{workflowId}/skiptask/{taskReferenceName} See below. DELETE /workflow/{workflowId} Terminates the running workflow. DELETE /workflow/{workflowId}/remove Deletes the workflow from system. Use with caution. Rerun \u00b6 Re-runs a completed workflow from a specific task. POST /workflow/{workflowId}/rerun { \"reRunFromWorkflowId\": \"string\", \"workflowInput\": {}, \"reRunFromTaskId\": \"string\", \"taskInput\": {} } Skip Task \u00b6 Skips a task execution (specified as taskReferenceName parameter) in a running workflow and continues forward. Optionally updating task's input and output as specified in the payload. PUT /workflow/{workflowId}/skiptask/{taskReferenceName}?workflowId=&taskReferenceName= { \"taskInput\": {}, \"taskOutput\": {} } Manage Tasks \u00b6 Endpoint Description GET /tasks/{taskId} Get task details. GET /tasks/queue/all List the pending task sizes. GET /tasks/queue/all/verbose Same as above, includes the size per shard GET /tasks/queue/sizes?taskType=&taskType=&taskType Return the size of pending tasks for given task types Polling and Update Task \u00b6 These are critical endpoints used to poll for task and updating the task result by worker. Endpoint Description GET /tasks/poll/{taskType}?workerid=&domain= Poll for a task. workerid identifies the worker that polled for the job and domain allows the poller to poll for a task in a specific domain GET /tasks/poll/batch/{taskType}?count=&timeout=&workerid=&domain Poll for a task in a batch specified by count . This is a long poll and the connection will wait until timeout or if there is at-least 1 item available, whichever comes first. workerid identifies the worker that polled for the job and domain allows the poller to poll for a task in a specific domain POST /tasks Update the result of task execution. See the schema below. Schema for updating Task Result \u00b6 { \"workflowInstanceId\": \"Workflow Instance Id\", \"taskId\": \"ID of the task to be updated\", \"reasonForIncompletion\" : \"If failed, reason for failure\", \"callbackAfterSeconds\": 0, \"status\": \"IN_PROGRESS|FAILED|COMPLETED\", \"outputData\": { //JSON document representing Task execution output } }","title":"API Specification"},{"location":"apispec/#task-workflow-metadata","text":"Endpoint Description Input GET /metadata/taskdefs Get all the task definitions n/a GET /metadata/taskdefs/{taskType} Retrieve task definition Task Name POST /metadata/taskdefs Register new task definitions List of Task Definitions PUT /metadata/taskdefs Update a task definition A Task Definition DELETE /metadata/taskdefs/{taskType} Delete a task definition Task Name GET /metadata/workflow Get all the workflow definitions n/a POST /metadata/workflow Register new workflow Workflow Definition PUT /metadata/workflow Register/Update new workflows List of Workflow Definition GET /metadata/workflow/{name}?version= Get the workflow definitions workflow name, version (optional)","title":"Task &amp; Workflow Metadata"},{"location":"apispec/#start-a-workflow","text":"","title":"Start A Workflow"},{"location":"apispec/#with-input-only","text":"See Start Workflow Request .","title":"With Input only"},{"location":"apispec/#output","text":"Id of the workflow (GUID)","title":"Output"},{"location":"apispec/#with-input-and-task-domains","text":"POST /workflow { //JSON payload for Start workflow request }","title":"With Input and Task Domains"},{"location":"apispec/#start-workflow-request","text":"JSON for start workflow request { \"name\": \"myWorkflow\", // Name of the workflow \"version\": 1, // Version \u201ccorrelationId\u201d: \u201ccorr1\u201d, // correlation Id \"priority\": 1, // Priority \"input\": { // Input map. }, \"taskToDomain\": { // Task to domain map } }","title":"Start workflow request"},{"location":"apispec/#output_1","text":"Id of the workflow (GUID)","title":"Output"},{"location":"apispec/#retrieve-workflows","text":"Endpoint Description GET /workflow/{workflowId}?includeTasks=true | false Get Workflow State by workflow Id. If includeTasks is set, then also includes all the tasks executed and scheduled. GET /workflow/running/{name} Get all the running workflows of a given type GET /workflow/running/{name}/correlated/{correlationId}?includeClosed=true | false&includeTasks=true |false Get all the running workflows filtered by correlation Id. If includeClosed is set, also includes workflows that have completed running. GET /workflow/search Search for workflows. See Below.","title":"Retrieve Workflows"},{"location":"apispec/#search-for-workflows","text":"Conductor uses Elasticsearch for indexing workflow execution and is used by search APIs. GET /workflow/search?start=&size=&sort=&freeText=&query= Parameter Description start Page number. Defaults to 0 size Number of results to return sort Sorting. Format is: ASC:<fieldname> or DESC:<fieldname> to sort in ascending or descending order by a field freeText Elasticsearch supported query. e.g. workflowType:\"name_of_workflow\" query SQL like where clause. e.g. workflowType = 'name_of_workflow'. Optional if freeText is provided.","title":"Search for Workflows"},{"location":"apispec/#output_2","text":"Search result as described below: { \"totalHits\": 0, \"results\": [ { \"workflowType\": \"string\", \"version\": 0, \"workflowId\": \"string\", \"correlationId\": \"string\", \"startTime\": \"string\", \"updateTime\": \"string\", \"endTime\": \"string\", \"status\": \"RUNNING\", \"input\": \"string\", \"output\": \"string\", \"reasonForIncompletion\": \"string\", \"executionTime\": 0, \"event\": \"string\" } ] }","title":"Output"},{"location":"apispec/#manage-workflows","text":"Endpoint Description PUT /workflow/{workflowId}/pause Pause. No further tasks will be scheduled until resumed. Currently running tasks are not paused. PUT /workflow/{workflowId}/resume Resume normal operations after a pause. POST /workflow/{workflowId}/rerun See Below. POST /workflow/{workflowId}/restart Restart workflow execution from the start. Current execution history is wiped out. POST /workflow/{workflowId}/retry Retry the last failed task. PUT /workflow/{workflowId}/skiptask/{taskReferenceName} See below. DELETE /workflow/{workflowId} Terminates the running workflow. DELETE /workflow/{workflowId}/remove Deletes the workflow from system. Use with caution.","title":"Manage Workflows"},{"location":"apispec/#rerun","text":"Re-runs a completed workflow from a specific task. POST /workflow/{workflowId}/rerun { \"reRunFromWorkflowId\": \"string\", \"workflowInput\": {}, \"reRunFromTaskId\": \"string\", \"taskInput\": {} }","title":"Rerun"},{"location":"apispec/#skip-task","text":"Skips a task execution (specified as taskReferenceName parameter) in a running workflow and continues forward. Optionally updating task's input and output as specified in the payload. PUT /workflow/{workflowId}/skiptask/{taskReferenceName}?workflowId=&taskReferenceName= { \"taskInput\": {}, \"taskOutput\": {} }","title":"Skip Task"},{"location":"apispec/#manage-tasks","text":"Endpoint Description GET /tasks/{taskId} Get task details. GET /tasks/queue/all List the pending task sizes. GET /tasks/queue/all/verbose Same as above, includes the size per shard GET /tasks/queue/sizes?taskType=&taskType=&taskType Return the size of pending tasks for given task types","title":"Manage Tasks"},{"location":"apispec/#polling-and-update-task","text":"These are critical endpoints used to poll for task and updating the task result by worker. Endpoint Description GET /tasks/poll/{taskType}?workerid=&domain= Poll for a task. workerid identifies the worker that polled for the job and domain allows the poller to poll for a task in a specific domain GET /tasks/poll/batch/{taskType}?count=&timeout=&workerid=&domain Poll for a task in a batch specified by count . This is a long poll and the connection will wait until timeout or if there is at-least 1 item available, whichever comes first. workerid identifies the worker that polled for the job and domain allows the poller to poll for a task in a specific domain POST /tasks Update the result of task execution. See the schema below.","title":"Polling and Update Task"},{"location":"apispec/#schema-for-updating-task-result","text":"{ \"workflowInstanceId\": \"Workflow Instance Id\", \"taskId\": \"ID of the task to be updated\", \"reasonForIncompletion\" : \"If failed, reason for failure\", \"callbackAfterSeconds\": 0, \"status\": \"IN_PROGRESS|FAILED|COMPLETED\", \"outputData\": { //JSON document representing Task execution output } }","title":"Schema for updating Task Result"},{"location":"architecture/","text":"High Level Architecture \u00b6 The API and storage layers are pluggable and provide ability to work with different backends and queue service providers. Installing and Running \u00b6 Running in production For a detailed configuration guide on installing and running Conductor server in production visit Conductor Server documentation. Running In-Memory Server \u00b6 Follow the steps below to quickly bring up a local Conductor instance backed by an in-memory database with a simple kitchen sink workflow that demonstrate all the capabilities of Conductor. !!!warning: In-Memory server is meant for a quick demonstration purposes and does not store any data on disk. All the data is lost once the server dies. Checkout the source from GitHub \u00b6 git clone git@github.com:Netflix/conductor.git Start Local Server \u00b6 NOTE for Mac users : If you are using a new Mac with an Apple Silicon Chip, you must make a small change to conductor/grpc/build.gradle - adding \"osx-x86_64\" to two lines: protobuf { protoc { artifact = \"com.google.protobuf:protoc:${revProtoBuf}:osx-x86_64\" } plugins { grpc { artifact = \"io.grpc:protoc-gen-grpc-java:${revGrpc}:osx-x86_64\" } } ... } The server is in the directory conductor/server . To start it execute the following command in the root of the project. ./gradlew bootRun # wait for the server to come online Swagger APIs can be accessed at http://localhost:8080/swagger-ui.html Start UI Server \u00b6 The UI Server is in the directory conductor/ui . To run it you need to have Node 14 (or greater) and Yarn installed. In a terminal other than the one running the Conductor server: cd ui yarn install yarn run start If you get an error message ReferenceError: primordials is not defined , you need to use an earlier version of Node (pre-12). See this issue for more details . Or Start all the services using docker-compose \u00b6 Using compose (with Dynomite): shell docker-compose -f docker-compose.yaml -f docker-compose-dynomite.yaml up Using compose (with Postgres): shell docker-compose -f docker-compose.yaml -f docker-compose-postgres.yaml up Assuming that you started Conductor locally (directly, or with Docker), launch the UI at http://localhost:5000/ . Note The server will load a sample kitchensink workflow definition by default. See here for details. Runtime Model \u00b6 Conductor follows RPC based communication model where workers are running on a separate machine from the server. Workers communicate with server over HTTP based endpoints and employs polling model for managing work queues. Notes Workers are remote systems that communicate over HTTP with the conductor servers. Task Queues are used to schedule tasks for workers. We use dyno-queues internally but it can easily be swapped with SQS or similar pub-sub mechanism. conductor-redis-persistence module uses Dynomite for storing the state and metadata along with Elasticsearch for indexing backend. See section under extending backend for implementing support for different databases for storage and indexing. High Level Steps \u00b6 Steps required for a new workflow to be registered and get executed: Define task definitions used by the workflow. Create the workflow definition Create task worker(s) that polls for scheduled tasks at regular interval The Beginner lab has a good walkthrough of steps 1-3. Trigger Workflow Execution POST /workflow/{name} { ... //json payload as workflow input } Polling for a task GET /tasks/poll/batch/{taskType} Update task status POST /tasks { \"outputData\": { \"encodeResult\":\"success\", \"location\": \"http://cdn.example.com/file/location.png\" //any task specific output }, \"status\": \"COMPLETED\" }","title":"Architecture"},{"location":"architecture/#high-level-architecture","text":"The API and storage layers are pluggable and provide ability to work with different backends and queue service providers.","title":"High Level Architecture"},{"location":"architecture/#installing-and-running","text":"Running in production For a detailed configuration guide on installing and running Conductor server in production visit Conductor Server documentation.","title":"Installing and Running"},{"location":"architecture/#running-in-memory-server","text":"Follow the steps below to quickly bring up a local Conductor instance backed by an in-memory database with a simple kitchen sink workflow that demonstrate all the capabilities of Conductor. !!!warning: In-Memory server is meant for a quick demonstration purposes and does not store any data on disk. All the data is lost once the server dies.","title":"Running In-Memory Server"},{"location":"architecture/#checkout-the-source-from-github","text":"git clone git@github.com:Netflix/conductor.git","title":"Checkout the source from GitHub"},{"location":"architecture/#start-local-server","text":"NOTE for Mac users : If you are using a new Mac with an Apple Silicon Chip, you must make a small change to conductor/grpc/build.gradle - adding \"osx-x86_64\" to two lines: protobuf { protoc { artifact = \"com.google.protobuf:protoc:${revProtoBuf}:osx-x86_64\" } plugins { grpc { artifact = \"io.grpc:protoc-gen-grpc-java:${revGrpc}:osx-x86_64\" } } ... } The server is in the directory conductor/server . To start it execute the following command in the root of the project. ./gradlew bootRun # wait for the server to come online Swagger APIs can be accessed at http://localhost:8080/swagger-ui.html","title":"Start Local Server"},{"location":"architecture/#start-ui-server","text":"The UI Server is in the directory conductor/ui . To run it you need to have Node 14 (or greater) and Yarn installed. In a terminal other than the one running the Conductor server: cd ui yarn install yarn run start If you get an error message ReferenceError: primordials is not defined , you need to use an earlier version of Node (pre-12). See this issue for more details .","title":"Start UI Server"},{"location":"architecture/#or-start-all-the-services-using-docker-compose","text":"Using compose (with Dynomite): shell docker-compose -f docker-compose.yaml -f docker-compose-dynomite.yaml up Using compose (with Postgres): shell docker-compose -f docker-compose.yaml -f docker-compose-postgres.yaml up Assuming that you started Conductor locally (directly, or with Docker), launch the UI at http://localhost:5000/ . Note The server will load a sample kitchensink workflow definition by default. See here for details.","title":"Or Start all the services using docker-compose"},{"location":"architecture/#runtime-model","text":"Conductor follows RPC based communication model where workers are running on a separate machine from the server. Workers communicate with server over HTTP based endpoints and employs polling model for managing work queues. Notes Workers are remote systems that communicate over HTTP with the conductor servers. Task Queues are used to schedule tasks for workers. We use dyno-queues internally but it can easily be swapped with SQS or similar pub-sub mechanism. conductor-redis-persistence module uses Dynomite for storing the state and metadata along with Elasticsearch for indexing backend. See section under extending backend for implementing support for different databases for storage and indexing.","title":"Runtime Model"},{"location":"architecture/#high-level-steps","text":"Steps required for a new workflow to be registered and get executed: Define task definitions used by the workflow. Create the workflow definition Create task worker(s) that polls for scheduled tasks at regular interval The Beginner lab has a good walkthrough of steps 1-3. Trigger Workflow Execution POST /workflow/{name} { ... //json payload as workflow input } Polling for a task GET /tasks/poll/batch/{taskType} Update task status POST /tasks { \"outputData\": { \"encodeResult\":\"success\", \"location\": \"http://cdn.example.com/file/location.png\" //any task specific output }, \"status\": \"COMPLETED\" }","title":"High Level Steps"},{"location":"bestpractices/","text":"Response Timeout \u00b6 Configure the responseTimeoutSeconds of each task to be > 0. Should be less than or equal to timeoutSeconds. Payload sizes \u00b6 Configure your workflows such that conductor is not used as a persistence store. Ensure that the output data in the task result set in your worker is used by your workflow for execution. If the values in the output payloads are not used by subsequent tasks in your workflow, this data should not be sent back to conductor in the task result. In cases where the output data of your task is used within subsequent tasks in your workflow but is substantially large (> 100KB), consider uploading this data to an object store (S3 or similar) and set the location to the object in your task output. The subsequent tasks can then download this data from the given location and use it during execution.","title":"Best Practices"},{"location":"bestpractices/#response-timeout","text":"Configure the responseTimeoutSeconds of each task to be > 0. Should be less than or equal to timeoutSeconds.","title":"Response Timeout"},{"location":"bestpractices/#payload-sizes","text":"Configure your workflows such that conductor is not used as a persistence store. Ensure that the output data in the task result set in your worker is used by your workflow for execution. If the values in the output payloads are not used by subsequent tasks in your workflow, this data should not be sent back to conductor in the task result. In cases where the output data of your task is used within subsequent tasks in your workflow but is substantially large (> 100KB), consider uploading this data to an object store (S3 or similar) and set the location to the object in your task output. The subsequent tasks can then download this data from the given location and use it during execution.","title":"Payload sizes"},{"location":"extend/","text":"Backend \u00b6 Conductor provides a pluggable backend. The current implementation uses Dynomite. There are 4 interfaces that need to be implemented for each backend: //Store for workflow and task definitions com.netflix.conductor.dao.MetadataDAO //Store for workflow executions com.netflix.conductor.dao.ExecutionDAO //Index for workflow executions com.netflix.conductor.dao.IndexDAO //Queue provider for tasks com.netflix.conductor.dao.QueueDAO It is possible to mix and match different implementations for each of these. For example, SQS for queueing and a relational store for others. System Tasks \u00b6 To create system tasks follow the steps below: Extend com.netflix.conductor.core.execution.tasks.WorkflowSystemTask Instantiate the new class as part of the startup (eager singleton) Implement the TaskMapper interface Add this implementation to the map identified by TaskMappers External Payload Storage \u00b6 To configure conductor to externalize the storage of large payloads: Implement the ExternalPayloadStorage interface . Add the storage option to the enum here . Set this JVM system property workflow.external.payload.storage to the value of the enum element added above. Add a binding similar to this . Workflow Status Listener \u00b6 To provide a notification mechanism upon completion/termination of workflows: Implement the WorkflowStatusListener interface This can be configured to plugin custom notification/eventing upon workflows reaching a terminal state. Locking Service \u00b6 By default, Conductor Server module loads Zookeeper lock module. If you'd like to provide your own locking implementation module, for eg., with Dynomite and Redlock: Implement Lock interface. Add a binding similar to this Enable locking service: conductor.app.workflowExecutionLockEnabled: true","title":"Extending Conductor"},{"location":"extend/#backend","text":"Conductor provides a pluggable backend. The current implementation uses Dynomite. There are 4 interfaces that need to be implemented for each backend: //Store for workflow and task definitions com.netflix.conductor.dao.MetadataDAO //Store for workflow executions com.netflix.conductor.dao.ExecutionDAO //Index for workflow executions com.netflix.conductor.dao.IndexDAO //Queue provider for tasks com.netflix.conductor.dao.QueueDAO It is possible to mix and match different implementations for each of these. For example, SQS for queueing and a relational store for others.","title":"Backend"},{"location":"extend/#system-tasks","text":"To create system tasks follow the steps below: Extend com.netflix.conductor.core.execution.tasks.WorkflowSystemTask Instantiate the new class as part of the startup (eager singleton) Implement the TaskMapper interface Add this implementation to the map identified by TaskMappers","title":"System Tasks"},{"location":"extend/#external-payload-storage","text":"To configure conductor to externalize the storage of large payloads: Implement the ExternalPayloadStorage interface . Add the storage option to the enum here . Set this JVM system property workflow.external.payload.storage to the value of the enum element added above. Add a binding similar to this .","title":"External Payload Storage"},{"location":"extend/#workflow-status-listener","text":"To provide a notification mechanism upon completion/termination of workflows: Implement the WorkflowStatusListener interface This can be configured to plugin custom notification/eventing upon workflows reaching a terminal state.","title":"Workflow Status Listener"},{"location":"extend/#locking-service","text":"By default, Conductor Server module loads Zookeeper lock module. If you'd like to provide your own locking implementation module, for eg., with Dynomite and Redlock: Implement Lock interface. Add a binding similar to this Enable locking service: conductor.app.workflowExecutionLockEnabled: true","title":"Locking Service"},{"location":"externalpayloadstorage/","text":"Warning The external payload storage is currently only implemented to be used to by the Java client. Client libraries in other languages need to be modified to enable this. Contributions are welcomed. Context \u00b6 Conductor can be configured to enforce barriers on the size of workflow and task payloads for both input and output. These barriers can be used as safeguards to prevent the usage of conductor as a data persistence system and to reduce the pressure on its datastore. Barriers \u00b6 Conductor typically applies two kinds of barriers: Soft Barrier Hard Barrier Soft Barrier \u00b6 The soft barrier is used to alleviate pressure on the conductor datastore. In some special workflow use-cases, the size of the payload is warranted enough to be stored as part of the workflow execution. In such cases, conductor externalizes the storage of such payloads to S3 and uploads/downloads to/from S3 as needed during the execution. This process is completely transparent to the user/worker process. Hard Barrier \u00b6 The hard barriers are enforced to safeguard the conductor backend from the pressure of having to persist and deal with voluminous data which is not essential for workflow execution. In such cases, conductor will reject such payloads and will terminate/fail the workflow execution with the reasonForIncompletion set to an appropriate error message detailing the payload size. Usage \u00b6 Barriers setup \u00b6 Set the following properties to the desired values in the JVM system properties: Property Description default value conductor.workflow.input.payload.threshold.kb Soft barrier for workflow input payload in KB 5120 conductor.max.workflow.input.payload.threshold.kb Hard barrier for workflow input payload in KB 10240 conductor.workflow.output.payload.threshold.kb Soft barrier for workflow output payload in KB 5120 conductor.max.workflow.output.payload.threshold.kb Hard barrier for workflow output payload in KB 10240 conductor.task.input.payload.threshold.kb Soft barrier for task input payload in KB 3072 conductor.max.task.input.payload.threshold.kb Hard barrier for task input payload in KB 10240 conductor.task.output.payload.threshold.kb Soft barrier for task output payload in KB 3072 conductor.max.task.output.payload.threshold.kb Hard barrier for task output payload in KB 10240 Amazon S3 \u00b6 Conductor provides an implementation of Amazon S3 used to externalize large payload storage. Set the following property in the JVM system properties: workflow.external.payload.storage=S3 Note This implementation assumes that S3 access is configured on the instance. Set the following properties to the desired values in the JVM system properties: Property Description default value workflow.external.payload.storage.s3.bucket S3 bucket where the payloads will be stored workflow.external.payload.storage.s3.signedurlexpirationseconds The expiration time in seconds of the signed url for the payload 5 The payloads will be stored in the bucket configured above in a UUID.json file at locations determined by the type of the payload. See here for information about how the object key is determined. Azure Blob Storage \u00b6 ProductLive provides an implementation of Azure Blob Storage used to externalize large payload storage. To build conductor with azure blob feature read the README.md in azureblob-storage module Note This implementation assumes that you have an Azure Blob Storage account's connection string or SAS Token . If you want signed url to expired you must specify a Connection String. Set the following properties to the desired values in the JVM system properties: Property Description default value workflow.external.payload.storage.azure_blob.connection_string Azure Blob Storage connection string. Required to sign Url. workflow.external.payload.storage.azure_blob.endpoint Azure Blob Storage endpoint. Optional if connection_string is set. workflow.external.payload.storage.azure_blob.sas_token Azure Blob Storage SAS Token. Must have permissions Read and Write on Resource Object on Service Blob . Optional if connection_string is set. workflow.external.payload.storage.azure_blob.container_name Azure Blob Storage container where the payloads will be stored conductor-payloads workflow.external.payload.storage.azure_blob.signedurlexpirationseconds The expiration time in seconds of the signed url for the payload 5 workflow.external.payload.storage.azure_blob.workflow_input_path Path prefix where workflows input will be stored with an random UUID filename workflow/input/ workflow.external.payload.storage.azure_blob.workflow_output_path Path prefix where workflows output will be stored with an random UUID filename workflow/output/ workflow.external.payload.storage.azure_blob.task_input_path Path prefix where tasks input will be stored with an random UUID filename task/input/ workflow.external.payload.storage.azure_blob.task_output_path Path prefix where tasks output will be stored with an random UUID filename task/output/ The payloads will be stored as done in Amazon S3 . PostgreSQL Storage \u00b6 Frinx provides an implementation of PostgreSQL Storage used to externalize large payload storage. Note This implementation assumes that you have an PostgreSQL database server with all required credentials . Set the following properties to your application.properties: Property Description default value conductor.external-payload-storage.postgres.conductor-url URL, that can be used to pull the json configurations, that will be downloaded from PostgreSQL to the conductor server. For example: for local development it is http://localhost:8080 \"\" conductor.external-payload-storage.postgres.url PostgreSQL database connection URL. Required to connect to database. conductor.external-payload-storage.postgres.username Username for connecting to PostgreSQL database. Required to connect to database. conductor.external-payload-storage.postgres.password Password for connecting to PostgreSQL database. Required to connect to database. conductor.external-payload-storage.postgres.table-name The PostgreSQL schema and table name where the payloads will be stored external.external_payload conductor.external-payload-storage.postgres.max-data-rows Maximum count of data rows in PostgreSQL database. After overcoming this limit, the oldest data will be deleted. Long.MAX_VALUE (9223372036854775807L) conductor.external-payload-storage.postgres.max-data-days Maximum count of days of data age in PostgreSQL database. After overcoming limit, the oldest data will be deleted. 0 conductor.external-payload-storage.postgres.max-data-months Maximum count of months of data age in PostgreSQL database. After overcoming limit, the oldest data will be deleted. 0 conductor.external-payload-storage.postgres.max-data-years Maximum count of years of data age in PostgreSQL database. After overcoming limit, the oldest data will be deleted. 1 The maximum date age for fields in the database will be: years + months + days The payloads will be stored in PostgreSQL database with key (externalPayloadPath) UUID.json and you can generate URI for this data using external-postgres-payload-resource rest controller. To make this URI work correctly, you must correctly set the conductor-url property.","title":"External Payload Storage"},{"location":"externalpayloadstorage/#context","text":"Conductor can be configured to enforce barriers on the size of workflow and task payloads for both input and output. These barriers can be used as safeguards to prevent the usage of conductor as a data persistence system and to reduce the pressure on its datastore.","title":"Context"},{"location":"externalpayloadstorage/#barriers","text":"Conductor typically applies two kinds of barriers: Soft Barrier Hard Barrier","title":"Barriers"},{"location":"externalpayloadstorage/#soft-barrier","text":"The soft barrier is used to alleviate pressure on the conductor datastore. In some special workflow use-cases, the size of the payload is warranted enough to be stored as part of the workflow execution. In such cases, conductor externalizes the storage of such payloads to S3 and uploads/downloads to/from S3 as needed during the execution. This process is completely transparent to the user/worker process.","title":"Soft Barrier"},{"location":"externalpayloadstorage/#hard-barrier","text":"The hard barriers are enforced to safeguard the conductor backend from the pressure of having to persist and deal with voluminous data which is not essential for workflow execution. In such cases, conductor will reject such payloads and will terminate/fail the workflow execution with the reasonForIncompletion set to an appropriate error message detailing the payload size.","title":"Hard Barrier"},{"location":"externalpayloadstorage/#usage","text":"","title":"Usage"},{"location":"externalpayloadstorage/#barriers-setup","text":"Set the following properties to the desired values in the JVM system properties: Property Description default value conductor.workflow.input.payload.threshold.kb Soft barrier for workflow input payload in KB 5120 conductor.max.workflow.input.payload.threshold.kb Hard barrier for workflow input payload in KB 10240 conductor.workflow.output.payload.threshold.kb Soft barrier for workflow output payload in KB 5120 conductor.max.workflow.output.payload.threshold.kb Hard barrier for workflow output payload in KB 10240 conductor.task.input.payload.threshold.kb Soft barrier for task input payload in KB 3072 conductor.max.task.input.payload.threshold.kb Hard barrier for task input payload in KB 10240 conductor.task.output.payload.threshold.kb Soft barrier for task output payload in KB 3072 conductor.max.task.output.payload.threshold.kb Hard barrier for task output payload in KB 10240","title":"Barriers setup"},{"location":"externalpayloadstorage/#amazon-s3","text":"Conductor provides an implementation of Amazon S3 used to externalize large payload storage. Set the following property in the JVM system properties: workflow.external.payload.storage=S3 Note This implementation assumes that S3 access is configured on the instance. Set the following properties to the desired values in the JVM system properties: Property Description default value workflow.external.payload.storage.s3.bucket S3 bucket where the payloads will be stored workflow.external.payload.storage.s3.signedurlexpirationseconds The expiration time in seconds of the signed url for the payload 5 The payloads will be stored in the bucket configured above in a UUID.json file at locations determined by the type of the payload. See here for information about how the object key is determined.","title":"Amazon S3"},{"location":"externalpayloadstorage/#azure-blob-storage","text":"ProductLive provides an implementation of Azure Blob Storage used to externalize large payload storage. To build conductor with azure blob feature read the README.md in azureblob-storage module Note This implementation assumes that you have an Azure Blob Storage account's connection string or SAS Token . If you want signed url to expired you must specify a Connection String. Set the following properties to the desired values in the JVM system properties: Property Description default value workflow.external.payload.storage.azure_blob.connection_string Azure Blob Storage connection string. Required to sign Url. workflow.external.payload.storage.azure_blob.endpoint Azure Blob Storage endpoint. Optional if connection_string is set. workflow.external.payload.storage.azure_blob.sas_token Azure Blob Storage SAS Token. Must have permissions Read and Write on Resource Object on Service Blob . Optional if connection_string is set. workflow.external.payload.storage.azure_blob.container_name Azure Blob Storage container where the payloads will be stored conductor-payloads workflow.external.payload.storage.azure_blob.signedurlexpirationseconds The expiration time in seconds of the signed url for the payload 5 workflow.external.payload.storage.azure_blob.workflow_input_path Path prefix where workflows input will be stored with an random UUID filename workflow/input/ workflow.external.payload.storage.azure_blob.workflow_output_path Path prefix where workflows output will be stored with an random UUID filename workflow/output/ workflow.external.payload.storage.azure_blob.task_input_path Path prefix where tasks input will be stored with an random UUID filename task/input/ workflow.external.payload.storage.azure_blob.task_output_path Path prefix where tasks output will be stored with an random UUID filename task/output/ The payloads will be stored as done in Amazon S3 .","title":"Azure Blob Storage"},{"location":"externalpayloadstorage/#postgresql-storage","text":"Frinx provides an implementation of PostgreSQL Storage used to externalize large payload storage. Note This implementation assumes that you have an PostgreSQL database server with all required credentials . Set the following properties to your application.properties: Property Description default value conductor.external-payload-storage.postgres.conductor-url URL, that can be used to pull the json configurations, that will be downloaded from PostgreSQL to the conductor server. For example: for local development it is http://localhost:8080 \"\" conductor.external-payload-storage.postgres.url PostgreSQL database connection URL. Required to connect to database. conductor.external-payload-storage.postgres.username Username for connecting to PostgreSQL database. Required to connect to database. conductor.external-payload-storage.postgres.password Password for connecting to PostgreSQL database. Required to connect to database. conductor.external-payload-storage.postgres.table-name The PostgreSQL schema and table name where the payloads will be stored external.external_payload conductor.external-payload-storage.postgres.max-data-rows Maximum count of data rows in PostgreSQL database. After overcoming this limit, the oldest data will be deleted. Long.MAX_VALUE (9223372036854775807L) conductor.external-payload-storage.postgres.max-data-days Maximum count of days of data age in PostgreSQL database. After overcoming limit, the oldest data will be deleted. 0 conductor.external-payload-storage.postgres.max-data-months Maximum count of months of data age in PostgreSQL database. After overcoming limit, the oldest data will be deleted. 0 conductor.external-payload-storage.postgres.max-data-years Maximum count of years of data age in PostgreSQL database. After overcoming limit, the oldest data will be deleted. 1 The maximum date age for fields in the database will be: years + months + days The payloads will be stored in PostgreSQL database with key (externalPayloadPath) UUID.json and you can generate URI for this data using external-postgres-payload-resource rest controller. To make this URI work correctly, you must correctly set the conductor-url property.","title":"PostgreSQL Storage"},{"location":"faq/","text":"Frequently asked Questions \u00b6 How do you schedule a task to be put in the queue after some time (e.g. 1 hour, 1 day etc.) \u00b6 After polling for the task update the status of the task to IN_PROGRESS and set the callbackAfterSeconds value to the desired time. The task will remain in the queue until the specified second before worker polling for it will receive it again. If there is a timeout set for the task, and the callbackAfterSeconds exceeds the timeout value, it will result in task being TIMED_OUT. How long can a workflow be in running state? Can I have a workflow that keeps running for days or months? \u00b6 Yes. As long as the timeouts on the tasks are set to handle long running workflows, it will stay in running state. My workflow fails to start with missing task error \u00b6 Ensure all the tasks are registered via /metadata/taskdefs APIs. Add any missing task definition (as reported in the error) and try again. Where does my worker run? How does conductor run my tasks? \u00b6 Conductor does not run the workers. When a task is scheduled, it is put into the queue maintained by Conductor. Workers are required to poll for tasks using /tasks/poll API at periodic interval, execute the business logic for the task and report back the results using POST /tasks API call. Conductor, however will run system tasks on the Conductor server. How can I schedule workflows to run at a specific time? \u00b6 Netflix Conductor itself does not provide any scheduling mechanism. But there is a community project Schedule Conductor Workflows which provides workflow scheduling capability as a pluggable module as well as workflow server. Other way is you can use any of the available scheduling systems to make REST calls to Conductor to start a workflow. Alternatively, publish a message to a supported eventing system like SQS to trigger a workflow. More details about eventing . How do I setup Dynomite cluster? \u00b6 Visit Dynomite's Github page to find details on setup and support mechanism. Can I use conductor with Ruby / Go / Python? \u00b6 Yes. Workers can be written any language as long as they can poll and update the task results via HTTP endpoints. Conductor provides frameworks for Java and Python to simplify the task of polling and updating the status back to Conductor server. Note: Python and Go clients have been contributed by the community. How can I get help with Dynomite? \u00b6 Visit Dynomite's Github page to find details on setup and support mechanism. My workflow is running and the task is SCHEDULED but it is not being processed. \u00b6 Make sure that the worker is actively polling for this task. Navigate to the Task Queues tab on the Conductor UI and select your task name in the search box. Ensure that Last Poll Time for this task is current. In Conductor 3.x, conductor.redis.availabilityZone defaults to us-east-1c . Ensure that this matches where your workers are, and that it also maytches conductor.redis.hosts . How do I configure a notification when my workflow completes or fails? \u00b6 When a workflow fails, you can configure a \"failure workflow\" to run using the failureWorkflow parameter. By default, three parameters are passed: reason workflowId: use this to pull the details of the failed workflow. failureStatus You can also use the Workflow Status Listener: Set the workflowStatusListenerEnabled field in your workflow definition to true which enables notifications . Add a custom implementation of the Workflow Status Listener. Refer this . This notification can be implemented in such a way as to either send a notification to an external system or to send an event on the conductor queue to complete/fail another task in another workflow as described here . Refer to this documentation to extend conductor to send out events/notifications upon workflow completion/failure. I want my worker to stop polling and executing tasks when the process is being terminated. (Java client) \u00b6 In a PreDestroy block within your application, call the shutdown() method on the TaskRunnerConfigurer instance that you have created to facilitate a graceful shutdown of your worker in case the process is being terminated. Can I exit early from a task without executing the configured automatic retries in the task definition? \u00b6 Set the status to FAILED_WITH_TERMINAL_ERROR in the TaskResult object within your worker. This would mark the task as FAILED and fail the workflow without retrying the task as a fail-fast mechanism.","title":"FAQ"},{"location":"faq/#frequently-asked-questions","text":"","title":"Frequently asked Questions"},{"location":"faq/#how-do-you-schedule-a-task-to-be-put-in-the-queue-after-some-time-eg-1-hour-1-day-etc","text":"After polling for the task update the status of the task to IN_PROGRESS and set the callbackAfterSeconds value to the desired time. The task will remain in the queue until the specified second before worker polling for it will receive it again. If there is a timeout set for the task, and the callbackAfterSeconds exceeds the timeout value, it will result in task being TIMED_OUT.","title":"How do you schedule a task to be put in the queue after some time (e.g. 1 hour, 1 day etc.)"},{"location":"faq/#how-long-can-a-workflow-be-in-running-state-can-i-have-a-workflow-that-keeps-running-for-days-or-months","text":"Yes. As long as the timeouts on the tasks are set to handle long running workflows, it will stay in running state.","title":"How long can a workflow be in running state?  Can I have a workflow that keeps running for days or months?"},{"location":"faq/#my-workflow-fails-to-start-with-missing-task-error","text":"Ensure all the tasks are registered via /metadata/taskdefs APIs. Add any missing task definition (as reported in the error) and try again.","title":"My workflow fails to start with missing task error"},{"location":"faq/#where-does-my-worker-run-how-does-conductor-run-my-tasks","text":"Conductor does not run the workers. When a task is scheduled, it is put into the queue maintained by Conductor. Workers are required to poll for tasks using /tasks/poll API at periodic interval, execute the business logic for the task and report back the results using POST /tasks API call. Conductor, however will run system tasks on the Conductor server.","title":"Where does my worker run?  How does conductor run my tasks?"},{"location":"faq/#how-can-i-schedule-workflows-to-run-at-a-specific-time","text":"Netflix Conductor itself does not provide any scheduling mechanism. But there is a community project Schedule Conductor Workflows which provides workflow scheduling capability as a pluggable module as well as workflow server. Other way is you can use any of the available scheduling systems to make REST calls to Conductor to start a workflow. Alternatively, publish a message to a supported eventing system like SQS to trigger a workflow. More details about eventing .","title":"How can I schedule workflows to run at a specific time?"},{"location":"faq/#how-do-i-setup-dynomite-cluster","text":"Visit Dynomite's Github page to find details on setup and support mechanism.","title":"How do I setup Dynomite cluster?"},{"location":"faq/#can-i-use-conductor-with-ruby-go-python","text":"Yes. Workers can be written any language as long as they can poll and update the task results via HTTP endpoints. Conductor provides frameworks for Java and Python to simplify the task of polling and updating the status back to Conductor server. Note: Python and Go clients have been contributed by the community.","title":"Can I use conductor with Ruby / Go / Python?"},{"location":"faq/#how-can-i-get-help-with-dynomite","text":"Visit Dynomite's Github page to find details on setup and support mechanism.","title":"How can I get help with Dynomite?"},{"location":"faq/#my-workflow-is-running-and-the-task-is-scheduled-but-it-is-not-being-processed","text":"Make sure that the worker is actively polling for this task. Navigate to the Task Queues tab on the Conductor UI and select your task name in the search box. Ensure that Last Poll Time for this task is current. In Conductor 3.x, conductor.redis.availabilityZone defaults to us-east-1c . Ensure that this matches where your workers are, and that it also maytches conductor.redis.hosts .","title":"My workflow is running and the task is SCHEDULED but it is not being processed."},{"location":"faq/#how-do-i-configure-a-notification-when-my-workflow-completes-or-fails","text":"When a workflow fails, you can configure a \"failure workflow\" to run using the failureWorkflow parameter. By default, three parameters are passed: reason workflowId: use this to pull the details of the failed workflow. failureStatus You can also use the Workflow Status Listener: Set the workflowStatusListenerEnabled field in your workflow definition to true which enables notifications . Add a custom implementation of the Workflow Status Listener. Refer this . This notification can be implemented in such a way as to either send a notification to an external system or to send an event on the conductor queue to complete/fail another task in another workflow as described here . Refer to this documentation to extend conductor to send out events/notifications upon workflow completion/failure.","title":"How do I configure a notification when my workflow completes or fails?"},{"location":"faq/#i-want-my-worker-to-stop-polling-and-executing-tasks-when-the-process-is-being-terminated-java-client","text":"In a PreDestroy block within your application, call the shutdown() method on the TaskRunnerConfigurer instance that you have created to facilitate a graceful shutdown of your worker in case the process is being terminated.","title":"I want my worker to stop polling and executing tasks when the process is being terminated. (Java client)"},{"location":"faq/#can-i-exit-early-from-a-task-without-executing-the-configured-automatic-retries-in-the-task-definition","text":"Set the status to FAILED_WITH_TERMINAL_ERROR in the TaskResult object within your worker. This would mark the task as FAILED and fail the workflow without retrying the task as a fail-fast mechanism.","title":"Can I exit early from a task without executing the configured automatic retries in the task definition?"},{"location":"license/","text":"Copyright 2022 Netflix, Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"running-locally-docker/","text":"Running via Docker Compose \u00b6 In this article we will explore how you can set up Netflix Conductor on your local machine using Docker compose. The docker compose will bring up the following: 1. Conductor API Server 2. Conductor UI 3. Elasticsearch for searching workflows Prerequisites \u00b6 Docker: https://docs.docker.com/get-docker/ Recommended host with CPU and RAM to be able to run multiple docker containers (at-least 16GB RAM) Steps \u00b6 1. Clone the Conductor Code \u00b6 $ git clone https://github.com/Netflix/conductor.git 2. Build the Docker Compose \u00b6 $ cd conductor conductor $ cd docker docker $ docker-compose build Note: Conductor supplies multiple docker compose templates that can be used with different configurations: \u00b6 File Containers docker-compose.yaml 1. In Memory Conductor Server 2. Elasticsearch 3. UI docker-compose-dynomite.yaml 1. In Memory Conductor Server 2. Elasticsearch 3. UI 4. Dynomite Redis for persistence docker-compose-postgres.yaml 1. In Memory Conductor Server 2. Elasticsearch 3. UI 4. Postgres persistence docker-compose-prometheus.yaml Brings up Prometheus server 3. Run Docker Compose \u00b6 docker $ docker-compose up Once up and running, you will see the following in your Docker dashboard: Elasticsearch Conductor UI Conductor Server You can access all three on your browser to verify that it is running correctly: Conductor Server URL: http://localhost:8080/swagger-ui/index.html?configUrl=/api-docs/swagger-config Conductor UI URL: http://localhost:5000/ Potential problems \u00b6 Not enough memory You will need at least 16 GB of memory to run everything. You can modify the docker compose to skip using Elasticsearch if you have no option to run this with your memory options. To disable Elasticsearch using Docker Compose - follow the steps listed here: TODO LINK Elasticsearch fails to come up in arm64 based CPU machines As of writing this article, Conductor relies on 6.8.x version of Elasticsearch. This version doesn't have an arm64 based Docker image. You will need to use Elasticsearch 7.x which requires a bit of customization to get up and running Elasticsearch remains in Yellow health When you run Elasticsearch, sometimes the health remains in Yellow state. Conductor server by default requires Green state to run when indexing is enabled. To work around this, you can use the following property: conductor.elasticsearch.clusteHealthColor=yellow Reference: Issue 2262","title":"Installing via Docker Compose"},{"location":"running-locally-docker/#running-via-docker-compose","text":"In this article we will explore how you can set up Netflix Conductor on your local machine using Docker compose. The docker compose will bring up the following: 1. Conductor API Server 2. Conductor UI 3. Elasticsearch for searching workflows","title":"Running via Docker Compose"},{"location":"running-locally-docker/#prerequisites","text":"Docker: https://docs.docker.com/get-docker/ Recommended host with CPU and RAM to be able to run multiple docker containers (at-least 16GB RAM)","title":"Prerequisites"},{"location":"running-locally-docker/#steps","text":"","title":"Steps"},{"location":"running-locally-docker/#1-clone-the-conductor-code","text":"$ git clone https://github.com/Netflix/conductor.git","title":"1. Clone the Conductor Code"},{"location":"running-locally-docker/#2-build-the-docker-compose","text":"$ cd conductor conductor $ cd docker docker $ docker-compose build","title":"2. Build the Docker Compose"},{"location":"running-locally-docker/#note-conductor-supplies-multiple-docker-compose-templates-that-can-be-used-with-different-configurations","text":"File Containers docker-compose.yaml 1. In Memory Conductor Server 2. Elasticsearch 3. UI docker-compose-dynomite.yaml 1. In Memory Conductor Server 2. Elasticsearch 3. UI 4. Dynomite Redis for persistence docker-compose-postgres.yaml 1. In Memory Conductor Server 2. Elasticsearch 3. UI 4. Postgres persistence docker-compose-prometheus.yaml Brings up Prometheus server","title":"Note: Conductor supplies multiple docker compose templates that can be used with different configurations:"},{"location":"running-locally-docker/#3-run-docker-compose","text":"docker $ docker-compose up Once up and running, you will see the following in your Docker dashboard: Elasticsearch Conductor UI Conductor Server You can access all three on your browser to verify that it is running correctly: Conductor Server URL: http://localhost:8080/swagger-ui/index.html?configUrl=/api-docs/swagger-config Conductor UI URL: http://localhost:5000/","title":"3. Run Docker Compose"},{"location":"running-locally-docker/#potential-problems","text":"Not enough memory You will need at least 16 GB of memory to run everything. You can modify the docker compose to skip using Elasticsearch if you have no option to run this with your memory options. To disable Elasticsearch using Docker Compose - follow the steps listed here: TODO LINK Elasticsearch fails to come up in arm64 based CPU machines As of writing this article, Conductor relies on 6.8.x version of Elasticsearch. This version doesn't have an arm64 based Docker image. You will need to use Elasticsearch 7.x which requires a bit of customization to get up and running Elasticsearch remains in Yellow health When you run Elasticsearch, sometimes the health remains in Yellow state. Conductor server by default requires Green state to run when indexing is enabled. To work around this, you can use the following property: conductor.elasticsearch.clusteHealthColor=yellow Reference: Issue 2262","title":"Potential problems"},{"location":"server/","text":"Running Conductor Locally \u00b6 Download and Run \u00b6 export CONDUCTOR_VER=3.3.4 export REPO_URL=https://repo1.maven.org/maven2/com/netflix/conductor/conductor-server curl $REPO_URL/$CONDUCTOR_VER/conductor-server-$CONDUCTOR_VER-boot.jar \\ --output conductor-server-$CONDUCTOR_VER-boot.jar; java -jar conductor-server-$CONDUCTOR_VER-boot.jar Navigate to the swagger URL: http://localhost:8080/swagger-ui/index.html?configUrl=/api-docs/swagger-config Build and Run \u00b6 In this article we will explore how you can set up Netflix Conductor on your local machine for trying out some of its features. Prerequisites \u00b6 JDK 11 or greater (Optional) Docker if you want to run tests. You can install docker from https://docs.docker.com/desktop/ Node for building and running UI. Install from https://nodejs.org/en/download/package-manager/ Yarn for building and running UI. https://classic.yarnpkg.com/en/docs/install . Steps to build Conductor server \u00b6 1. Checkout the code \u00b6 Clone conductor code from the repo: https://github.com/Netflix/conductor $ git clone https://github.com/Netflix/conductor.git 2. Build and run Server \u00b6 NOTE for Mac users : If you are using a new Mac with an Apple Silicon Chip, you must make a small change to conductor/grpc/build.gradle - adding \"osx-x86_64\" to two lines: protobuf { protoc { artifact = \"com.google.protobuf:protoc:${revProtoBuf}:osx-x86_64\" } plugins { grpc { artifact = \"io.grpc:protoc-gen-grpc-java:${revGrpc}:osx-x86_64\" } } ... } $ cd conductor conductor $ cd server server $ ../gradlew bootRun Navigate to the swagger API docs: http://localhost:8080/swagger-ui/index.html?configUrl=/api-docs/swagger-config Build and Run UI \u00b6 Conductor UI allows you to visualize the workflows. UI is built and run using node. $ cd conductor/ui ui $ yarn install ui $ yarn run start Launch UI http://localhost:5000 Summary \u00b6 All the data is stored in memory, so any workflows created or excuted will be wiped out once the server is terminated. Indexing is disabled, so search functionality in UI will not work and will result an empty set. See how to install Conductor using Docker with persistence and indexing.","title":"Installing Conductor Locally"},{"location":"server/#running-conductor-locally","text":"","title":"Running Conductor Locally"},{"location":"server/#download-and-run","text":"export CONDUCTOR_VER=3.3.4 export REPO_URL=https://repo1.maven.org/maven2/com/netflix/conductor/conductor-server curl $REPO_URL/$CONDUCTOR_VER/conductor-server-$CONDUCTOR_VER-boot.jar \\ --output conductor-server-$CONDUCTOR_VER-boot.jar; java -jar conductor-server-$CONDUCTOR_VER-boot.jar Navigate to the swagger URL: http://localhost:8080/swagger-ui/index.html?configUrl=/api-docs/swagger-config","title":"Download and Run"},{"location":"server/#build-and-run","text":"In this article we will explore how you can set up Netflix Conductor on your local machine for trying out some of its features.","title":"Build and Run"},{"location":"server/#prerequisites","text":"JDK 11 or greater (Optional) Docker if you want to run tests. You can install docker from https://docs.docker.com/desktop/ Node for building and running UI. Install from https://nodejs.org/en/download/package-manager/ Yarn for building and running UI. https://classic.yarnpkg.com/en/docs/install .","title":"Prerequisites"},{"location":"server/#steps-to-build-conductor-server","text":"","title":"Steps to build Conductor server"},{"location":"server/#1-checkout-the-code","text":"Clone conductor code from the repo: https://github.com/Netflix/conductor $ git clone https://github.com/Netflix/conductor.git","title":"1. Checkout the code"},{"location":"server/#2-build-and-run-server","text":"NOTE for Mac users : If you are using a new Mac with an Apple Silicon Chip, you must make a small change to conductor/grpc/build.gradle - adding \"osx-x86_64\" to two lines: protobuf { protoc { artifact = \"com.google.protobuf:protoc:${revProtoBuf}:osx-x86_64\" } plugins { grpc { artifact = \"io.grpc:protoc-gen-grpc-java:${revGrpc}:osx-x86_64\" } } ... } $ cd conductor conductor $ cd server server $ ../gradlew bootRun Navigate to the swagger API docs: http://localhost:8080/swagger-ui/index.html?configUrl=/api-docs/swagger-config","title":"2. Build and run Server"},{"location":"server/#build-and-run-ui","text":"Conductor UI allows you to visualize the workflows. UI is built and run using node. $ cd conductor/ui ui $ yarn install ui $ yarn run start Launch UI http://localhost:5000","title":"Build and Run UI"},{"location":"server/#summary","text":"All the data is stored in memory, so any workflows created or excuted will be wiped out once the server is terminated. Indexing is disabled, so search functionality in UI will not work and will result an empty set. See how to install Conductor using Docker with persistence and indexing.","title":"Summary"},{"location":"tasklifecycle/","text":"Task state transitions \u00b6 The figure below depicts the state transitions that a task can go through within a workflow execution. Retries and Failure Scenarios \u00b6 Task failure and retries \u00b6 Retries for failed task executions of each task can be configured independently. retryCount, retryDelaySeconds and retryLogic can be used to configure the retry mechanism. Worker (W1) polls for task T1 from the Conductor server and receives the task. Upon processing this task, the worker determines that the task execution is a failure and reports this to the server with FAILED status after 10 seconds. The server will persist this FAILED execution of T1. A new execution of task T1 will be created and scheduled to be polled. This task will be available to be polled after 5 (retryDelaySeconds) seconds. Timeout seconds \u00b6 Timeout is the maximum amount of time that the task must reach a terminal state in, else the task will be marked as TIMED_OUT. 0 seconds -> Worker polls for task T1 fom the Conductor server and receives the task. T1 is put into IN_PROGRESS status by the server. Worker starts processing the task but is unable to process the task at this time. Worker updates the server with T1 set to IN_PROGRESS status and a callback of 9 seconds. Server puts T1 back in the queue but makes it invisible and the worker continues to poll for the task but does not receive T1 for 9 seconds. 9,18 seconds -> Worker receives T1 from the server and is still unable to process the task and updates the server with a callback of 9 seconds. 27 seconds -> Worker polls and receives task T1 from the server and is now able to process this task. 30 seconds (T1 timeout) -> Server marks T1 as TIMED_OUT because it is not in a terminal state after first being moved to IN_PROGRESS status. Server schedules a new task based on the retry count. 32 seconds -> Worker completes processing of T1 and updates the server with COMPLETED status. Server will ignore this update since T1 has already been moved to a terminal status (TIMED_OUT). Response timeout seconds \u00b6 Response timeout is the time within which the worker must respond to the server with an update for the task, else the task will be marked as TIMED_OUT. 0 seconds -> Worker polls for the task T1 from the Conductor server and receives the task. T1 is put into IN_PROGRESS status by the server. Worker starts processing the task but the worker instance dies during this execution. 20 seconds (T1 responseTimeout) -> Server marks T1 as TIMED_OUT since the task has not been updated by the worker within the configured responseTimeoutSeconds (20). A new instance of task T1 is scheduled as per the retry configuration. 25 seconds -> The retried instance of T1 is available to be polled by the worker, after the retryDelaySeconds (5) has elapsed.","title":"Task Lifecycle"},{"location":"tasklifecycle/#task-state-transitions","text":"The figure below depicts the state transitions that a task can go through within a workflow execution.","title":"Task state transitions"},{"location":"tasklifecycle/#retries-and-failure-scenarios","text":"","title":"Retries and Failure Scenarios"},{"location":"tasklifecycle/#task-failure-and-retries","text":"Retries for failed task executions of each task can be configured independently. retryCount, retryDelaySeconds and retryLogic can be used to configure the retry mechanism. Worker (W1) polls for task T1 from the Conductor server and receives the task. Upon processing this task, the worker determines that the task execution is a failure and reports this to the server with FAILED status after 10 seconds. The server will persist this FAILED execution of T1. A new execution of task T1 will be created and scheduled to be polled. This task will be available to be polled after 5 (retryDelaySeconds) seconds.","title":"Task failure and retries"},{"location":"tasklifecycle/#timeout-seconds","text":"Timeout is the maximum amount of time that the task must reach a terminal state in, else the task will be marked as TIMED_OUT. 0 seconds -> Worker polls for task T1 fom the Conductor server and receives the task. T1 is put into IN_PROGRESS status by the server. Worker starts processing the task but is unable to process the task at this time. Worker updates the server with T1 set to IN_PROGRESS status and a callback of 9 seconds. Server puts T1 back in the queue but makes it invisible and the worker continues to poll for the task but does not receive T1 for 9 seconds. 9,18 seconds -> Worker receives T1 from the server and is still unable to process the task and updates the server with a callback of 9 seconds. 27 seconds -> Worker polls and receives task T1 from the server and is now able to process this task. 30 seconds (T1 timeout) -> Server marks T1 as TIMED_OUT because it is not in a terminal state after first being moved to IN_PROGRESS status. Server schedules a new task based on the retry count. 32 seconds -> Worker completes processing of T1 and updates the server with COMPLETED status. Server will ignore this update since T1 has already been moved to a terminal status (TIMED_OUT).","title":"Timeout seconds"},{"location":"tasklifecycle/#response-timeout-seconds","text":"Response timeout is the time within which the worker must respond to the server with an update for the task, else the task will be marked as TIMED_OUT. 0 seconds -> Worker polls for the task T1 from the Conductor server and receives the task. T1 is put into IN_PROGRESS status by the server. Worker starts processing the task but the worker instance dies during this execution. 20 seconds (T1 responseTimeout) -> Server marks T1 as TIMED_OUT since the task has not been updated by the worker within the configured responseTimeoutSeconds (20). A new instance of task T1 is scheduled as per the retry configuration. 25 seconds -> The retried instance of T1 is available to be polled by the worker, after the retryDelaySeconds (5) has elapsed.","title":"Response timeout seconds"},{"location":"technicaldetails/","text":"gRPC Framework \u00b6 As part of this addition, all of the modules and bootstrap code within them were refactored to leverage providers, which facilitated moving the Jetty server into a separate module and the conformance to Guice guidelines and best practices. This feature constitutes a server-side gRPC implementation along with protobuf RPC schemas for the workflow, metadata and task APIs that can be run concurrently with the Jersey-based HTTP/REST server. The protobuf models for all the types are exposed through the API. gRPC java clients for the workflow, metadata and task APIs are also available for use. Another valuable addition is an idiomatic Go gRPC client implementation for the worker API. The proto models are auto-generated at compile time using this ProtoGen library. This custom library adds messageInput and messageOutput fields to all proto tasks and task definitions. The goal of these fields is providing a type-safe way to pass input and input metadata through tasks that use the gRPC API. These fields use the Any protobuf type which can store any arbitrary message type in a type-safe way, without the server needing to know the exact serialization format of the message. In order to expose these Any objects in the REST API, a custom encoding is used that contains the raw data of the serialized message by converting it into a dictionary with '@type' and '@value' keys, where '@type' is identical to the canonical representation and '@value' contains a base64 encoded string with the binary data of the serialized message. The JsonMapperProvider provides the object mapper initialized with this module to enable serialization/deserialization of these JSON objects. Cassandra Persistence \u00b6 The Cassandra persistence layer currently provides a partial implementation of the ExecutionDAO that supports all the CRUD operations for tasks and workflow execution. The data modelling is done in a denormalized manner and stored in two tables. The \u201cworkflows\u201d table houses all the information for a workflow execution including all its tasks and is the source of truth for all the information regarding a workflow and its tasks. The \u201ctask_lookup\u201d table, as the name suggests stores a lookup of taskIds to workflowId. This table facilitates the fast retrieval of task data given a taskId. All the datastore operations that are used during the critical execution path of a workflow have been implemented currently. Few of the operational abilities of the ExecutionDAO are yet to be implemented. This module also does not provide implementations for QueueDAO and MetadataDAO. We envision using the Cassandra DAO with an external queue implementation, since implementing a queuing recipe on top of Cassandra is an anti-pattern that we want to stay away from. External Payload Storage \u00b6 The implementation of this feature is such that the externalization of payloads is fully transparent and automated to the user. Conductor operators can configure the usage of this feature and is completely abstracted and hidden from the user, thereby allowing the operators full control over the barrier limits. Currently, only AWS S3 is supported as a storage system, however, as with all other Conductor components, this is pluggable and can be extended to enable any other object store to be used as an external payload storage system. The externalization of payloads is enforced using two kinds of barriers . Soft barriers are used when the payload size is warranted enough to be stored as part of workflow execution. These payloads will be stored in external storage and used during execution. Hard barriers are enforced to safeguard against voluminous data, and such payloads are rejected and the workflow execution is failed. The payload size is evaluated in the client before being sent over the wire to the server. If the payload size exceeds the configured soft limit, the client makes a request to the server for the location at which the payload is to be stored. In this case where S3 is being used, the server returns a signed url for the location and the client uploads the payload using this signed url. The relative path to the payload object is then stored in the workflow/task metadata. The server can then download this payload from this path and use as needed during execution. This allows the server to control access to the S3 bucket, thereby making the user applications where the worker processes are run completely agnostic of the permissions needed to access this location. Dynamic Workflow Executions \u00b6 In the earlier version (v1.x), Conductor allowed the execution of workflows referencing the workflow and task definitions stored as metadata in the system. This meant that a workflow execution with 10 custom tasks to run entailed: Registration of the 10 task definitions if they don't exist (assuming workflow task type SIMPLE for simplicity) Registration of the workflow definition Each time a definition needs to be retrieved, a call to the metadata store needed to be performed In addition to that, the system allowed current metadata that is in use to be altered, leading to possible inconsistencies/race conditions To eliminate these pain points, the execution was changed such that the workflow definition is embedded within the workflow execution and the task definitions are themselves embedded within this workflow definition. This enables the concept of ephemeral/dynamic workflows and tasks. Instead of fetching metadata definitions throughout the execution, the definitions are fetched and embedded into the execution at the start of the workflow execution. This also enabled the StartWorkflowRequest to be extended to provide the complete workflow definition that will be used during execution, thus removing the need for pre-registration. The MetadataMapperService prefetches the workflow and task definitions and embeds these within the workflow data, if not provided in the StartWorkflowRequest. Following benefits are seen as a result of these changes: Grants immutability of the definition stored within the execution data against modifications to the metadata store Better testability of workflows with faster experimental changes to definitions Reduced stress on the datastore due to prefetching the metadata only once at the start Decoupling Elasticsearch from Persistence \u00b6 In the earlier version (1.x), the indexing logic was imbibed within the persistence layer, thus creating a tight coupling between the primary datastore and the indexing engine. This meant that the primary datastore determines how we orchestrate between the storage (redis, mysql, etc) and the indexer(elastic search). The main disadvantage of this approach is the lack of flexibility, that is, we cannot run an in-memory database and external elastic search or vice-versa. We plan to improve this further by removing the indexing from the critical path of workflow execution, thus reducing possible points of failure during execution. Elasticsearch 5/6 Support \u00b6 Indexing workflow execution is one of the primary features of Conductor. This enables archival of terminal state workflows from the primary data store, along with providing a clean search capability from the UI. In Conductor 1.x, we supported both versions 2 and 5 of Elasticsearch by shadowing version 5 and all its dependencies. This proved to be rather tedious increasing build times by over 10 minutes. In Conductor 2.x, we have removed active support for ES 2.x, because of valuable community contributions for elasticsearch 5 and elasticsearch 6 modules. Unlike Conductor 1.x, Conductor 2.x supports elasticsearch 5 by default, which can easily be replaced with version 6 by following the simple instructions here . Maintaining workflow consistency with distributed locking and fencing tokens \u00b6 Problem \u00b6 Conductor\u2019s Workflow decide is the core logic which recursively evaluates the state of the workflow, schedules tasks, persists workflow and task(s) state at several checkpoints, and progresses the workflow. In a multi-node Conductor server deployment, the decide on a workflow can be triggered concurrently. For example, the worker can update Conductor server with latest task state, which calls decide, while the sweeper service (which periodically evaluates the workflow state to progress from task timeouts) would also call the decide on a different instance. The decide can be run concurrently in two different jvm nodes with two different workflow states, and based on the workflow configuration and current state, the result could be inconsistent. A two-part solution to maintain Workflow Consistency \u00b6 Preventing concurrent decides with distributed locking: The goal is to allow only one decide to run on a workflow at any given time across the whole Conductor Server cluster. This can be achieved by plugging in distributed locking implementations like Zookeeper, Redlock etc. A Zookeeper module implementing Conductor\u2019s Locking service is provided. Preventing stale data updates with fencing tokens: While the locking service helps to run one decide at a time, it might still be possible for nodes with timed out locks to reactivate and continue execution from where it left off (usually with stale data). This can be avoided with fencing tokens, which basically is an incrementing counter on workflow state with read-before-write support in a transaction or similar construct. At Netflix, we use Cassandra. Considering the tradeoffs of Cassandra\u2019s Lightweight Transactions (LWT) and the probability of this stale updates happening, and our testing results, we\u2019ve decided to first only rollout distributed locking with Zookeeper. We'll monitor our system and add C LWT if needed. Setting up desired level of consistency \u00b6 Based on your requirements, it is possible to use none, one or both of the distributed locking and fencing tokens implementations. Alternative solution to distributed \"decide\" evaluation \u00b6 As mentioned in the previous section, the \"decide\" logic is triggered from multiple places in a conductor instance. Either a direct trigger such as user starting a workflow or a timed trigger from the Sweeper service. Sweeper service is responsible for continually checking state of all workflows executions and trigger the \"decide\" logic which in turn can time the workflow out. In a single node deployment (single dynomite rack and single conductor server) this shouldn't be a problem. But when running multiple replicated dynomite racks and a conductor server on top of each rack, this might trigger the race condition described in previous section. Dynomite rack is a single or multiple instance dynomite setup that holds all the data. More on dynomite HA setup: (https://netflixtechblog.com/introducing-dynomite-making-non-distributed-databases-distributed-c7bce3d89404) In a cluster deployment, the default behavior for Dyno Queues is such, that it distributes the workload (round-robin style) to all the conductor servers. This can create a situation where the first task to be executed is queued for conductor server #1 but the sweeper service is queued for conductor server #2. More on dyno queues \u00b6 Dyno queues are the default queuing mechanism of conductor. Queues are allocated and used for: * Task execution - each task type gets a queue * Workflow execution - single queue with all currently executing workflows (deciderQueue) * This queue is used by SweeperService Each conductor server instance gets its own set of queues . Or more precisely a queue shard of its own. This means that if you have 2 task types, you end up with 6 queues altogether e.g. conductor_queues.test.QUEUE._deciderQueue.c conductor_queues.test.QUEUE._deciderQueue.d conductor_queues.test.QUEUE.HTTP.c conductor_queues.test.QUEUE.HTTP.d conductor_queues.test.QUEUE.LAMBDA.c conductor_queues.test.QUEUE.LAMBDA.d The \"c\" and \"d\" suffixes are the shards identifying conductor server instace #1 and instance #2 respectively. The shard names are extracted from dynomite rack name such as us-east-1c that is set in \"LOCAL_RACK\" or \"EC2_AVAILABILTY_ZONE\" Considering an execution of a simple workflow with just 2 tasks: [HTTP, LAMBDA], you should end up with queues being filled as follows: Workflow execution -> conductor_queues.test.QUEUE._deciderQueue.c HTTP taks execution -> conductor_queues.test.QUEUE.HTTP.d LAMBDA task execution -> conductor_queues.test.QUEUE.LAMBDA.c Which means that SweeperService in conductor instance #1 is responsible for sweeping the workflow, conductor #2 is responsible for executing HTTP task and conductor #1 again responsible for executing LAMBDA task. This illustrates the race condition: If the HTTP task completion in instance #2 happens at the same time as sweep in instance #1 ... you can end up with 2 different updates to a workflow execution: one update timing workflow out while the other completing the task and scheduling next. The round-robin strategy responsible for work distribution is defined here Back to alternative solution \u00b6 The alternative solution here is Switching round-robin queue allocation for a local-only strategy . Meaning that a workflow and its task executions are queued only for the conductor instance which started the workflow. This completely avoids the race condition for the price of removing task execution distribution. Since all tasks and the sweeper service read/write only from/to \"local\" queues, it is impossible to run into a race condition between conductor instances. The downside here is that the workload is not distributed across all conductor servers. Which might be an advantage in active-standby deployments. Considering other downsides ... Considering a situation where a conductor instance goes down: * With local-only strategy, the workflow executions from failed conductor instance will not progress until: * The conductor instance is restarted or * The executions are manually terminated and restarted from a different node * With round-robin strategy, there is a chance the tasks will be rescheduled on a different conductor node * This is nondeterministic though Enabling local only queue allocation strategy for dyno queues: Just enable following setting the config.properties: workflow.dyno.queue.sharding.strategy=localOnly The default is roundRobin","title":"Technical Details"},{"location":"technicaldetails/#grpc-framework","text":"As part of this addition, all of the modules and bootstrap code within them were refactored to leverage providers, which facilitated moving the Jetty server into a separate module and the conformance to Guice guidelines and best practices. This feature constitutes a server-side gRPC implementation along with protobuf RPC schemas for the workflow, metadata and task APIs that can be run concurrently with the Jersey-based HTTP/REST server. The protobuf models for all the types are exposed through the API. gRPC java clients for the workflow, metadata and task APIs are also available for use. Another valuable addition is an idiomatic Go gRPC client implementation for the worker API. The proto models are auto-generated at compile time using this ProtoGen library. This custom library adds messageInput and messageOutput fields to all proto tasks and task definitions. The goal of these fields is providing a type-safe way to pass input and input metadata through tasks that use the gRPC API. These fields use the Any protobuf type which can store any arbitrary message type in a type-safe way, without the server needing to know the exact serialization format of the message. In order to expose these Any objects in the REST API, a custom encoding is used that contains the raw data of the serialized message by converting it into a dictionary with '@type' and '@value' keys, where '@type' is identical to the canonical representation and '@value' contains a base64 encoded string with the binary data of the serialized message. The JsonMapperProvider provides the object mapper initialized with this module to enable serialization/deserialization of these JSON objects.","title":"gRPC Framework"},{"location":"technicaldetails/#cassandra-persistence","text":"The Cassandra persistence layer currently provides a partial implementation of the ExecutionDAO that supports all the CRUD operations for tasks and workflow execution. The data modelling is done in a denormalized manner and stored in two tables. The \u201cworkflows\u201d table houses all the information for a workflow execution including all its tasks and is the source of truth for all the information regarding a workflow and its tasks. The \u201ctask_lookup\u201d table, as the name suggests stores a lookup of taskIds to workflowId. This table facilitates the fast retrieval of task data given a taskId. All the datastore operations that are used during the critical execution path of a workflow have been implemented currently. Few of the operational abilities of the ExecutionDAO are yet to be implemented. This module also does not provide implementations for QueueDAO and MetadataDAO. We envision using the Cassandra DAO with an external queue implementation, since implementing a queuing recipe on top of Cassandra is an anti-pattern that we want to stay away from.","title":"Cassandra Persistence"},{"location":"technicaldetails/#external-payload-storage","text":"The implementation of this feature is such that the externalization of payloads is fully transparent and automated to the user. Conductor operators can configure the usage of this feature and is completely abstracted and hidden from the user, thereby allowing the operators full control over the barrier limits. Currently, only AWS S3 is supported as a storage system, however, as with all other Conductor components, this is pluggable and can be extended to enable any other object store to be used as an external payload storage system. The externalization of payloads is enforced using two kinds of barriers . Soft barriers are used when the payload size is warranted enough to be stored as part of workflow execution. These payloads will be stored in external storage and used during execution. Hard barriers are enforced to safeguard against voluminous data, and such payloads are rejected and the workflow execution is failed. The payload size is evaluated in the client before being sent over the wire to the server. If the payload size exceeds the configured soft limit, the client makes a request to the server for the location at which the payload is to be stored. In this case where S3 is being used, the server returns a signed url for the location and the client uploads the payload using this signed url. The relative path to the payload object is then stored in the workflow/task metadata. The server can then download this payload from this path and use as needed during execution. This allows the server to control access to the S3 bucket, thereby making the user applications where the worker processes are run completely agnostic of the permissions needed to access this location.","title":"External Payload Storage"},{"location":"technicaldetails/#dynamic-workflow-executions","text":"In the earlier version (v1.x), Conductor allowed the execution of workflows referencing the workflow and task definitions stored as metadata in the system. This meant that a workflow execution with 10 custom tasks to run entailed: Registration of the 10 task definitions if they don't exist (assuming workflow task type SIMPLE for simplicity) Registration of the workflow definition Each time a definition needs to be retrieved, a call to the metadata store needed to be performed In addition to that, the system allowed current metadata that is in use to be altered, leading to possible inconsistencies/race conditions To eliminate these pain points, the execution was changed such that the workflow definition is embedded within the workflow execution and the task definitions are themselves embedded within this workflow definition. This enables the concept of ephemeral/dynamic workflows and tasks. Instead of fetching metadata definitions throughout the execution, the definitions are fetched and embedded into the execution at the start of the workflow execution. This also enabled the StartWorkflowRequest to be extended to provide the complete workflow definition that will be used during execution, thus removing the need for pre-registration. The MetadataMapperService prefetches the workflow and task definitions and embeds these within the workflow data, if not provided in the StartWorkflowRequest. Following benefits are seen as a result of these changes: Grants immutability of the definition stored within the execution data against modifications to the metadata store Better testability of workflows with faster experimental changes to definitions Reduced stress on the datastore due to prefetching the metadata only once at the start","title":"Dynamic Workflow Executions"},{"location":"technicaldetails/#decoupling-elasticsearch-from-persistence","text":"In the earlier version (1.x), the indexing logic was imbibed within the persistence layer, thus creating a tight coupling between the primary datastore and the indexing engine. This meant that the primary datastore determines how we orchestrate between the storage (redis, mysql, etc) and the indexer(elastic search). The main disadvantage of this approach is the lack of flexibility, that is, we cannot run an in-memory database and external elastic search or vice-versa. We plan to improve this further by removing the indexing from the critical path of workflow execution, thus reducing possible points of failure during execution.","title":"Decoupling Elasticsearch from Persistence"},{"location":"technicaldetails/#elasticsearch-56-support","text":"Indexing workflow execution is one of the primary features of Conductor. This enables archival of terminal state workflows from the primary data store, along with providing a clean search capability from the UI. In Conductor 1.x, we supported both versions 2 and 5 of Elasticsearch by shadowing version 5 and all its dependencies. This proved to be rather tedious increasing build times by over 10 minutes. In Conductor 2.x, we have removed active support for ES 2.x, because of valuable community contributions for elasticsearch 5 and elasticsearch 6 modules. Unlike Conductor 1.x, Conductor 2.x supports elasticsearch 5 by default, which can easily be replaced with version 6 by following the simple instructions here .","title":"Elasticsearch 5/6 Support"},{"location":"technicaldetails/#maintaining-workflow-consistency-with-distributed-locking-and-fencing-tokens","text":"","title":"Maintaining workflow consistency with distributed locking and fencing tokens"},{"location":"technicaldetails/#problem","text":"Conductor\u2019s Workflow decide is the core logic which recursively evaluates the state of the workflow, schedules tasks, persists workflow and task(s) state at several checkpoints, and progresses the workflow. In a multi-node Conductor server deployment, the decide on a workflow can be triggered concurrently. For example, the worker can update Conductor server with latest task state, which calls decide, while the sweeper service (which periodically evaluates the workflow state to progress from task timeouts) would also call the decide on a different instance. The decide can be run concurrently in two different jvm nodes with two different workflow states, and based on the workflow configuration and current state, the result could be inconsistent.","title":"Problem"},{"location":"technicaldetails/#a-two-part-solution-to-maintain-workflow-consistency","text":"Preventing concurrent decides with distributed locking: The goal is to allow only one decide to run on a workflow at any given time across the whole Conductor Server cluster. This can be achieved by plugging in distributed locking implementations like Zookeeper, Redlock etc. A Zookeeper module implementing Conductor\u2019s Locking service is provided. Preventing stale data updates with fencing tokens: While the locking service helps to run one decide at a time, it might still be possible for nodes with timed out locks to reactivate and continue execution from where it left off (usually with stale data). This can be avoided with fencing tokens, which basically is an incrementing counter on workflow state with read-before-write support in a transaction or similar construct. At Netflix, we use Cassandra. Considering the tradeoffs of Cassandra\u2019s Lightweight Transactions (LWT) and the probability of this stale updates happening, and our testing results, we\u2019ve decided to first only rollout distributed locking with Zookeeper. We'll monitor our system and add C LWT if needed.","title":"A two-part solution to maintain Workflow Consistency"},{"location":"technicaldetails/#setting-up-desired-level-of-consistency","text":"Based on your requirements, it is possible to use none, one or both of the distributed locking and fencing tokens implementations.","title":"Setting up desired level of consistency"},{"location":"technicaldetails/#alternative-solution-to-distributed-decide-evaluation","text":"As mentioned in the previous section, the \"decide\" logic is triggered from multiple places in a conductor instance. Either a direct trigger such as user starting a workflow or a timed trigger from the Sweeper service. Sweeper service is responsible for continually checking state of all workflows executions and trigger the \"decide\" logic which in turn can time the workflow out. In a single node deployment (single dynomite rack and single conductor server) this shouldn't be a problem. But when running multiple replicated dynomite racks and a conductor server on top of each rack, this might trigger the race condition described in previous section. Dynomite rack is a single or multiple instance dynomite setup that holds all the data. More on dynomite HA setup: (https://netflixtechblog.com/introducing-dynomite-making-non-distributed-databases-distributed-c7bce3d89404) In a cluster deployment, the default behavior for Dyno Queues is such, that it distributes the workload (round-robin style) to all the conductor servers. This can create a situation where the first task to be executed is queued for conductor server #1 but the sweeper service is queued for conductor server #2.","title":"Alternative solution to distributed \"decide\" evaluation"},{"location":"technicaldetails/#more-on-dyno-queues","text":"Dyno queues are the default queuing mechanism of conductor. Queues are allocated and used for: * Task execution - each task type gets a queue * Workflow execution - single queue with all currently executing workflows (deciderQueue) * This queue is used by SweeperService Each conductor server instance gets its own set of queues . Or more precisely a queue shard of its own. This means that if you have 2 task types, you end up with 6 queues altogether e.g. conductor_queues.test.QUEUE._deciderQueue.c conductor_queues.test.QUEUE._deciderQueue.d conductor_queues.test.QUEUE.HTTP.c conductor_queues.test.QUEUE.HTTP.d conductor_queues.test.QUEUE.LAMBDA.c conductor_queues.test.QUEUE.LAMBDA.d The \"c\" and \"d\" suffixes are the shards identifying conductor server instace #1 and instance #2 respectively. The shard names are extracted from dynomite rack name such as us-east-1c that is set in \"LOCAL_RACK\" or \"EC2_AVAILABILTY_ZONE\" Considering an execution of a simple workflow with just 2 tasks: [HTTP, LAMBDA], you should end up with queues being filled as follows: Workflow execution -> conductor_queues.test.QUEUE._deciderQueue.c HTTP taks execution -> conductor_queues.test.QUEUE.HTTP.d LAMBDA task execution -> conductor_queues.test.QUEUE.LAMBDA.c Which means that SweeperService in conductor instance #1 is responsible for sweeping the workflow, conductor #2 is responsible for executing HTTP task and conductor #1 again responsible for executing LAMBDA task. This illustrates the race condition: If the HTTP task completion in instance #2 happens at the same time as sweep in instance #1 ... you can end up with 2 different updates to a workflow execution: one update timing workflow out while the other completing the task and scheduling next. The round-robin strategy responsible for work distribution is defined here","title":"More on dyno queues"},{"location":"technicaldetails/#back-to-alternative-solution","text":"The alternative solution here is Switching round-robin queue allocation for a local-only strategy . Meaning that a workflow and its task executions are queued only for the conductor instance which started the workflow. This completely avoids the race condition for the price of removing task execution distribution. Since all tasks and the sweeper service read/write only from/to \"local\" queues, it is impossible to run into a race condition between conductor instances. The downside here is that the workload is not distributed across all conductor servers. Which might be an advantage in active-standby deployments. Considering other downsides ... Considering a situation where a conductor instance goes down: * With local-only strategy, the workflow executions from failed conductor instance will not progress until: * The conductor instance is restarted or * The executions are manually terminated and restarted from a different node * With round-robin strategy, there is a chance the tasks will be rescheduled on a different conductor node * This is nondeterministic though Enabling local only queue allocation strategy for dyno queues: Just enable following setting the config.properties: workflow.dyno.queue.sharding.strategy=localOnly The default is roundRobin","title":"Back to alternative solution"},{"location":"configuration/eventhandlers/","text":"Introduction \u00b6 Eventing in Conductor provides for loose coupling between workflows and support for producing and consuming events from external systems. This includes: Being able to produce an event (message) in an external system like SQS or internal to Conductor. Start a workflow when a specific event occurs that matches the provided criteria. Conductor provides SUB_WORKFLOW task that can be used to embed a workflow inside parent workflow. Eventing supports provides similar capability without explicitly adding dependencies and provides fire-and-forget style integrations. Event Task \u00b6 Event task provides ability to publish an event (message) to either Conductor or an external eventing system like SQS. Event tasks are useful for creating event based dependencies for workflows and tasks. See Event Task for documentation. Event Handler \u00b6 Event handlers are listeners registered that executes an action when a matching event occurs. The supported actions are: Start a Workflow Fail a Task Complete a Task Event Handlers can be configured to listen to Conductor Events or an external event like SQS. Configuration \u00b6 Event Handlers are configured via /event/ APIs. Structure: \u00b6 { \"name\" : \"descriptive unique name\", \"event\": \"event_type:event_location\", \"condition\": \"boolean condition\", \"actions\": [\"see examples below\"] } Condition \u00b6 Condition is an expression that MUST evaluate to a boolean value. A Javascript like syntax is supported that can be used to evaluate condition based on the payload. Actions are executed only when the condition evaluates to true . Examples Given the following payload in the message: { \"fileType\": \"AUDIO\", \"version\": 3, \"metadata\": { \"length\": 300, \"codec\": \"aac\" } } Expression Result $.version > 1 true $.version > 10 false $.metadata.length == 300 true Actions \u00b6 Start A Workflow { \"action\": \"start_workflow\", \"start_workflow\": { \"name\": \"WORKFLOW_NAME\", \"version\": \"<optional_param>\", \"input\": { \"param1\": \"${param1}\" } } } Complete Task * { \"action\": \"complete_task\", \"complete_task\": { \"workflowId\": \"${workflowId}\", \"taskRefName\": \"task_1\", \"output\": { \"response\": \"${result}\" } }, \"expandInlineJSON\": true } Fail Task * { \"action\": \"fail_task\", \"fail_task\": { \"workflowId\": \"${workflowId}\", \"taskRefName\": \"task_1\", \"output\": { \"response\": \"${result}\" } }, \"expandInlineJSON\": true } Input for starting a workflow and output when completing / failing task follows the same expressions used for wiring workflow inputs. Expanding stringified JSON elements in payload expandInlineJSON property, when set to true will expand the inlined stringified JSON elements in the payload to JSON documents and replace the string value with JSON document. This feature allows such elements to be used with JSON path expressions. Extending \u00b6 Provide the implementation of EventQueueProvider . SQS Queue Provider: SQSEventQueueProvider.java","title":"Event Handlers"},{"location":"configuration/eventhandlers/#introduction","text":"Eventing in Conductor provides for loose coupling between workflows and support for producing and consuming events from external systems. This includes: Being able to produce an event (message) in an external system like SQS or internal to Conductor. Start a workflow when a specific event occurs that matches the provided criteria. Conductor provides SUB_WORKFLOW task that can be used to embed a workflow inside parent workflow. Eventing supports provides similar capability without explicitly adding dependencies and provides fire-and-forget style integrations.","title":"Introduction"},{"location":"configuration/eventhandlers/#event-task","text":"Event task provides ability to publish an event (message) to either Conductor or an external eventing system like SQS. Event tasks are useful for creating event based dependencies for workflows and tasks. See Event Task for documentation.","title":"Event Task"},{"location":"configuration/eventhandlers/#event-handler","text":"Event handlers are listeners registered that executes an action when a matching event occurs. The supported actions are: Start a Workflow Fail a Task Complete a Task Event Handlers can be configured to listen to Conductor Events or an external event like SQS.","title":"Event Handler"},{"location":"configuration/eventhandlers/#configuration","text":"Event Handlers are configured via /event/ APIs.","title":"Configuration"},{"location":"configuration/eventhandlers/#structure","text":"{ \"name\" : \"descriptive unique name\", \"event\": \"event_type:event_location\", \"condition\": \"boolean condition\", \"actions\": [\"see examples below\"] }","title":"Structure:"},{"location":"configuration/eventhandlers/#condition","text":"Condition is an expression that MUST evaluate to a boolean value. A Javascript like syntax is supported that can be used to evaluate condition based on the payload. Actions are executed only when the condition evaluates to true . Examples Given the following payload in the message: { \"fileType\": \"AUDIO\", \"version\": 3, \"metadata\": { \"length\": 300, \"codec\": \"aac\" } } Expression Result $.version > 1 true $.version > 10 false $.metadata.length == 300 true","title":"Condition"},{"location":"configuration/eventhandlers/#actions","text":"Start A Workflow { \"action\": \"start_workflow\", \"start_workflow\": { \"name\": \"WORKFLOW_NAME\", \"version\": \"<optional_param>\", \"input\": { \"param1\": \"${param1}\" } } } Complete Task * { \"action\": \"complete_task\", \"complete_task\": { \"workflowId\": \"${workflowId}\", \"taskRefName\": \"task_1\", \"output\": { \"response\": \"${result}\" } }, \"expandInlineJSON\": true } Fail Task * { \"action\": \"fail_task\", \"fail_task\": { \"workflowId\": \"${workflowId}\", \"taskRefName\": \"task_1\", \"output\": { \"response\": \"${result}\" } }, \"expandInlineJSON\": true } Input for starting a workflow and output when completing / failing task follows the same expressions used for wiring workflow inputs. Expanding stringified JSON elements in payload expandInlineJSON property, when set to true will expand the inlined stringified JSON elements in the payload to JSON documents and replace the string value with JSON document. This feature allows such elements to be used with JSON path expressions.","title":"Actions"},{"location":"configuration/eventhandlers/#extending","text":"Provide the implementation of EventQueueProvider . SQS Queue Provider: SQSEventQueueProvider.java","title":"Extending"},{"location":"configuration/isolationgroups/","text":"Isolation Group Id \u00b6 Consider an HTTP task where the latency of an API is high, task queue piles up effecting execution of other HTTP tasks which have low latency. We can isolate the execution of such tasks to have predictable performance using isolationgroupId , a property of task definition. When we set isolationGroupId, the executor(SystemTaskWorkerCoordinator) will allocate an isolated queue and an isolated thread pool for execution of those tasks. If no isolationgroupId is specified in task definition, then fallback is default behaviour where the executor executes the task in shared thread-pool for all tasks. Example taskdef { \"name\": \"encode_task\", \"retryCount\": 3, \"timeoutSeconds\": 1200, \"inputKeys\": [ \"sourceRequestId\", \"qcElementType\" ], \"outputKeys\": [ \"state\", \"skipped\", \"result\" ], \"timeoutPolicy\": \"TIME_OUT_WF\", \"retryLogic\": \"FIXED\", \"retryDelaySeconds\": 600, \"responseTimeoutSeconds\": 3600, \"concurrentExecLimit\": 100, \"rateLimitFrequencyInSeconds\": 60, \"rateLimitPerFrequency\": 50, \"isolationgroupId\": \"myIsolationGroupId\" } Example Workflow task { \"name\": \"encode_and_deploy\", \"description\": \"Encodes a file and deploys to CDN\", \"version\": 1, \"tasks\": [ { \"name\": \"encode\", \"taskReferenceName\": \"encode\", \"type\": \"HTTP\", \"inputParameters\": { \"http_request\": { \"uri\": \"http://localhost:9200/conductor/_search?size=10\", \"method\": \"GET\" } } } ], \"outputParameters\": { \"cdn_url\": \"${d1.output.location}\" }, \"failureWorkflow\": \"cleanup_encode_resources\", \"restartable\": true, \"workflowStatusListenerEnabled\": true, \"schemaVersion\": 2 } puts encode in HTTP-myIsolationGroupId queue, and allocates a new thread pool for this for execution. Note: To enable this feature, the workflow.isolated.system.task.enable property needs to be made true ,its default value is false The property workflow.isolated.system.task.worker.thread.count sets the thread pool size for isolated tasks; default is 1 . isolationGroupId is currently supported only in HTTP and kafka Task. Execution Name Space \u00b6 executionNameSpace A property of taskdef can be used to provide JVM isolation to task execution and scale executor deployments horizontally. Limitation of using isolationGroupId is that we need to scale executors vertically as the executor allocates a new thread pool per isolationGroupId . Also, since the executor runs the tasks in the same JVM, task execution is not isolated completely. To support JVM isolation, and also allow the executors to scale horizontally, we can use executionNameSpace property in taskdef. Executor consumes tasks whose executionNameSpace matches with the configuration property workflow.system.task.worker.executionNameSpace If the property is not set, the executor executes tasks without any executionNameSpace set. { \"name\": \"encode_task\", \"retryCount\": 3, \"timeoutSeconds\": 1200, \"inputKeys\": [ \"sourceRequestId\", \"qcElementType\" ], \"outputKeys\": [ \"state\", \"skipped\", \"result\" ], \"timeoutPolicy\": \"TIME_OUT_WF\", \"retryLogic\": \"FIXED\", \"retryDelaySeconds\": 600, \"responseTimeoutSeconds\": 3600, \"concurrentExecLimit\": 100, \"rateLimitFrequencyInSeconds\": 60, \"rateLimitPerFrequency\": 50, \"executionNameSpace\": \"myExecutionNameSpace\" } Example Workflow task { \"name\": \"encode_and_deploy\", \"description\": \"Encodes a file and deploys to CDN\", \"version\": 1, \"tasks\": [ { \"name\": \"encode\", \"taskReferenceName\": \"encode\", \"type\": \"HTTP\", \"inputParameters\": { \"http_request\": { \"uri\": \"http://localhost:9200/conductor/_search?size=10\", \"method\": \"GET\" } } } ], \"outputParameters\": { \"cdn_url\": \"${d1.output.location}\" }, \"failureWorkflow\": \"cleanup_encode_resources\", \"restartable\": true, \"workflowStatusListenerEnabled\": true, \"schemaVersion\": 2 } encode task is executed by the executor deployment whose workflow.system.task.worker.executionNameSpace property is myExecutionNameSpace executionNameSpace can be used along with isolationGroupId If the above task contains a isolationGroupId myIsolationGroupId , the tasks will be scheduled in a queue HTTP@myExecutionNameSpace-myIsolationGroupId, and have a new threadpool for execution in the deployment group with myExecutionNameSpace","title":"Isolation Groups"},{"location":"configuration/isolationgroups/#isolation-group-id","text":"Consider an HTTP task where the latency of an API is high, task queue piles up effecting execution of other HTTP tasks which have low latency. We can isolate the execution of such tasks to have predictable performance using isolationgroupId , a property of task definition. When we set isolationGroupId, the executor(SystemTaskWorkerCoordinator) will allocate an isolated queue and an isolated thread pool for execution of those tasks. If no isolationgroupId is specified in task definition, then fallback is default behaviour where the executor executes the task in shared thread-pool for all tasks. Example taskdef { \"name\": \"encode_task\", \"retryCount\": 3, \"timeoutSeconds\": 1200, \"inputKeys\": [ \"sourceRequestId\", \"qcElementType\" ], \"outputKeys\": [ \"state\", \"skipped\", \"result\" ], \"timeoutPolicy\": \"TIME_OUT_WF\", \"retryLogic\": \"FIXED\", \"retryDelaySeconds\": 600, \"responseTimeoutSeconds\": 3600, \"concurrentExecLimit\": 100, \"rateLimitFrequencyInSeconds\": 60, \"rateLimitPerFrequency\": 50, \"isolationgroupId\": \"myIsolationGroupId\" } Example Workflow task { \"name\": \"encode_and_deploy\", \"description\": \"Encodes a file and deploys to CDN\", \"version\": 1, \"tasks\": [ { \"name\": \"encode\", \"taskReferenceName\": \"encode\", \"type\": \"HTTP\", \"inputParameters\": { \"http_request\": { \"uri\": \"http://localhost:9200/conductor/_search?size=10\", \"method\": \"GET\" } } } ], \"outputParameters\": { \"cdn_url\": \"${d1.output.location}\" }, \"failureWorkflow\": \"cleanup_encode_resources\", \"restartable\": true, \"workflowStatusListenerEnabled\": true, \"schemaVersion\": 2 } puts encode in HTTP-myIsolationGroupId queue, and allocates a new thread pool for this for execution. Note: To enable this feature, the workflow.isolated.system.task.enable property needs to be made true ,its default value is false The property workflow.isolated.system.task.worker.thread.count sets the thread pool size for isolated tasks; default is 1 . isolationGroupId is currently supported only in HTTP and kafka Task.","title":"Isolation Group Id"},{"location":"configuration/isolationgroups/#execution-name-space","text":"executionNameSpace A property of taskdef can be used to provide JVM isolation to task execution and scale executor deployments horizontally. Limitation of using isolationGroupId is that we need to scale executors vertically as the executor allocates a new thread pool per isolationGroupId . Also, since the executor runs the tasks in the same JVM, task execution is not isolated completely. To support JVM isolation, and also allow the executors to scale horizontally, we can use executionNameSpace property in taskdef. Executor consumes tasks whose executionNameSpace matches with the configuration property workflow.system.task.worker.executionNameSpace If the property is not set, the executor executes tasks without any executionNameSpace set. { \"name\": \"encode_task\", \"retryCount\": 3, \"timeoutSeconds\": 1200, \"inputKeys\": [ \"sourceRequestId\", \"qcElementType\" ], \"outputKeys\": [ \"state\", \"skipped\", \"result\" ], \"timeoutPolicy\": \"TIME_OUT_WF\", \"retryLogic\": \"FIXED\", \"retryDelaySeconds\": 600, \"responseTimeoutSeconds\": 3600, \"concurrentExecLimit\": 100, \"rateLimitFrequencyInSeconds\": 60, \"rateLimitPerFrequency\": 50, \"executionNameSpace\": \"myExecutionNameSpace\" } Example Workflow task { \"name\": \"encode_and_deploy\", \"description\": \"Encodes a file and deploys to CDN\", \"version\": 1, \"tasks\": [ { \"name\": \"encode\", \"taskReferenceName\": \"encode\", \"type\": \"HTTP\", \"inputParameters\": { \"http_request\": { \"uri\": \"http://localhost:9200/conductor/_search?size=10\", \"method\": \"GET\" } } } ], \"outputParameters\": { \"cdn_url\": \"${d1.output.location}\" }, \"failureWorkflow\": \"cleanup_encode_resources\", \"restartable\": true, \"workflowStatusListenerEnabled\": true, \"schemaVersion\": 2 } encode task is executed by the executor deployment whose workflow.system.task.worker.executionNameSpace property is myExecutionNameSpace executionNameSpace can be used along with isolationGroupId If the above task contains a isolationGroupId myIsolationGroupId , the tasks will be scheduled in a queue HTTP@myExecutionNameSpace-myIsolationGroupId, and have a new threadpool for execution in the deployment group with myExecutionNameSpace","title":"Execution Name Space"},{"location":"configuration/sysoperator/","text":"Operators \u00b6 Operators are built-in primitives in Conductor that allow you to define the control flow in the workflow. Operators are similar to programming constructs such as for loops, decisions, etc. Conductor has support for most of the programing primitives allowing you to define the most advanced workflows. Supported Operators \u00b6 Conductor supports the following programming language constructs: Language Construct Conductor Operator Do/While or Loops Do While Task Dynamic Fork Dynamic Fork Task Fork / Parallel execution Fork Task Join Join Task Sub Process / Sub-Flow Sub Workflow Task Switch//Decision/if..then...else Switch Task Terminate Terminate Task Variables Variable Task Wait Wait Task","title":"System Operators"},{"location":"configuration/sysoperator/#operators","text":"Operators are built-in primitives in Conductor that allow you to define the control flow in the workflow. Operators are similar to programming constructs such as for loops, decisions, etc. Conductor has support for most of the programing primitives allowing you to define the most advanced workflows.","title":"Operators"},{"location":"configuration/sysoperator/#supported-operators","text":"Conductor supports the following programming language constructs: Language Construct Conductor Operator Do/While or Loops Do While Task Dynamic Fork Dynamic Fork Task Fork / Parallel execution Fork Task Join Join Task Sub Process / Sub-Flow Sub Workflow Task Switch//Decision/if..then...else Switch Task Terminate Terminate Task Variables Variable Task Wait Wait Task","title":"Supported Operators"},{"location":"configuration/systask/","text":"System Task \u00b6 System Tasks (Workers) are built-in tasks that are general purpose and re-usable. They run on the Conductor servers. Such tasks allow you to get started without having to write custom workers. Available System Tasks \u00b6 Conductor has the following set of system tasks available. Task Description Use Case Event Publishing Event Task External eventing system integration. e.g. amqp, sqs, nats HTTP HTTP Task Invoke any HTTP(S) endpoints Inline Code Execution Inline Task Execute arbitrary lightweight javascript code JQ Transform JQ Task Use JQ to transform task input/output Kafka Publish Kafka Task Publish messages to Kafka Name Description joinOn List of task reference names, which the EXCLUSIVE_JOIN will lookout for to capture output. From above example, this could be [\"T2\", \"T3\"] defaultExclusiveJoinTask Task reference name, whose output should be used incase the decision case is undefined. From above example, this could be [\"T1\"] Example { \"name\": \"exclusive_join\", \"taskReferenceName\": \"exclusiveJoin\", \"type\": \"EXCLUSIVE_JOIN\", \"joinOn\": [ \"task2\", \"task3\" ], \"defaultExclusiveJoinTask\": [ \"task1\" ] } Wait \u00b6 A wait task is implemented as a gate that remains in IN_PROGRESS state unless marked as COMPLETED or FAILED by an external trigger. To use a wait task, set the task type as WAIT Parameters: None required. External Triggers for Wait Task Task Resource endpoint can be used to update the status of a task to a terminate state. Contrib module provides SQS integration where an external system can place a message in a pre-configured queue that the server listens on. As the messages arrive, they are marked as COMPLETED or FAILED . SQS Queues SQS queues used by the server to update the task status can be retrieve using the following API: GET /queue When updating the status of the task, the message needs to conform to the following spec: Message has to be a valid JSON string. The message JSON should contain a key named externalId with the value being a JSONified string that contains the following keys: workflowId : Id of the workflow taskRefName : Task reference name that should be updated. Each queue represents a specific task status and tasks are marked accordingly. e.g. message coming to a COMPLETED queue marks the task status as COMPLETED . Tasks' output is updated with the message. Example SQS Payload: { \"some_key\": \"valuex\", \"externalId\": \"{\\\"taskRefName\\\":\\\"TASK_REFERENCE_NAME\\\",\\\"workflowId\\\":\\\"WORKFLOW_ID\\\"}\" } Dynamic Task \u00b6 Dynamic Task allows to execute one of the registered Tasks dynamically at run-time. It accepts the task name to execute in inputParameters. Parameters: name description dynamicTaskNameParam Name of the parameter from the task input whose value is used to schedule the task. e.g. if the value of the parameter is ABC, the next task scheduled is of type 'ABC'. Example { \"name\": \"user_task\", \"taskReferenceName\": \"t1\", \"inputParameters\": { \"files\": \"${workflow.input.files}\", \"taskToExecute\": \"${workflow.input.user_supplied_task}\" }, \"type\": \"DYNAMIC\", \"dynamicTaskNameParam\": \"taskToExecute\" } If the workflow is started with input parameter user_supplied_task's value as user_task_2 , Conductor will schedule user_task_2 when scheduling this dynamic task. Inline Task \u00b6 Inline Task helps execute ad-hoc logic at Workflow run-time, using any evaluator engine. Supported evaluators are value-param evaluator which simply translates the input parameter to output and javascript evaluator that evaluates Javascript expression. This is particularly helpful in running simple evaluations in Conductor server, over creating Workers. Parameters: name type description notes evaluatorType String Type of the evaluator. Supported evaluators: value-param , javascript which evaluates javascript expression. expression String Expression associated with the type of evaluator. For javascript evaluator, Javascript evaluation engine is used to evaluate expression defined as a string. Must return a value. Must be non-empty. Besides expression , any value is accessible as $.value for the expression to evaluate. Outputs: name type description result Map Contains the output returned by the evaluator based on the expression The task output can then be referenced in downstream tasks like: \"${inline_test.output.result.testvalue}\" Example { \"name\": \"INLINE_TASK\", \"taskReferenceName\": \"inline_test\", \"type\": \"INLINE\", \"inputParameters\": { \"inlineValue\": \"${workflow.input.inlineValue}\", \"evaluatorType\": \"javascript\", \"expression\": \"function scriptFun(){if ($.inlineValue == 1){ return {testvalue: true} } else { return {testvalue: false} }} scriptFun();\" } } Terminate Task \u00b6 Task that can terminate a workflow with a given status and modify the workflow's output with a given parameter. It can act as a \"return\" statement for conditions where you simply want to terminate your workflow. For example, if you have a decision where the first condition is met, you want to execute some tasks, otherwise you want to finish your workflow. Parameters: name type description notes terminationStatus String can only accept \"COMPLETED\" or \"FAILED\" task cannot be optional terminationReason String reason for incompletion to be set in the workflow optional workflowOutput Any Expected workflow output optional Outputs: name type description output Map The content of workflowOutput from the inputParameters. An empty object if workflowOutput is not set. { \"name\": \"terminate\", \"taskReferenceName\": \"terminate0\", \"inputParameters\": { \"terminationStatus\": \"COMPLETED\", \"terminationReason\": \"\", \"workflowOutput\": \"${task0.output}\" }, \"type\": \"TERMINATE\", \"startDelay\": 0, \"optional\": false } Kafka Publish Task \u00b6 A kafka Publish task is used to push messages to another microservice via kafka Parameters: The task expects an input parameter named kafka_request as part of the task's input with the following details: name description bootStrapServers bootStrapServers for connecting to given kafka. key Key to be published keySerializer Serializer used for serializing the key published to kafka. One of the following can be set : 1. org.apache.kafka.common.serialization.IntegerSerializer 2. org.apache.kafka.common.serialization.LongSerializer 3. org.apache.kafka.common.serialization.StringSerializer. Default is String serializer value Value published to kafka requestTimeoutMs Request timeout while publishing to kafka. If this value is not given the value is read from the property kafka.publish.request.timeout.ms . If the property is not set the value defaults to 100 ms maxBlockMs maxBlockMs while publishing to kafka. If this value is not given the value is read from the property kafka.publish.max.block.ms . If the property is not set the value defaults to 500 ms headers A map of additional kafka headers to be sent along with the request. topic Topic to publish The producer created in the kafka task is cached. By default the cache size is 10 and expiry time is 120000 ms. To change the defaults following can be modified kafka.publish.producer.cache.size,kafka.publish.producer.cache.time.ms respectively. Kafka Task Output Task status transitions to COMPLETED Example Task sample { \"name\": \"call_kafka\", \"taskReferenceName\": \"call_kafka\", \"inputParameters\": { \"kafka_request\": { \"topic\": \"userTopic\", \"value\": \"Message to publish\", \"bootStrapServers\": \"localhost:9092\", \"headers\": { \"x-Auth\":\"Auth-key\" }, \"key\": \"123\", \"keySerializer\": \"org.apache.kafka.common.serialization.IntegerSerializer\" } }, \"type\": \"KAFKA_PUBLISH\" } The task is marked as FAILED if the message could not be published to the Kafka queue. Do While Task \u00b6 Sequentially execute a list of task as long as a condition is true. The list of tasks is executed first, before the condition is checked (even for the first iteration). When scheduled, each task of this loop will see its taskReferenceName concatenated with __i , with i being the iteration number, starting at 1. Warning: taskReferenceName containing arithmetic operators must not be used. Each task output is stored as part of the DO_WHILE task, indexed by the iteration value (see example below), allowing the condition to reference the output of a task for a specific iteration (eg. $.LoopTask['iteration]['first_task'] ) The DO_WHILE task is set to FAILED as soon as one of the loopTask fails. In such case retry, iteration starts from 1. Limitations: - Domain or isolation group execution is unsupported; - Nested DO_WHILE is unsupported; - SUB_WORKFLOW is unsupported; - Since loopover tasks will be executed in loop inside scope of parent do while task, crossing branching outside of DO_WHILE task is not respected. Branching inside loopover task is supported. Parameters: |name|type|description|","title":"System Tasks"},{"location":"configuration/systask/#system-task","text":"System Tasks (Workers) are built-in tasks that are general purpose and re-usable. They run on the Conductor servers. Such tasks allow you to get started without having to write custom workers.","title":"System Task"},{"location":"configuration/systask/#available-system-tasks","text":"Conductor has the following set of system tasks available. Task Description Use Case Event Publishing Event Task External eventing system integration. e.g. amqp, sqs, nats HTTP HTTP Task Invoke any HTTP(S) endpoints Inline Code Execution Inline Task Execute arbitrary lightweight javascript code JQ Transform JQ Task Use JQ to transform task input/output Kafka Publish Kafka Task Publish messages to Kafka Name Description joinOn List of task reference names, which the EXCLUSIVE_JOIN will lookout for to capture output. From above example, this could be [\"T2\", \"T3\"] defaultExclusiveJoinTask Task reference name, whose output should be used incase the decision case is undefined. From above example, this could be [\"T1\"] Example { \"name\": \"exclusive_join\", \"taskReferenceName\": \"exclusiveJoin\", \"type\": \"EXCLUSIVE_JOIN\", \"joinOn\": [ \"task2\", \"task3\" ], \"defaultExclusiveJoinTask\": [ \"task1\" ] }","title":"Available System Tasks"},{"location":"configuration/systask/#wait","text":"A wait task is implemented as a gate that remains in IN_PROGRESS state unless marked as COMPLETED or FAILED by an external trigger. To use a wait task, set the task type as WAIT Parameters: None required. External Triggers for Wait Task Task Resource endpoint can be used to update the status of a task to a terminate state. Contrib module provides SQS integration where an external system can place a message in a pre-configured queue that the server listens on. As the messages arrive, they are marked as COMPLETED or FAILED . SQS Queues SQS queues used by the server to update the task status can be retrieve using the following API: GET /queue When updating the status of the task, the message needs to conform to the following spec: Message has to be a valid JSON string. The message JSON should contain a key named externalId with the value being a JSONified string that contains the following keys: workflowId : Id of the workflow taskRefName : Task reference name that should be updated. Each queue represents a specific task status and tasks are marked accordingly. e.g. message coming to a COMPLETED queue marks the task status as COMPLETED . Tasks' output is updated with the message. Example SQS Payload: { \"some_key\": \"valuex\", \"externalId\": \"{\\\"taskRefName\\\":\\\"TASK_REFERENCE_NAME\\\",\\\"workflowId\\\":\\\"WORKFLOW_ID\\\"}\" }","title":"Wait"},{"location":"configuration/systask/#dynamic-task","text":"Dynamic Task allows to execute one of the registered Tasks dynamically at run-time. It accepts the task name to execute in inputParameters. Parameters: name description dynamicTaskNameParam Name of the parameter from the task input whose value is used to schedule the task. e.g. if the value of the parameter is ABC, the next task scheduled is of type 'ABC'. Example { \"name\": \"user_task\", \"taskReferenceName\": \"t1\", \"inputParameters\": { \"files\": \"${workflow.input.files}\", \"taskToExecute\": \"${workflow.input.user_supplied_task}\" }, \"type\": \"DYNAMIC\", \"dynamicTaskNameParam\": \"taskToExecute\" } If the workflow is started with input parameter user_supplied_task's value as user_task_2 , Conductor will schedule user_task_2 when scheduling this dynamic task.","title":"Dynamic Task"},{"location":"configuration/systask/#inline-task","text":"Inline Task helps execute ad-hoc logic at Workflow run-time, using any evaluator engine. Supported evaluators are value-param evaluator which simply translates the input parameter to output and javascript evaluator that evaluates Javascript expression. This is particularly helpful in running simple evaluations in Conductor server, over creating Workers. Parameters: name type description notes evaluatorType String Type of the evaluator. Supported evaluators: value-param , javascript which evaluates javascript expression. expression String Expression associated with the type of evaluator. For javascript evaluator, Javascript evaluation engine is used to evaluate expression defined as a string. Must return a value. Must be non-empty. Besides expression , any value is accessible as $.value for the expression to evaluate. Outputs: name type description result Map Contains the output returned by the evaluator based on the expression The task output can then be referenced in downstream tasks like: \"${inline_test.output.result.testvalue}\" Example { \"name\": \"INLINE_TASK\", \"taskReferenceName\": \"inline_test\", \"type\": \"INLINE\", \"inputParameters\": { \"inlineValue\": \"${workflow.input.inlineValue}\", \"evaluatorType\": \"javascript\", \"expression\": \"function scriptFun(){if ($.inlineValue == 1){ return {testvalue: true} } else { return {testvalue: false} }} scriptFun();\" } }","title":"Inline Task"},{"location":"configuration/systask/#terminate-task","text":"Task that can terminate a workflow with a given status and modify the workflow's output with a given parameter. It can act as a \"return\" statement for conditions where you simply want to terminate your workflow. For example, if you have a decision where the first condition is met, you want to execute some tasks, otherwise you want to finish your workflow. Parameters: name type description notes terminationStatus String can only accept \"COMPLETED\" or \"FAILED\" task cannot be optional terminationReason String reason for incompletion to be set in the workflow optional workflowOutput Any Expected workflow output optional Outputs: name type description output Map The content of workflowOutput from the inputParameters. An empty object if workflowOutput is not set. { \"name\": \"terminate\", \"taskReferenceName\": \"terminate0\", \"inputParameters\": { \"terminationStatus\": \"COMPLETED\", \"terminationReason\": \"\", \"workflowOutput\": \"${task0.output}\" }, \"type\": \"TERMINATE\", \"startDelay\": 0, \"optional\": false }","title":"Terminate Task"},{"location":"configuration/systask/#kafka-publish-task","text":"A kafka Publish task is used to push messages to another microservice via kafka Parameters: The task expects an input parameter named kafka_request as part of the task's input with the following details: name description bootStrapServers bootStrapServers for connecting to given kafka. key Key to be published keySerializer Serializer used for serializing the key published to kafka. One of the following can be set : 1. org.apache.kafka.common.serialization.IntegerSerializer 2. org.apache.kafka.common.serialization.LongSerializer 3. org.apache.kafka.common.serialization.StringSerializer. Default is String serializer value Value published to kafka requestTimeoutMs Request timeout while publishing to kafka. If this value is not given the value is read from the property kafka.publish.request.timeout.ms . If the property is not set the value defaults to 100 ms maxBlockMs maxBlockMs while publishing to kafka. If this value is not given the value is read from the property kafka.publish.max.block.ms . If the property is not set the value defaults to 500 ms headers A map of additional kafka headers to be sent along with the request. topic Topic to publish The producer created in the kafka task is cached. By default the cache size is 10 and expiry time is 120000 ms. To change the defaults following can be modified kafka.publish.producer.cache.size,kafka.publish.producer.cache.time.ms respectively. Kafka Task Output Task status transitions to COMPLETED Example Task sample { \"name\": \"call_kafka\", \"taskReferenceName\": \"call_kafka\", \"inputParameters\": { \"kafka_request\": { \"topic\": \"userTopic\", \"value\": \"Message to publish\", \"bootStrapServers\": \"localhost:9092\", \"headers\": { \"x-Auth\":\"Auth-key\" }, \"key\": \"123\", \"keySerializer\": \"org.apache.kafka.common.serialization.IntegerSerializer\" } }, \"type\": \"KAFKA_PUBLISH\" } The task is marked as FAILED if the message could not be published to the Kafka queue.","title":"Kafka Publish Task"},{"location":"configuration/systask/#do-while-task","text":"Sequentially execute a list of task as long as a condition is true. The list of tasks is executed first, before the condition is checked (even for the first iteration). When scheduled, each task of this loop will see its taskReferenceName concatenated with __i , with i being the iteration number, starting at 1. Warning: taskReferenceName containing arithmetic operators must not be used. Each task output is stored as part of the DO_WHILE task, indexed by the iteration value (see example below), allowing the condition to reference the output of a task for a specific iteration (eg. $.LoopTask['iteration]['first_task'] ) The DO_WHILE task is set to FAILED as soon as one of the loopTask fails. In such case retry, iteration starts from 1. Limitations: - Domain or isolation group execution is unsupported; - Nested DO_WHILE is unsupported; - SUB_WORKFLOW is unsupported; - Since loopover tasks will be executed in loop inside scope of parent do while task, crossing branching outside of DO_WHILE task is not respected. Branching inside loopover task is supported. Parameters: |name|type|description|","title":"Do While Task"},{"location":"configuration/taskdef/","text":"Task Definition \u00b6 Tasks are the building blocks of workflow in Conductor. A task can be an operator, system task or custom code written in any programming language. A typical Conductor workflow is a list of tasks that are executed until completion or the termination of the workflow. Conductor maintains a registry of worker tasks. A task MUST be registered before being used in a workflow. Example { \"name\": \"encode_task\", \"retryCount\": 3, \"timeoutSeconds\": 1200, \"pollTimeoutSeconds\": 3600, \"inputKeys\": [ \"sourceRequestId\", \"qcElementType\" ], \"outputKeys\": [ \"state\", \"skipped\", \"result\" ], \"timeoutPolicy\": \"TIME_OUT_WF\", \"retryLogic\": \"FIXED\", \"retryDelaySeconds\": 600, \"responseTimeoutSeconds\": 1200, \"concurrentExecLimit\": 100, \"rateLimitFrequencyInSeconds\": 60, \"rateLimitPerFrequency\": 50, \"ownerEmail\": \"foo@bar.com\", \"description\": \"Sample Encoding task\" } Field Description Notes name Task Type. Unique name of the Task that resonates with it's function. Unique description Description of the task optional retryCount No. of retries to attempt when a Task is marked as failure defaults to 3 retryLogic Mechanism for the retries Retry Logic values retryDelaySeconds Time to wait before retries defaults to 60 seconds timeoutPolicy Task's timeout policy timeout policy values timeoutSeconds Time in seconds, after which the task is marked as TIMED_OUT if not completed after transitioning to IN_PROGRESS status for the first time No timeouts if set to 0 pollTimeoutSeconds Time in seconds, after which the task is marked as TIMED_OUT if not polled by a worker No timeouts if set to 0 responseTimeoutSeconds Must be greater than 0 and less than timeoutSeconds. The task is rescheduled if not updated with a status after this time (heartbeat mechanism). Useful when the worker polls for the task but fails to complete due to errors/network failure. defaults to 3600 backoffScaleFactor Must be greater than 0. Scale factor for linearity of the backoff defaults to 1 inputKeys Array of keys of task's expected input. Used for documenting task's input. See Using inputKeys and outputKeys . optional outputKeys Array of keys of task's expected output. Used for documenting task's output optional inputTemplate See Using inputTemplate below. optional concurrentExecLimit Number of tasks that can be executed at any given time. optional rateLimitFrequencyInSeconds, rateLimitPerFrequency See Task Rate limits below. optional Retry Logic \u00b6 FIXED : Reschedule the task after the retryDelaySeconds EXPONENTIAL_BACKOFF : Reschedule after retryDelaySeconds 2^(attemptNumber) LINEAR_BACKOFF : Reschedule after retryDelaySeconds * backoffRate * attemptNumber Timeout Policy \u00b6 RETRY : Retries the task again TIME_OUT_WF : Workflow is marked as TIMED_OUT and terminated ALERT_ONLY : Registers a counter (task_timeout) Task Concurrent Execution Limits \u00b6 concurrentExecLimit limits the number of simultaneous Task executions at any point. Example: If you have 1000 task executions waiting in the queue, and 1000 workers polling this queue for tasks, but if you have set concurrentExecLimit to 10, only 10 tasks would be given to workers (which would lead to starvation). If any of the workers finishes execution, a new task(s) will be removed from the queue, while still keeping the current execution count to 10. Task Rate limits \u00b6 Note: Rate limiting is only supported for the Redis-persistence module and is not available with other persistence layers. rateLimitFrequencyInSeconds and rateLimitPerFrequency should be used together. rateLimitFrequencyInSeconds sets the \"frequency window\", i.e the duration to be used in events per duration . Eg: 1s, 5s, 60s, 300s etc. rateLimitPerFrequency defines the number of Tasks that can be given to Workers per given \"frequency window\". Example: Let's set rateLimitFrequencyInSeconds = 5 , and rateLimitPerFrequency = 12 . This means, our frequency window is of 5 seconds duration, and for each frequency window, Conductor would only give 12 tasks to workers. So, in a given minute, Conductor would only give 12*(60/5) = 144 tasks to workers irrespective of the number of workers that are polling for the task. Note that unlike concurrentExecLimit , rate limiting doesn't take into account tasks already in progress/completed. Even if all the previous tasks are executed within 1 sec, or would take a few days, the new tasks are still given to workers at configured frequency, 144 tasks per minute in above example. Using inputKeys and outputKeys \u00b6 inputKeys and outputKeys can be considered as parameters and return values for the Task. Consider the task Definition as being represented by an interface: (value1, value2 .. valueN) someTaskDefinition(key1, key2 .. keyN); However, these parameters are not strictly enforced at the moment. Both inputKeys and outputKeys act as a documentation for task re-use. The tasks in workflow need not define all of the keys in the task definition. In the future, this can be extended to be a strict template that all task implementations must adhere to, just like interfaces in programming languages. Using inputTemplate \u00b6 inputTemplate allows to define default values, which can be overridden by values provided in Workflow. Eg: In your Task Definition, you can define your inputTemplate as: \"inputTemplate\": { \"url\": \"https://some_url:7004\" } Now, in your workflow Definition, when using above task, you can use the default url or override with something else in the task's inputParameters . \"inputParameters\": { \"url\": \"${workflow.input.some_new_url}\" }","title":"Task Definition"},{"location":"configuration/taskdef/#task-definition","text":"Tasks are the building blocks of workflow in Conductor. A task can be an operator, system task or custom code written in any programming language. A typical Conductor workflow is a list of tasks that are executed until completion or the termination of the workflow. Conductor maintains a registry of worker tasks. A task MUST be registered before being used in a workflow. Example { \"name\": \"encode_task\", \"retryCount\": 3, \"timeoutSeconds\": 1200, \"pollTimeoutSeconds\": 3600, \"inputKeys\": [ \"sourceRequestId\", \"qcElementType\" ], \"outputKeys\": [ \"state\", \"skipped\", \"result\" ], \"timeoutPolicy\": \"TIME_OUT_WF\", \"retryLogic\": \"FIXED\", \"retryDelaySeconds\": 600, \"responseTimeoutSeconds\": 1200, \"concurrentExecLimit\": 100, \"rateLimitFrequencyInSeconds\": 60, \"rateLimitPerFrequency\": 50, \"ownerEmail\": \"foo@bar.com\", \"description\": \"Sample Encoding task\" } Field Description Notes name Task Type. Unique name of the Task that resonates with it's function. Unique description Description of the task optional retryCount No. of retries to attempt when a Task is marked as failure defaults to 3 retryLogic Mechanism for the retries Retry Logic values retryDelaySeconds Time to wait before retries defaults to 60 seconds timeoutPolicy Task's timeout policy timeout policy values timeoutSeconds Time in seconds, after which the task is marked as TIMED_OUT if not completed after transitioning to IN_PROGRESS status for the first time No timeouts if set to 0 pollTimeoutSeconds Time in seconds, after which the task is marked as TIMED_OUT if not polled by a worker No timeouts if set to 0 responseTimeoutSeconds Must be greater than 0 and less than timeoutSeconds. The task is rescheduled if not updated with a status after this time (heartbeat mechanism). Useful when the worker polls for the task but fails to complete due to errors/network failure. defaults to 3600 backoffScaleFactor Must be greater than 0. Scale factor for linearity of the backoff defaults to 1 inputKeys Array of keys of task's expected input. Used for documenting task's input. See Using inputKeys and outputKeys . optional outputKeys Array of keys of task's expected output. Used for documenting task's output optional inputTemplate See Using inputTemplate below. optional concurrentExecLimit Number of tasks that can be executed at any given time. optional rateLimitFrequencyInSeconds, rateLimitPerFrequency See Task Rate limits below. optional","title":"Task Definition"},{"location":"configuration/taskdef/#retry-logic","text":"FIXED : Reschedule the task after the retryDelaySeconds EXPONENTIAL_BACKOFF : Reschedule after retryDelaySeconds 2^(attemptNumber) LINEAR_BACKOFF : Reschedule after retryDelaySeconds * backoffRate * attemptNumber","title":"Retry Logic"},{"location":"configuration/taskdef/#timeout-policy","text":"RETRY : Retries the task again TIME_OUT_WF : Workflow is marked as TIMED_OUT and terminated ALERT_ONLY : Registers a counter (task_timeout)","title":"Timeout Policy"},{"location":"configuration/taskdef/#task-concurrent-execution-limits","text":"concurrentExecLimit limits the number of simultaneous Task executions at any point. Example: If you have 1000 task executions waiting in the queue, and 1000 workers polling this queue for tasks, but if you have set concurrentExecLimit to 10, only 10 tasks would be given to workers (which would lead to starvation). If any of the workers finishes execution, a new task(s) will be removed from the queue, while still keeping the current execution count to 10.","title":"Task Concurrent Execution Limits"},{"location":"configuration/taskdef/#task-rate-limits","text":"Note: Rate limiting is only supported for the Redis-persistence module and is not available with other persistence layers. rateLimitFrequencyInSeconds and rateLimitPerFrequency should be used together. rateLimitFrequencyInSeconds sets the \"frequency window\", i.e the duration to be used in events per duration . Eg: 1s, 5s, 60s, 300s etc. rateLimitPerFrequency defines the number of Tasks that can be given to Workers per given \"frequency window\". Example: Let's set rateLimitFrequencyInSeconds = 5 , and rateLimitPerFrequency = 12 . This means, our frequency window is of 5 seconds duration, and for each frequency window, Conductor would only give 12 tasks to workers. So, in a given minute, Conductor would only give 12*(60/5) = 144 tasks to workers irrespective of the number of workers that are polling for the task. Note that unlike concurrentExecLimit , rate limiting doesn't take into account tasks already in progress/completed. Even if all the previous tasks are executed within 1 sec, or would take a few days, the new tasks are still given to workers at configured frequency, 144 tasks per minute in above example.","title":"Task Rate limits"},{"location":"configuration/taskdef/#using-inputkeys-and-outputkeys","text":"inputKeys and outputKeys can be considered as parameters and return values for the Task. Consider the task Definition as being represented by an interface: (value1, value2 .. valueN) someTaskDefinition(key1, key2 .. keyN); However, these parameters are not strictly enforced at the moment. Both inputKeys and outputKeys act as a documentation for task re-use. The tasks in workflow need not define all of the keys in the task definition. In the future, this can be extended to be a strict template that all task implementations must adhere to, just like interfaces in programming languages.","title":"Using inputKeys and outputKeys"},{"location":"configuration/taskdef/#using-inputtemplate","text":"inputTemplate allows to define default values, which can be overridden by values provided in Workflow. Eg: In your Task Definition, you can define your inputTemplate as: \"inputTemplate\": { \"url\": \"https://some_url:7004\" } Now, in your workflow Definition, when using above task, you can use the default url or override with something else in the task's inputParameters . \"inputParameters\": { \"url\": \"${workflow.input.some_new_url}\" }","title":"Using inputTemplate"},{"location":"configuration/taskdomains/","text":"Task Domains \u00b6 Task domains helps support task development. The idea is same \u201ctask definition\u201d can be implemented in different \u201cdomains\u201d. A domain is some arbitrary name that the developer controls. So when the workflow is started, the caller can specify, out of all the tasks in the workflow, which tasks need to run in a specific domain, this domain is then used to poll for task on the client side to execute it. As an example if a workflow (WF1) has 3 tasks T1, T2, T3. The workflow is deployed and working fine, which means there are T2 workers polling and executing. If you modify T2 and run it locally there is no guarantee that your modified T2 worker will get the task that you are looking for as it coming from the general T2 queue. \u201cTask Domain\u201d feature solves this problem by splitting the T2 queue by domains, so when the app polls for task T2 in a specific domain, it get the correct task. When starting a workflow multiple domains can be specified as a fall backs, for example \"domain1,domain2\". Conductor keeps track of last polling time for each task, so in this case it checks if the there are any active workers for \"domain1\" then the task is put in \"domain1\", if not then the same check is done for the next domain in sequence \"domain2\" and so on. If no workers are active for the domains provided: If NO_DOMAIN is provided as last token in list of domains, then no domain is set. Else, task will be added to last inactive domain in list of domains, hoping that workers would soon be available for that domain. Also, a * token can be used to apply domains for all tasks. This can be overridden by providing task specific mappings along with * . For example, the below configuration: \"taskToDomain\": { \"*\": \"mydomain\", \"some_task_x\":\"NO_DOMAIN\", \"some_task_y\": \"someDomain, NO_DOMAIN\", \"some_task_z\": \"someInactiveDomain1, someInactiveDomain2\" } puts some_task_x in default queue (no domain). puts some_task_y in someDomain domain, if available or in default otherwise. puts some_task_z in someInactiveDomain2 , even though workers are not available yet. and puts all other tasks in mydomain (even if workers are not available). Note that this \"fall back\" type domain strings can only be used when starting the workflow, when polling from the client only one domain is used. Also, NO_DOMAIN token should be used last. How to use Task Domains \u00b6 Change the poll call \u00b6 The poll call must now specify the domain. Java Client \u00b6 If you are using the java client then a simple property change will force TaskRunnerConfigurer to pass the domain to the poller. conductor.worker.T2.domain=mydomain //Task T2 needs to poll for domain \"mydomain\" REST call \u00b6 GET /tasks/poll/batch/T2?workerid=myworker&domain=mydomain GET /tasks/poll/T2?workerid=myworker&domain=mydomain Change the start workflow call \u00b6 When starting the workflow, make sure the task to domain mapping is passes Java Client \u00b6 Map<String, Object> input = new HashMap<>(); input.put(\"wf_input1\", \"one\u201d); Map<String, String> taskToDomain = new HashMap<>(); taskToDomain.put(\"T2\", \"mydomain\"); // Other options ... // taskToDomain.put(\"*\", \"mydomain, NO_DOMAIN\") // taskToDomain.put(\"T2\", \"mydomain, fallbackDomain1, fallbackDomain2\") StartWorkflowRequest swr = new StartWorkflowRequest(); swr.withName(\u201cmyWorkflow\u201d) .withCorrelationId(\u201ccorr1\u201d) .withVersion(1) .withInput(input) .withTaskToDomain(taskToDomain); wfclient.startWorkflow(swr); REST call \u00b6 POST /workflow { \"name\": \"myWorkflow\", \"version\": 1, \"correlatonId\": \"corr1\" \"input\": { \"wf_input1\": \"one\" }, \"taskToDomain\": { \"*\": \"mydomain\", \"some_task_x\":\"NO_DOMAIN\", \"some_task_y\": \"someDomain, NO_DOMAIN\" } }","title":"Task Domains"},{"location":"configuration/taskdomains/#task-domains","text":"Task domains helps support task development. The idea is same \u201ctask definition\u201d can be implemented in different \u201cdomains\u201d. A domain is some arbitrary name that the developer controls. So when the workflow is started, the caller can specify, out of all the tasks in the workflow, which tasks need to run in a specific domain, this domain is then used to poll for task on the client side to execute it. As an example if a workflow (WF1) has 3 tasks T1, T2, T3. The workflow is deployed and working fine, which means there are T2 workers polling and executing. If you modify T2 and run it locally there is no guarantee that your modified T2 worker will get the task that you are looking for as it coming from the general T2 queue. \u201cTask Domain\u201d feature solves this problem by splitting the T2 queue by domains, so when the app polls for task T2 in a specific domain, it get the correct task. When starting a workflow multiple domains can be specified as a fall backs, for example \"domain1,domain2\". Conductor keeps track of last polling time for each task, so in this case it checks if the there are any active workers for \"domain1\" then the task is put in \"domain1\", if not then the same check is done for the next domain in sequence \"domain2\" and so on. If no workers are active for the domains provided: If NO_DOMAIN is provided as last token in list of domains, then no domain is set. Else, task will be added to last inactive domain in list of domains, hoping that workers would soon be available for that domain. Also, a * token can be used to apply domains for all tasks. This can be overridden by providing task specific mappings along with * . For example, the below configuration: \"taskToDomain\": { \"*\": \"mydomain\", \"some_task_x\":\"NO_DOMAIN\", \"some_task_y\": \"someDomain, NO_DOMAIN\", \"some_task_z\": \"someInactiveDomain1, someInactiveDomain2\" } puts some_task_x in default queue (no domain). puts some_task_y in someDomain domain, if available or in default otherwise. puts some_task_z in someInactiveDomain2 , even though workers are not available yet. and puts all other tasks in mydomain (even if workers are not available). Note that this \"fall back\" type domain strings can only be used when starting the workflow, when polling from the client only one domain is used. Also, NO_DOMAIN token should be used last.","title":"Task Domains"},{"location":"configuration/taskdomains/#how-to-use-task-domains","text":"","title":"How to use Task Domains"},{"location":"configuration/taskdomains/#change-the-poll-call","text":"The poll call must now specify the domain.","title":"Change the poll call"},{"location":"configuration/taskdomains/#java-client","text":"If you are using the java client then a simple property change will force TaskRunnerConfigurer to pass the domain to the poller. conductor.worker.T2.domain=mydomain //Task T2 needs to poll for domain \"mydomain\"","title":"Java Client"},{"location":"configuration/taskdomains/#rest-call","text":"GET /tasks/poll/batch/T2?workerid=myworker&domain=mydomain GET /tasks/poll/T2?workerid=myworker&domain=mydomain","title":"REST call"},{"location":"configuration/taskdomains/#change-the-start-workflow-call","text":"When starting the workflow, make sure the task to domain mapping is passes","title":"Change the start workflow call"},{"location":"configuration/taskdomains/#java-client_1","text":"Map<String, Object> input = new HashMap<>(); input.put(\"wf_input1\", \"one\u201d); Map<String, String> taskToDomain = new HashMap<>(); taskToDomain.put(\"T2\", \"mydomain\"); // Other options ... // taskToDomain.put(\"*\", \"mydomain, NO_DOMAIN\") // taskToDomain.put(\"T2\", \"mydomain, fallbackDomain1, fallbackDomain2\") StartWorkflowRequest swr = new StartWorkflowRequest(); swr.withName(\u201cmyWorkflow\u201d) .withCorrelationId(\u201ccorr1\u201d) .withVersion(1) .withInput(input) .withTaskToDomain(taskToDomain); wfclient.startWorkflow(swr);","title":"Java Client"},{"location":"configuration/taskdomains/#rest-call_1","text":"POST /workflow { \"name\": \"myWorkflow\", \"version\": 1, \"correlatonId\": \"corr1\" \"input\": { \"wf_input1\": \"one\" }, \"taskToDomain\": { \"*\": \"mydomain\", \"some_task_x\":\"NO_DOMAIN\", \"some_task_y\": \"someDomain, NO_DOMAIN\" } }","title":"REST call"},{"location":"configuration/workerdef/","text":"Worker \u00b6 A worker is responsible for executing a task. Operator and System tasks are handled by the Conductor server, while user defined tasks needs to have a worker created that awaits the work to be scheduled by the server for it to be executed. Workers can be implemented in any language, and Conductor provides support for Java, Golang and Python worker framework that provides features such as polling threads, metrics and server communication that makes creating workers each. Each worker embodies Microservice design pattern and follows certain basic principles: Workers are stateless and do not implement a workflow specific logic. Each worker executes a very specific task and produces well defined output given specific inputs. Workers are meant to be idempotent (or should handle cases where the task that partially executed gets rescheduled due to timeouts etc.) Workers do not implement the logic to handle retries etc, that is taken care by the Conductor server.","title":"Worker Definition"},{"location":"configuration/workerdef/#worker","text":"A worker is responsible for executing a task. Operator and System tasks are handled by the Conductor server, while user defined tasks needs to have a worker created that awaits the work to be scheduled by the server for it to be executed. Workers can be implemented in any language, and Conductor provides support for Java, Golang and Python worker framework that provides features such as polling threads, metrics and server communication that makes creating workers each. Each worker embodies Microservice design pattern and follows certain basic principles: Workers are stateless and do not implement a workflow specific logic. Each worker executes a very specific task and produces well defined output given specific inputs. Workers are meant to be idempotent (or should handle cases where the task that partially executed gets rescheduled due to timeouts etc.) Workers do not implement the logic to handle retries etc, that is taken care by the Conductor server.","title":"Worker"},{"location":"configuration/workflowdef/","text":"Workflows \u00b6 What are Workflows? \u00b6 At a high level, a workflow is the Conductor primitive that encompasses the definition and flow of your business logic. A workflow is a collection (graph) of tasks and sub-workflows. A workflow definition specifies the order of execution of these Tasks . It also specifies how data/state is passed from one task to the other (using the input/output parameters). These are then combined to give you the final result. This orchestration of Tasks can happen in a hybrid ecosystem that includes microservices, serverless functions, and monolithic applications. They can also span across any public cloud and on-premise data center footprints. In addition, the orchestration of tasks can be across any programming language since Conductor is also language agnostic. One key benefit of this approach is that you can build a complex application using simple and granular tasks that do not need to be aware of or keep track of the state of your application's execution flow. Conductor keeps track of the state, calls tasks in the right order (sequentially or in parallel, as defined by you), retry calls if needed, handle failure scenarios gracefully, and outputs the final result. Leveraging workflows in Conductor enables developers to truly focus on their core mission - building their application code in the languages of their choice. Conductor does the heavy lifting associated with ensuring high reliability, transactional consistency, and long durability of their workflows. Simply put, wherever your application's component lives and whichever languages they were written in, you can build a workflow in Conductor to orchestrate their execution in a reliable & scalable manner. What does a Workflow look like? \u00b6 Let's start with a basic workflow and understand what are the different aspects of it. In particular, we will talk about two stages of a workflow, defining a workflow and executing a workflow Simple Workflow Example \u00b6 Assume your business logic is to simply to get some shipping information and then do the shipping. You start by logically partitioning them into two tasks: shipping_info shipping_task First we would build these two task definitions. Let's assume that shipping info takes an account number, and returns a name and address. Example { \"name\": \"mail_a_box\", \"description\": \"shipping Workflow\", \"version\": 1, \"tasks\": [ { \"name\": \"shipping_info\", \"taskReferenceName\": \"shipping_info_ref\", \"inputParameters\": { \"account\": \"${workflow.input.accountNumber}\" }, \"type\": \"SIMPLE\" }, { \"name\": \"shipping_task\", \"taskReferenceName\": \"shipping_task_ref\", \"inputParameters\": { \"name\": \"${shipping_info_ref.output.name}\", \"streetAddress\": \"${shipping_info_ref.output.streetAddress}\", \"city\": \"${shipping_info_ref.output.city}\", \"state\": \"${shipping_info_ref.output.state}\", \"zipcode\": \"${shipping_info_ref.output.zipcode}\", }, \"type\": \"SIMPLE\" } ], \"outputParameters\": { \"trackingNumber\": \"${shipping_task_ref.output.trackinNumber}\" }, \"failureWorkflow\": \"shipping_issues\", \"restartable\": true, \"workflowStatusListenerEnabled\": true, \"ownerEmail\": \"devrel@orkes.io\", \"timeoutPolicy\": \"ALERT_ONLY\", \"timeoutSeconds\": 0, \"variables\": {}, \"inputTemplate\": {} } The mail_a_box workflow has 2 tasks: 1. The first task takes the provided account number, and outpus an address. 2. The 2nd task takes the address infom and generates a shipping label. Upon completion of the 2 tasks, the workflow outpust the trackking number generated in the 2nd task. If the workflow fails, a second workflow named shipping_issues is run. Fields in a Workflow \u00b6 Field Description Notes name Name of the workflow description Description of the workflow optional version Numeric field used to identify the version of the schema. Use incrementing numbers When starting a workflow execution, if not specified, the definition with highest version is used tasks An array of task definitions. Task properties inputParameters List of input parameters. Used for documenting the required inputs to workflow optional inputTemplate Default input values. See Using inputTemplate optional outputParameters JSON template used to generate the output of the workflow If not specified, the output is defined as the output of the last executed task failureWorkflow String; Workflow to be run on current Workflow failure. Useful for cleanup or post actions on failure. optional schemaVersion Current Conductor Schema version. schemaVersion 1 is discontinued. Must be 2 restartable Boolean flag to allow Workflow restarts defaults to true workflowStatusListenerEnabled If true, every workflow that gets terminated or completed will send a notification. See workflow notifictions optional (false by default) Tasks within Workflow \u00b6 tasks property in a workflow execution defines an array of tasks to be executed in that order. Field Description Notes name Name of the task. MUST be registered as a task with Conductor before starting the workflow taskReferenceName Alias used to refer the task within the workflow. MUST be unique within workflow. type Type of task. SIMPLE for tasks executed by remote workers, or one of the system task types description Description of the task optional optional true or false. When set to true - workflow continues even if the task fails. The status of the task is reflected as COMPLETED_WITH_ERRORS Defaults to false inputParameters JSON template that defines the input given to the task See Wiring Inputs and Outputs for details domain See Task Domains for more information. optional In addition to these parameters, System Tasks have their own parameters. Checkout System Tasks for more information. Wiring Inputs and Outputs \u00b6 Workflows are supplied inputs by client when a new execution is triggered. Workflow input is a JSON payload that is available via ${workflow.input...} expressions. Each task in the workflow is given input based on the inputParameters template configured in workflow definition. inputParameters is a JSON fragment with value containing parameters for mapping values from input or output of a workflow or another task during the execution. Syntax for mapping the values follows the pattern as: ${SOURCE.input/output.JSONPath} field description SOURCE can be either \"workflow\" or any of the task reference name input/output refers to either the input or output of the source JSONPath JSON path expression to extract JSON fragment from source's input/output JSON Path Support Conductor supports JSONPath specification and uses Java implementation from here . Escaping expressions To escape an expression, prefix it with an extra $ character (ex.: $${workflow.input...} ). Example Consider a task with input configured to use input/output parameters from workflow and a task named loc_task . { \"inputParameters\": { \"movieId\": \"${workflow.input.movieId}\", \"url\": \"${workflow.input.fileLocation}\", \"lang\": \"${loc_task.output.languages[0]}\", \"http_request\": { \"method\": \"POST\", \"url\": \"http://example.com/${loc_task.output.fileId}/encode\", \"body\": { \"recipe\": \"${workflow.input.recipe}\", \"params\": { \"width\": 100, \"height\": 100 } }, \"headers\": { \"Accept\": \"application/json\", \"Content-Type\": \"application/json\" } } } } Consider the following as the workflow input { \"movieId\": \"movie_123\", \"fileLocation\":\"s3://moviebucket/file123\", \"recipe\":\"png\" } And the output of the loc_task as the following; { \"fileId\": \"file_xxx_yyy_zzz\", \"languages\": [\"en\",\"ja\",\"es\"] } When scheduling the task, Conductor will merge the values from workflow input and loc_task's output and create the input to the task as follows: { \"movieId\": \"movie_123\", \"url\": \"s3://moviebucket/file123\", \"lang\": \"en\", \"http_request\": { \"method\": \"POST\", \"url\": \"http://example.com/file_xxx_yyy_zzz/encode\", \"body\": { \"recipe\": \"png\", \"params\": { \"width\": 100, \"height\": 100 } }, \"headers\": { \"Accept\": \"application/json\", \"Content-Type\": \"application/json\" } } } Using inputTemplate \u00b6 inputTemplate allows to define default values, which can be overridden by values provided in Workflow. Eg: In your Workflow Definition, you can define your inputTemplate as: \"inputTemplate\": { \"url\": \"https://some_url:7004\" } And url would be https://some_url:7004 if no url was provided as input to your workflow. Workflow notifications \u00b6 Conductor can be configured to publish notifications to external systems upon completion/termination of workflows. See extending conductor for details.","title":"Workflow Definition"},{"location":"configuration/workflowdef/#workflows","text":"","title":"Workflows"},{"location":"configuration/workflowdef/#what-are-workflows","text":"At a high level, a workflow is the Conductor primitive that encompasses the definition and flow of your business logic. A workflow is a collection (graph) of tasks and sub-workflows. A workflow definition specifies the order of execution of these Tasks . It also specifies how data/state is passed from one task to the other (using the input/output parameters). These are then combined to give you the final result. This orchestration of Tasks can happen in a hybrid ecosystem that includes microservices, serverless functions, and monolithic applications. They can also span across any public cloud and on-premise data center footprints. In addition, the orchestration of tasks can be across any programming language since Conductor is also language agnostic. One key benefit of this approach is that you can build a complex application using simple and granular tasks that do not need to be aware of or keep track of the state of your application's execution flow. Conductor keeps track of the state, calls tasks in the right order (sequentially or in parallel, as defined by you), retry calls if needed, handle failure scenarios gracefully, and outputs the final result. Leveraging workflows in Conductor enables developers to truly focus on their core mission - building their application code in the languages of their choice. Conductor does the heavy lifting associated with ensuring high reliability, transactional consistency, and long durability of their workflows. Simply put, wherever your application's component lives and whichever languages they were written in, you can build a workflow in Conductor to orchestrate their execution in a reliable & scalable manner.","title":"What are Workflows?"},{"location":"configuration/workflowdef/#what-does-a-workflow-look-like","text":"Let's start with a basic workflow and understand what are the different aspects of it. In particular, we will talk about two stages of a workflow, defining a workflow and executing a workflow","title":"What does a Workflow look like?"},{"location":"configuration/workflowdef/#simple-workflow-example","text":"Assume your business logic is to simply to get some shipping information and then do the shipping. You start by logically partitioning them into two tasks: shipping_info shipping_task First we would build these two task definitions. Let's assume that shipping info takes an account number, and returns a name and address. Example { \"name\": \"mail_a_box\", \"description\": \"shipping Workflow\", \"version\": 1, \"tasks\": [ { \"name\": \"shipping_info\", \"taskReferenceName\": \"shipping_info_ref\", \"inputParameters\": { \"account\": \"${workflow.input.accountNumber}\" }, \"type\": \"SIMPLE\" }, { \"name\": \"shipping_task\", \"taskReferenceName\": \"shipping_task_ref\", \"inputParameters\": { \"name\": \"${shipping_info_ref.output.name}\", \"streetAddress\": \"${shipping_info_ref.output.streetAddress}\", \"city\": \"${shipping_info_ref.output.city}\", \"state\": \"${shipping_info_ref.output.state}\", \"zipcode\": \"${shipping_info_ref.output.zipcode}\", }, \"type\": \"SIMPLE\" } ], \"outputParameters\": { \"trackingNumber\": \"${shipping_task_ref.output.trackinNumber}\" }, \"failureWorkflow\": \"shipping_issues\", \"restartable\": true, \"workflowStatusListenerEnabled\": true, \"ownerEmail\": \"devrel@orkes.io\", \"timeoutPolicy\": \"ALERT_ONLY\", \"timeoutSeconds\": 0, \"variables\": {}, \"inputTemplate\": {} } The mail_a_box workflow has 2 tasks: 1. The first task takes the provided account number, and outpus an address. 2. The 2nd task takes the address infom and generates a shipping label. Upon completion of the 2 tasks, the workflow outpust the trackking number generated in the 2nd task. If the workflow fails, a second workflow named shipping_issues is run.","title":"Simple Workflow Example"},{"location":"configuration/workflowdef/#fields-in-a-workflow","text":"Field Description Notes name Name of the workflow description Description of the workflow optional version Numeric field used to identify the version of the schema. Use incrementing numbers When starting a workflow execution, if not specified, the definition with highest version is used tasks An array of task definitions. Task properties inputParameters List of input parameters. Used for documenting the required inputs to workflow optional inputTemplate Default input values. See Using inputTemplate optional outputParameters JSON template used to generate the output of the workflow If not specified, the output is defined as the output of the last executed task failureWorkflow String; Workflow to be run on current Workflow failure. Useful for cleanup or post actions on failure. optional schemaVersion Current Conductor Schema version. schemaVersion 1 is discontinued. Must be 2 restartable Boolean flag to allow Workflow restarts defaults to true workflowStatusListenerEnabled If true, every workflow that gets terminated or completed will send a notification. See workflow notifictions optional (false by default)","title":"Fields in a Workflow"},{"location":"configuration/workflowdef/#tasks-within-workflow","text":"tasks property in a workflow execution defines an array of tasks to be executed in that order. Field Description Notes name Name of the task. MUST be registered as a task with Conductor before starting the workflow taskReferenceName Alias used to refer the task within the workflow. MUST be unique within workflow. type Type of task. SIMPLE for tasks executed by remote workers, or one of the system task types description Description of the task optional optional true or false. When set to true - workflow continues even if the task fails. The status of the task is reflected as COMPLETED_WITH_ERRORS Defaults to false inputParameters JSON template that defines the input given to the task See Wiring Inputs and Outputs for details domain See Task Domains for more information. optional In addition to these parameters, System Tasks have their own parameters. Checkout System Tasks for more information.","title":"Tasks within Workflow"},{"location":"configuration/workflowdef/#wiring-inputs-and-outputs","text":"Workflows are supplied inputs by client when a new execution is triggered. Workflow input is a JSON payload that is available via ${workflow.input...} expressions. Each task in the workflow is given input based on the inputParameters template configured in workflow definition. inputParameters is a JSON fragment with value containing parameters for mapping values from input or output of a workflow or another task during the execution. Syntax for mapping the values follows the pattern as: ${SOURCE.input/output.JSONPath} field description SOURCE can be either \"workflow\" or any of the task reference name input/output refers to either the input or output of the source JSONPath JSON path expression to extract JSON fragment from source's input/output JSON Path Support Conductor supports JSONPath specification and uses Java implementation from here . Escaping expressions To escape an expression, prefix it with an extra $ character (ex.: $${workflow.input...} ). Example Consider a task with input configured to use input/output parameters from workflow and a task named loc_task . { \"inputParameters\": { \"movieId\": \"${workflow.input.movieId}\", \"url\": \"${workflow.input.fileLocation}\", \"lang\": \"${loc_task.output.languages[0]}\", \"http_request\": { \"method\": \"POST\", \"url\": \"http://example.com/${loc_task.output.fileId}/encode\", \"body\": { \"recipe\": \"${workflow.input.recipe}\", \"params\": { \"width\": 100, \"height\": 100 } }, \"headers\": { \"Accept\": \"application/json\", \"Content-Type\": \"application/json\" } } } } Consider the following as the workflow input { \"movieId\": \"movie_123\", \"fileLocation\":\"s3://moviebucket/file123\", \"recipe\":\"png\" } And the output of the loc_task as the following; { \"fileId\": \"file_xxx_yyy_zzz\", \"languages\": [\"en\",\"ja\",\"es\"] } When scheduling the task, Conductor will merge the values from workflow input and loc_task's output and create the input to the task as follows: { \"movieId\": \"movie_123\", \"url\": \"s3://moviebucket/file123\", \"lang\": \"en\", \"http_request\": { \"method\": \"POST\", \"url\": \"http://example.com/file_xxx_yyy_zzz/encode\", \"body\": { \"recipe\": \"png\", \"params\": { \"width\": 100, \"height\": 100 } }, \"headers\": { \"Accept\": \"application/json\", \"Content-Type\": \"application/json\" } } }","title":"Wiring Inputs and Outputs"},{"location":"configuration/workflowdef/#using-inputtemplate","text":"inputTemplate allows to define default values, which can be overridden by values provided in Workflow. Eg: In your Workflow Definition, you can define your inputTemplate as: \"inputTemplate\": { \"url\": \"https://some_url:7004\" } And url would be https://some_url:7004 if no url was provided as input to your workflow.","title":"Using inputTemplate"},{"location":"configuration/workflowdef/#workflow-notifications","text":"Conductor can be configured to publish notifications to external systems upon completion/termination of workflows. See extending conductor for details.","title":"Workflow notifications"},{"location":"gettingstarted/basicconcepts/","text":"Definitions (aka Metadata or Blueprints) \u00b6 Conductor definitions are like class definitions in OOP paradigm, or templates. You define this once, and use for each workflow execution. Definitions to Executions have 1:N relationship. Tasks \u00b6 Tasks are the building blocks of Workflow. There must be at least one task in a Workflow. Tasks can be categorized into two types: System tasks - executed by Conductor server. Worker tasks - executed by your own workers. Workflow \u00b6 A Workflow is the container of your process flow. It could include several different types of Tasks, Sub-Workflows, inputs and outputs connected to each other, to effectively achieve the desired result. The tasks are either control tasks (fork, conditional etc) or application tasks (e.g. encode a file) that are executed on a remote machine. Detailed description Task Definition \u00b6 Task definitions help define Task level parameters like inputs and outputs, timeouts, retries etc. All tasks need to be registered before they can be used by active workflows. A task can be re-used within multiple workflows. Detailed description System Tasks \u00b6 System tasks are executed within the JVM of the Conductor server and managed by Conductor for its execution and scalability. See Systems tasks for list of available Task types, and instructions for using them. Note Conductor provides an API to create user defined tasks that are executed in the same JVM as the engine. See WorkflowSystemTask interface for details. Worker Tasks \u00b6 Worker tasks are implemented by your application(s) and run in a separate environment from Conductor. The worker tasks can be implemented in any language. These tasks talk to Conductor server via REST/gRPC to poll for tasks and update its status after execution. Worker tasks are identified by task type SIMPLE in the blueprint.","title":"Basic Concepts"},{"location":"gettingstarted/basicconcepts/#definitions-aka-metadata-or-blueprints","text":"Conductor definitions are like class definitions in OOP paradigm, or templates. You define this once, and use for each workflow execution. Definitions to Executions have 1:N relationship.","title":"Definitions (aka Metadata or Blueprints)"},{"location":"gettingstarted/basicconcepts/#tasks","text":"Tasks are the building blocks of Workflow. There must be at least one task in a Workflow. Tasks can be categorized into two types: System tasks - executed by Conductor server. Worker tasks - executed by your own workers.","title":"Tasks"},{"location":"gettingstarted/basicconcepts/#workflow","text":"A Workflow is the container of your process flow. It could include several different types of Tasks, Sub-Workflows, inputs and outputs connected to each other, to effectively achieve the desired result. The tasks are either control tasks (fork, conditional etc) or application tasks (e.g. encode a file) that are executed on a remote machine. Detailed description","title":"Workflow"},{"location":"gettingstarted/basicconcepts/#task-definition","text":"Task definitions help define Task level parameters like inputs and outputs, timeouts, retries etc. All tasks need to be registered before they can be used by active workflows. A task can be re-used within multiple workflows. Detailed description","title":"Task Definition"},{"location":"gettingstarted/basicconcepts/#system-tasks","text":"System tasks are executed within the JVM of the Conductor server and managed by Conductor for its execution and scalability. See Systems tasks for list of available Task types, and instructions for using them. Note Conductor provides an API to create user defined tasks that are executed in the same JVM as the engine. See WorkflowSystemTask interface for details.","title":"System Tasks"},{"location":"gettingstarted/basicconcepts/#worker-tasks","text":"Worker tasks are implemented by your application(s) and run in a separate environment from Conductor. The worker tasks can be implemented in any language. These tasks talk to Conductor server via REST/gRPC to poll for tasks and update its status after execution. Worker tasks are identified by task type SIMPLE in the blueprint.","title":"Worker Tasks"},{"location":"gettingstarted/client/","text":"Conductor tasks that are executed by remote workers communicate over HTTP endpoints/gRPC to poll for the task and update the status of the execution. Client APIs \u00b6 Conductor provides the following java clients to interact with the various APIs Client Usage Metadata Client Register / Update workflow and task definitions Workflow Client Start a new workflow / Get execution status of a workflow Task Client Poll for task / Update task result after execution / Get status of a task Java \u00b6 Worker \u00b6 Conductor provides an automated framework to poll for tasks, manage the execution thread and update the status of the execution back to the server. Implement the Worker interface to execute the task. TaskRunnerConfigurer \u00b6 The TaskRunnerConfigurer can be used to register the worker(s) and initialize the polling loop. Manages the task workers thread pool and server communication (poll and task update). Use the Builder to create an instance of the TaskRunnerConfigurer. The builder accepts the following parameters: Initialize the Builder with the following: TaskClient | TaskClient used to communicate to the Conductor server | | Workers | Workers that will be used for polling work and task execution. | Parameter Description Default withEurekaClient EurekaClient is used to identify if the server is in discovery or not. When the server goes out of discovery, the polling is stopped unless pollOutOfDiscovery is set to true. If passed null, discovery check is not done. provided by platform withThreadCount Number of threads assigned to the workers. Should be at-least the size of taskWorkers to avoid starvation in a busy system. Number of registered workers withSleepWhenRetry Time in milliseconds, for which the thread should sleep when task update call fails, before retrying the operation. 500 withUpdateRetryCount Number of attempts to be made when updating task status when update status call fails. 3 withWorkerNamePrefix String prefix that will be used for all the workers. workflow-worker- withShutdownGracePeriodSeconds Waiting seconds before forcing shutdown of your worker 10 Once an instance is created, call init() method to initialize the TaskPollExecutor and begin the polling and execution of tasks. Note To ensure that the TaskRunnerConfigurer stops polling for tasks when the instance becomes unhealthy, call the provided shutdown() hook in a PreDestroy block. Properties The worker behavior can be further controlled by using these properties: Property Type Description Default paused boolean If set to true, the worker stops polling. false pollInterval int Interval in milliseconds at which the server should be polled for tasks. 1000 pollOutOfDiscovery boolean If set to true, the instance will poll for tasks regardless of the discovery status. This is useful while running on a dev machine. false Further, these properties can be set either by Worker implementation or by setting the following system properties in the JVM: Name Description conductor.worker.<property> Applies to ALL the workers in the JVM. conductor.worker.<taskDefName>.<property> Applies to the specified worker. Overrides the global property. Examples Sample Worker Implementation Example Python \u00b6 https://github.com/Netflix/conductor/tree/main/polyglot-clients/python Follow the example as documented in the readme or take a look at kitchensink_workers.py Warning Python client is a community contribution. We encourage you to test it out and let us know the feedback. Pull Requests with fixes or enhancements are welcomed!","title":"Using the Client"},{"location":"gettingstarted/client/#client-apis","text":"Conductor provides the following java clients to interact with the various APIs Client Usage Metadata Client Register / Update workflow and task definitions Workflow Client Start a new workflow / Get execution status of a workflow Task Client Poll for task / Update task result after execution / Get status of a task","title":"Client APIs"},{"location":"gettingstarted/client/#java","text":"","title":"Java"},{"location":"gettingstarted/client/#worker","text":"Conductor provides an automated framework to poll for tasks, manage the execution thread and update the status of the execution back to the server. Implement the Worker interface to execute the task.","title":"Worker"},{"location":"gettingstarted/client/#taskrunnerconfigurer","text":"The TaskRunnerConfigurer can be used to register the worker(s) and initialize the polling loop. Manages the task workers thread pool and server communication (poll and task update). Use the Builder to create an instance of the TaskRunnerConfigurer. The builder accepts the following parameters: Initialize the Builder with the following: TaskClient | TaskClient used to communicate to the Conductor server | | Workers | Workers that will be used for polling work and task execution. | Parameter Description Default withEurekaClient EurekaClient is used to identify if the server is in discovery or not. When the server goes out of discovery, the polling is stopped unless pollOutOfDiscovery is set to true. If passed null, discovery check is not done. provided by platform withThreadCount Number of threads assigned to the workers. Should be at-least the size of taskWorkers to avoid starvation in a busy system. Number of registered workers withSleepWhenRetry Time in milliseconds, for which the thread should sleep when task update call fails, before retrying the operation. 500 withUpdateRetryCount Number of attempts to be made when updating task status when update status call fails. 3 withWorkerNamePrefix String prefix that will be used for all the workers. workflow-worker- withShutdownGracePeriodSeconds Waiting seconds before forcing shutdown of your worker 10 Once an instance is created, call init() method to initialize the TaskPollExecutor and begin the polling and execution of tasks. Note To ensure that the TaskRunnerConfigurer stops polling for tasks when the instance becomes unhealthy, call the provided shutdown() hook in a PreDestroy block. Properties The worker behavior can be further controlled by using these properties: Property Type Description Default paused boolean If set to true, the worker stops polling. false pollInterval int Interval in milliseconds at which the server should be polled for tasks. 1000 pollOutOfDiscovery boolean If set to true, the instance will poll for tasks regardless of the discovery status. This is useful while running on a dev machine. false Further, these properties can be set either by Worker implementation or by setting the following system properties in the JVM: Name Description conductor.worker.<property> Applies to ALL the workers in the JVM. conductor.worker.<taskDefName>.<property> Applies to the specified worker. Overrides the global property. Examples Sample Worker Implementation Example","title":"TaskRunnerConfigurer"},{"location":"gettingstarted/client/#python","text":"https://github.com/Netflix/conductor/tree/main/polyglot-clients/python Follow the example as documented in the readme or take a look at kitchensink_workers.py Warning Python client is a community contribution. We encourage you to test it out and let us know the feedback. Pull Requests with fixes or enhancements are welcomed!","title":"Python"},{"location":"gettingstarted/startworkflow/","text":"Start Workflow Request \u00b6 When starting a Workflow execution with a registered definition, Workflow accepts following parameters: Field Description Notes name Name of the Workflow. MUST be registered with Conductor before starting workflow version Workflow version defaults to latest available version input JSON object with key value params, that can be used by downstream tasks See Wiring Inputs and Outputs for details correlationId Unique Id that correlates multiple Workflow executions optional taskToDomain See Task Domains for more information. optional workflowDef An adhoc Workflow Definition to run, without registering. See Dynamic Workflows . optional externalInputPayloadStoragePath This is taken care of by Java client. See External Payload Storage for more info. optional priority Priority level for the tasks within this workflow execution. Possible values are between 0 - 99. optional Example: Send a POST request to /workflow with payload like: { \"name\": \"encode_and_deploy\", \"version\": 1, \"correlationId\": \"my_unique_correlation_id\", \"input\": { \"param1\": \"value1\", \"param2\": \"value2\" } } Dynamic Workflows \u00b6 If the need arises to run a one-time workflow, and it doesn't make sense to register Task and Workflow definitions in Conductor Server, as it could change dynamically for each execution, dynamic workflow executions can be used. This enables you to provide a workflow definition embedded with the required task definitions to the Start Workflow Request in the workflowDef parameter, avoiding the need to register the blueprints before execution. Example: Send a POST request to /workflow with payload like: { \"name\": \"my_adhoc_unregistered_workflow\", \"workflowDef\": { \"ownerApp\": \"my_owner_app\", \"ownerEmail\": \"my_owner_email@test.com\", \"createdBy\": \"my_username\", \"name\": \"my_adhoc_unregistered_workflow\", \"description\": \"Test Workflow setup\", \"version\": 1, \"tasks\": [ { \"name\": \"fetch_data\", \"type\": \"HTTP\", \"taskReferenceName\": \"fetch_data\", \"inputParameters\": { \"http_request\": { \"connectionTimeOut\": \"3600\", \"readTimeOut\": \"3600\", \"uri\": \"${workflow.input.uri}\", \"method\": \"GET\", \"accept\": \"application/json\", \"content-Type\": \"application/json\", \"headers\": { } } }, \"taskDefinition\": { \"name\": \"fetch_data\", \"retryCount\": 0, \"timeoutSeconds\": 3600, \"timeoutPolicy\": \"TIME_OUT_WF\", \"retryLogic\": \"FIXED\", \"retryDelaySeconds\": 0, \"responseTimeoutSeconds\": 3000 } } ], \"outputParameters\": { } }, \"input\": { \"uri\": \"http://www.google.com\" } } Note If the taskDefinition is defined with Metadata API, it doesn't have to be added in above dynamic workflow definition.","title":"Start a Workflow"},{"location":"gettingstarted/startworkflow/#start-workflow-request","text":"When starting a Workflow execution with a registered definition, Workflow accepts following parameters: Field Description Notes name Name of the Workflow. MUST be registered with Conductor before starting workflow version Workflow version defaults to latest available version input JSON object with key value params, that can be used by downstream tasks See Wiring Inputs and Outputs for details correlationId Unique Id that correlates multiple Workflow executions optional taskToDomain See Task Domains for more information. optional workflowDef An adhoc Workflow Definition to run, without registering. See Dynamic Workflows . optional externalInputPayloadStoragePath This is taken care of by Java client. See External Payload Storage for more info. optional priority Priority level for the tasks within this workflow execution. Possible values are between 0 - 99. optional Example: Send a POST request to /workflow with payload like: { \"name\": \"encode_and_deploy\", \"version\": 1, \"correlationId\": \"my_unique_correlation_id\", \"input\": { \"param1\": \"value1\", \"param2\": \"value2\" } }","title":"Start Workflow Request"},{"location":"gettingstarted/startworkflow/#dynamic-workflows","text":"If the need arises to run a one-time workflow, and it doesn't make sense to register Task and Workflow definitions in Conductor Server, as it could change dynamically for each execution, dynamic workflow executions can be used. This enables you to provide a workflow definition embedded with the required task definitions to the Start Workflow Request in the workflowDef parameter, avoiding the need to register the blueprints before execution. Example: Send a POST request to /workflow with payload like: { \"name\": \"my_adhoc_unregistered_workflow\", \"workflowDef\": { \"ownerApp\": \"my_owner_app\", \"ownerEmail\": \"my_owner_email@test.com\", \"createdBy\": \"my_username\", \"name\": \"my_adhoc_unregistered_workflow\", \"description\": \"Test Workflow setup\", \"version\": 1, \"tasks\": [ { \"name\": \"fetch_data\", \"type\": \"HTTP\", \"taskReferenceName\": \"fetch_data\", \"inputParameters\": { \"http_request\": { \"connectionTimeOut\": \"3600\", \"readTimeOut\": \"3600\", \"uri\": \"${workflow.input.uri}\", \"method\": \"GET\", \"accept\": \"application/json\", \"content-Type\": \"application/json\", \"headers\": { } } }, \"taskDefinition\": { \"name\": \"fetch_data\", \"retryCount\": 0, \"timeoutSeconds\": 3600, \"timeoutPolicy\": \"TIME_OUT_WF\", \"retryLogic\": \"FIXED\", \"retryDelaySeconds\": 0, \"responseTimeoutSeconds\": 3000 } } ], \"outputParameters\": { } }, \"input\": { \"uri\": \"http://www.google.com\" } } Note If the taskDefinition is defined with Metadata API, it doesn't have to be added in above dynamic workflow definition.","title":"Dynamic Workflows"},{"location":"how-tos/archival-of-workflows/","text":"Archival Of Workflows \u00b6 Conductor has support for archiving workflow upon termination or completion. Enabling this will delete the workflow from the configured database, but leave the associated data in Elasticsearch so it is still searchable. To enable, set the conductor.workflow-status-listener.type property to archive . A number of additional properties are available to control archival. Property Default Value Description conductor.workflow-status-listener.archival.ttlDuration 0s The time to live in seconds for workflow archiving module. Currently, only RedisExecutionDAO supports this conductor.workflow-status-listener.archival.delayQueueWorkerThreadCount 5 The number of threads to process the delay queue in workflow archival conductor.workflow-status-listener.archival.delaySeconds 60 The time to delay the archival of workflow","title":"Archival of Workflows"},{"location":"how-tos/archival-of-workflows/#archival-of-workflows","text":"Conductor has support for archiving workflow upon termination or completion. Enabling this will delete the workflow from the configured database, but leave the associated data in Elasticsearch so it is still searchable. To enable, set the conductor.workflow-status-listener.type property to archive . A number of additional properties are available to control archival. Property Default Value Description conductor.workflow-status-listener.archival.ttlDuration 0s The time to live in seconds for workflow archiving module. Currently, only RedisExecutionDAO supports this conductor.workflow-status-listener.archival.delayQueueWorkerThreadCount 5 The number of threads to process the delay queue in workflow archival conductor.workflow-status-listener.archival.delaySeconds 60 The time to delay the archival of workflow","title":"Archival Of Workflows"},{"location":"how-tos/build-a-nodejs-task-worker/","text":"Build a Node.js Task Worker \u00b6 TODO Summary \u00b6 TODO","title":"Build a Node.js Task Worker"},{"location":"how-tos/build-a-nodejs-task-worker/#build-a-nodejs-task-worker","text":"TODO","title":"Build a Node.js Task Worker"},{"location":"how-tos/build-a-nodejs-task-worker/#summary","text":"TODO","title":"Summary"},{"location":"how-tos/conductor-configurations/","text":"Conductor Configurations \u00b6 TODO Summary \u00b6 TODO","title":"Conductor Configurations"},{"location":"how-tos/conductor-configurations/#conductor-configurations","text":"TODO","title":"Conductor Configurations"},{"location":"how-tos/conductor-configurations/#summary","text":"TODO","title":"Summary"},{"location":"how-tos/configuring-metrics/","text":"Configuring Metrics \u00b6 TODO Summary \u00b6 TODO","title":"Configuring Metrics"},{"location":"how-tos/configuring-metrics/#configuring-metrics","text":"TODO","title":"Configuring Metrics"},{"location":"how-tos/configuring-metrics/#summary","text":"TODO","title":"Summary"},{"location":"how-tos/golang-sdk/","text":"Golang SDK \u00b6 TODO Summary \u00b6 TODO","title":"Golang SDK"},{"location":"how-tos/golang-sdk/#golang-sdk","text":"TODO","title":"Golang SDK"},{"location":"how-tos/golang-sdk/#summary","text":"TODO","title":"Summary"},{"location":"how-tos/idempotent-tasks/","text":"Idempotency \u00b6 TODO Summary \u00b6 TODO","title":"Idempotency"},{"location":"how-tos/idempotent-tasks/#idempotency","text":"TODO","title":"Idempotency"},{"location":"how-tos/idempotent-tasks/#summary","text":"TODO","title":"Summary"},{"location":"how-tos/java-sdk/","text":"Java SDK \u00b6 TODO Summary \u00b6 TODO","title":"Java SDK"},{"location":"how-tos/java-sdk/#java-sdk","text":"TODO","title":"Java SDK"},{"location":"how-tos/java-sdk/#summary","text":"TODO","title":"Summary"},{"location":"how-tos/nodejs-sdk/","text":"Nodejs SDK \u00b6 TODO Summary \u00b6 TODO","title":"Nodejs SDK"},{"location":"how-tos/nodejs-sdk/#nodejs-sdk","text":"TODO","title":"Nodejs SDK"},{"location":"how-tos/nodejs-sdk/#summary","text":"TODO","title":"Summary"},{"location":"how-tos/python-sdk/","text":"Python SDK \u00b6 TODO Summary \u00b6 TODO","title":"Python SDK"},{"location":"how-tos/python-sdk/#python-sdk","text":"TODO","title":"Python SDK"},{"location":"how-tos/python-sdk/#summary","text":"TODO","title":"Summary"},{"location":"how-tos/retry-configurations/","text":"Retry Configurations \u00b6 TODO Summary \u00b6 TODO","title":"Retry Configurations"},{"location":"how-tos/retry-configurations/#retry-configurations","text":"TODO","title":"Retry Configurations"},{"location":"how-tos/retry-configurations/#summary","text":"TODO","title":"Summary"},{"location":"how-tos/scaling-the-system/","text":"Scaling the System \u00b6 TODO Summary \u00b6 TODO","title":"Scaling the System"},{"location":"how-tos/scaling-the-system/#scaling-the-system","text":"TODO","title":"Scaling the System"},{"location":"how-tos/scaling-the-system/#summary","text":"TODO","title":"Summary"},{"location":"how-tos/timeouts/","text":"Timeouts \u00b6 TODO Summary \u00b6 TODO","title":"Timeouts"},{"location":"how-tos/timeouts/#timeouts","text":"TODO","title":"Timeouts"},{"location":"how-tos/timeouts/#summary","text":"TODO","title":"Summary"},{"location":"how-tos/versioning-workflows/","text":"Versioning Workflows \u00b6 TODO Summary \u00b6 TODO","title":"Versioning Workflows"},{"location":"how-tos/versioning-workflows/#versioning-workflows","text":"TODO","title":"Versioning Workflows"},{"location":"how-tos/versioning-workflows/#summary","text":"TODO","title":"Summary"},{"location":"how-tos/Monitoring/Conductor-LogLevel/","text":"Conductor Log Level \u00b6 Conductor is based on Spring Boot, so the log levels are set via Spring Boot properties : From the Spring Boot Docs: All the supported logging systems can have the logger levels set in the Spring Environment (for example, in application.properties) by using logging.level.<logger-name>=<level> where level is one of TRACE, DEBUG, INFO, WARN, ERROR, FATAL, or OFF. The root logger can be configured by using logging.level.root. The following example shows potential logging settings in application.properties : logging.level.root=warn logging.level.org.springframework.web=debug logging.level.org.hibernate=error It\u2019s also possible to set logging levels using environment variables. For example, LOGGING_LEVEL_ORG_SPRINGFRAMEWORK_WEB=DEBUG will set org.springframework.web to DEBUG .","title":"Conductor Log Level"},{"location":"how-tos/Monitoring/Conductor-LogLevel/#conductor-log-level","text":"Conductor is based on Spring Boot, so the log levels are set via Spring Boot properties : From the Spring Boot Docs: All the supported logging systems can have the logger levels set in the Spring Environment (for example, in application.properties) by using logging.level.<logger-name>=<level> where level is one of TRACE, DEBUG, INFO, WARN, ERROR, FATAL, or OFF. The root logger can be configured by using logging.level.root. The following example shows potential logging settings in application.properties : logging.level.root=warn logging.level.org.springframework.web=debug logging.level.org.hibernate=error It\u2019s also possible to set logging levels using environment variables. For example, LOGGING_LEVEL_ORG_SPRINGFRAMEWORK_WEB=DEBUG will set org.springframework.web to DEBUG .","title":"Conductor Log Level"},{"location":"how-tos/Tasks/creating-tasks/","text":"Creating Task Definitions \u00b6 Tasks can be created using the tasks metadata API ```http request POST /api/metadata/taskdefs This API takes an array of new task definitions. ### Example using curl ```shell curl 'http://localhost:8080/api/metadata/taskdefs' \\ -H 'accept: */*' \\ -H 'content-type: application/json' \\ --data-raw '[{\"createdBy\":\"user\",\"name\":\"sample_task_name_1\",\"description\":\"This is a sample task for demo\",\"responseTimeoutSeconds\":10,\"timeoutSeconds\":30,\"inputKeys\":[],\"outputKeys\":[],\"timeoutPolicy\":\"TIME_OUT_WF\",\"retryCount\":3,\"retryLogic\":\"FIXED\",\"retryDelaySeconds\":5,\"inputTemplate\":{},\"rateLimitPerFrequency\":0,\"rateLimitFrequencyInSeconds\":1}]' Example using node fetch \u00b6 fetch(\"http://localhost:8080/api/metadata/taskdefs\", { \"headers\": { \"accept\": \"*/*\", \"content-type\": \"application/json\", }, \"body\": \"[{\\\"createdBy\\\":\\\"user\\\",\\\"name\\\":\\\"sample_task_name_1\\\",\\\"description\\\":\\\"This is a sample task for demo\\\",\\\"responseTimeoutSeconds\\\":10,\\\"timeoutSeconds\\\":30,\\\"inputKeys\\\":[],\\\"outputKeys\\\":[],\\\"timeoutPolicy\\\":\\\"TIME_OUT_WF\\\",\\\"retryCount\\\":3,\\\"retryLogic\\\":\\\"FIXED\\\",\\\"retryDelaySeconds\\\":5,\\\"inputTemplate\\\":{},\\\"rateLimitPerFrequency\\\":0,\\\"rateLimitFrequencyInSeconds\\\":1}]\", \"method\": \"POST\" }); Best Practices \u00b6 You can update a set of tasks together in this API Task configurations are important attributes that controls the behavior of this task in a Workflow. Refer to Task Configurations for all the options and details' You can also use the Conductor Swagger UI to update the tasks","title":"Creating Task Definitions"},{"location":"how-tos/Tasks/creating-tasks/#creating-task-definitions","text":"Tasks can be created using the tasks metadata API ```http request POST /api/metadata/taskdefs This API takes an array of new task definitions. ### Example using curl ```shell curl 'http://localhost:8080/api/metadata/taskdefs' \\ -H 'accept: */*' \\ -H 'content-type: application/json' \\ --data-raw '[{\"createdBy\":\"user\",\"name\":\"sample_task_name_1\",\"description\":\"This is a sample task for demo\",\"responseTimeoutSeconds\":10,\"timeoutSeconds\":30,\"inputKeys\":[],\"outputKeys\":[],\"timeoutPolicy\":\"TIME_OUT_WF\",\"retryCount\":3,\"retryLogic\":\"FIXED\",\"retryDelaySeconds\":5,\"inputTemplate\":{},\"rateLimitPerFrequency\":0,\"rateLimitFrequencyInSeconds\":1}]'","title":"Creating Task Definitions"},{"location":"how-tos/Tasks/creating-tasks/#example-using-node-fetch","text":"fetch(\"http://localhost:8080/api/metadata/taskdefs\", { \"headers\": { \"accept\": \"*/*\", \"content-type\": \"application/json\", }, \"body\": \"[{\\\"createdBy\\\":\\\"user\\\",\\\"name\\\":\\\"sample_task_name_1\\\",\\\"description\\\":\\\"This is a sample task for demo\\\",\\\"responseTimeoutSeconds\\\":10,\\\"timeoutSeconds\\\":30,\\\"inputKeys\\\":[],\\\"outputKeys\\\":[],\\\"timeoutPolicy\\\":\\\"TIME_OUT_WF\\\",\\\"retryCount\\\":3,\\\"retryLogic\\\":\\\"FIXED\\\",\\\"retryDelaySeconds\\\":5,\\\"inputTemplate\\\":{},\\\"rateLimitPerFrequency\\\":0,\\\"rateLimitFrequencyInSeconds\\\":1}]\", \"method\": \"POST\" });","title":"Example using node fetch"},{"location":"how-tos/Tasks/creating-tasks/#best-practices","text":"You can update a set of tasks together in this API Task configurations are important attributes that controls the behavior of this task in a Workflow. Refer to Task Configurations for all the options and details' You can also use the Conductor Swagger UI to update the tasks","title":"Best Practices"},{"location":"how-tos/Tasks/dynamic-vs-switch-tasks/","text":"Dynamic vs Switch Tasks \u00b6 Learn more about Dynamic Tasks Switch Tasks Dynamic Tasks are useful in situations when need to run a task of which the task type is determined at runtime instead of during the configuration. It is similar to the SWITCH use case but with DYNAMIC we won't need to preconfigure all case options in the workflow definition itself. Instead, we can mark the task as DYNAMIC and determine which underlying task does it run during the workflow execution itself. Use DYNAMIC task as a replacement for SWITCH if you have too many case options DYNAMIC task is an option when you want to programmatically determine the next task to run instead of using expressions DYNAMIC task simplifies the workflow execution UI view which will now only show the selected task SWITCH task visualization is helpful as a documentation - showing you all options that the workflow could have taken SWITCH task comes with a default task option which can be useful in some use cases","title":"Dynamic vs Switch Tasks"},{"location":"how-tos/Tasks/dynamic-vs-switch-tasks/#dynamic-vs-switch-tasks","text":"Learn more about Dynamic Tasks Switch Tasks Dynamic Tasks are useful in situations when need to run a task of which the task type is determined at runtime instead of during the configuration. It is similar to the SWITCH use case but with DYNAMIC we won't need to preconfigure all case options in the workflow definition itself. Instead, we can mark the task as DYNAMIC and determine which underlying task does it run during the workflow execution itself. Use DYNAMIC task as a replacement for SWITCH if you have too many case options DYNAMIC task is an option when you want to programmatically determine the next task to run instead of using expressions DYNAMIC task simplifies the workflow execution UI view which will now only show the selected task SWITCH task visualization is helpful as a documentation - showing you all options that the workflow could have taken SWITCH task comes with a default task option which can be useful in some use cases","title":"Dynamic vs Switch Tasks"},{"location":"how-tos/Tasks/monitoring-task-queues/","text":"Monitoring Task Queues \u00b6 Conductor offers an API and UI interface to monitor the task queues. This is useful to see details of the number of workers polling and monitoring the queue backlog. Using the UI \u00b6 ```http request /taskQueue Access this screen via - Home > Task Queues On this screen you can select and view the details of the task queue. The following information is shown: 1. Queue Size - The number of tasks waiting to be executed 2. Workers - The count and list of works and their instance reference who are polling for work for this task ### Using APIs To see the size of the task queue via API: ```shell curl 'http://localhost:8080/api/tasks/queue/sizes?taskType=<TASK_NAME>' \\ -H 'accept: */*' To see the worker poll information of the task queue via API: curl 'http://localhost:8080/api/tasks/queue/polldata?taskType=<TASK_NAME>' \\ -H 'accept: */*' Replace <TASK_NAME> with your task name","title":"Monitoring Task Queues"},{"location":"how-tos/Tasks/monitoring-task-queues/#monitoring-task-queues","text":"Conductor offers an API and UI interface to monitor the task queues. This is useful to see details of the number of workers polling and monitoring the queue backlog.","title":"Monitoring Task Queues"},{"location":"how-tos/Tasks/monitoring-task-queues/#using-the-ui","text":"```http request /taskQueue Access this screen via - Home > Task Queues On this screen you can select and view the details of the task queue. The following information is shown: 1. Queue Size - The number of tasks waiting to be executed 2. Workers - The count and list of works and their instance reference who are polling for work for this task ### Using APIs To see the size of the task queue via API: ```shell curl 'http://localhost:8080/api/tasks/queue/sizes?taskType=<TASK_NAME>' \\ -H 'accept: */*' To see the worker poll information of the task queue via API: curl 'http://localhost:8080/api/tasks/queue/polldata?taskType=<TASK_NAME>' \\ -H 'accept: */*' Replace <TASK_NAME> with your task name","title":"Using the UI"},{"location":"how-tos/Tasks/reusing-tasks/","text":"Reusing Tasks \u00b6 A powerful feature of Conductor is that it supports and enables re-usability out of the box. Task workers typically perform a unit of work and is usually a part of a larger workflow. Such workers are often re-usable in multiple workflows. Once a task is defined, you can use it across as any workflow. When re-using tasks, it's important to think of situations that a multi-tenant system faces. All the work assigned to this worker by default goes to the same task scheduling queue. This could result in your worker not being polled quickly if there is a noisy neighbour in the ecosystem. One way you can tackle this situation is by re-using the worker code, but having different task names registered for different use cases. And for each task name, you can run an appropriate number of workers based on expected load.","title":"Reusing Tasks"},{"location":"how-tos/Tasks/reusing-tasks/#reusing-tasks","text":"A powerful feature of Conductor is that it supports and enables re-usability out of the box. Task workers typically perform a unit of work and is usually a part of a larger workflow. Such workers are often re-usable in multiple workflows. Once a task is defined, you can use it across as any workflow. When re-using tasks, it's important to think of situations that a multi-tenant system faces. All the work assigned to this worker by default goes to the same task scheduling queue. This could result in your worker not being polled quickly if there is a noisy neighbour in the ecosystem. One way you can tackle this situation is by re-using the worker code, but having different task names registered for different use cases. And for each task name, you can run an appropriate number of workers based on expected load.","title":"Reusing Tasks"},{"location":"how-tos/Tasks/task-configurations/","text":"Task Configurations \u00b6 Refer to Task Definitions for details on how to configure task definitions Example \u00b6 Here is a task template payload with commonly used fields: { \"createdBy\": \"user\", \"name\": \"sample_task_name_1\", \"description\": \"This is a sample task for demo\", \"responseTimeoutSeconds\": 10, \"timeoutSeconds\": 30, \"inputKeys\": [], \"outputKeys\": [], \"timeoutPolicy\": \"TIME_OUT_WF\", \"retryCount\": 3, \"retryLogic\": \"FIXED\", \"retryDelaySeconds\": 5, \"inputTemplate\": {}, \"rateLimitPerFrequency\": 0, \"rateLimitFrequencyInSeconds\": 1 } Best Practices \u00b6 Refer to Task Timeouts for additional information on how the various timeout settings work Refer to Monitoring Task Queues on how to monitor task queues","title":"Task Configurations"},{"location":"how-tos/Tasks/task-configurations/#task-configurations","text":"Refer to Task Definitions for details on how to configure task definitions","title":"Task Configurations"},{"location":"how-tos/Tasks/task-configurations/#example","text":"Here is a task template payload with commonly used fields: { \"createdBy\": \"user\", \"name\": \"sample_task_name_1\", \"description\": \"This is a sample task for demo\", \"responseTimeoutSeconds\": 10, \"timeoutSeconds\": 30, \"inputKeys\": [], \"outputKeys\": [], \"timeoutPolicy\": \"TIME_OUT_WF\", \"retryCount\": 3, \"retryLogic\": \"FIXED\", \"retryDelaySeconds\": 5, \"inputTemplate\": {}, \"rateLimitPerFrequency\": 0, \"rateLimitFrequencyInSeconds\": 1 }","title":"Example"},{"location":"how-tos/Tasks/task-configurations/#best-practices","text":"Refer to Task Timeouts for additional information on how the various timeout settings work Refer to Monitoring Task Queues on how to monitor task queues","title":"Best Practices"},{"location":"how-tos/Tasks/task-inputs/","text":"Task Inputs \u00b6 Task inputs can be provided in multiple ways. This is configured in the workflow definition when a task is participating in the workflow. Inputs referred from Workflow inputs \u00b6 When we start a workflow, we can provide inputs to the workflow in a json format. For example: { \"worfklowInputNumberExample\": 1, \"worfklowInputTextExample\": \"SAMPLE\", \"worfklowInputJsonExample\": { \"nestedKey\": \"nestedValue\" } } These values can be referred as inputs into your task using the following expression: { \"taskInput1Key\": \"${workflow.input.worfklowInputNumberExample}\", \"taskInput2Key\": \"${workflow.input.worfklowInputJsonExample.nestedKey}\" } In this example, the tasks will receive the following inputs after they are evaluated: { \"taskInput1Key\": 1, \"taskInput2Key\": \"nestedValue\" } Inputs referred from other Task outputs \u00b6 Similar to how we can refer to workflow inputs, we can also refer to an output field that was generated by a task that executed before. Let's assume a task with the task reference name previousTaskReference executed and produced the following output: { \"taskOutputKey1\": \"outputValue\", \"taskOutputKey2\": { \"nestedKey1\": \"outputValue-1\" } } We can refer to these as the new task's input by using the following expression: { \"taskInput1Key\": \"${previousTaskReference.output.taskOutputKey1}\", \"taskInput2Key\": \"${previousTaskReference.output.taskOutputKey2.nestedKey1}\" } The expression format is based on Json Path and you can construct complex input params based on the syntax. Hard coded inputs \u00b6 Task inputs can also be hard coded in the workflow definitions. This is useful when you have a re-usable task which has configurable options that can be applied in different workflow contexts. { \"taskInput1\": \"OPTION_A\", \"taskInput2\": 100 }","title":"Task Inputs"},{"location":"how-tos/Tasks/task-inputs/#task-inputs","text":"Task inputs can be provided in multiple ways. This is configured in the workflow definition when a task is participating in the workflow.","title":"Task Inputs"},{"location":"how-tos/Tasks/task-inputs/#inputs-referred-from-workflow-inputs","text":"When we start a workflow, we can provide inputs to the workflow in a json format. For example: { \"worfklowInputNumberExample\": 1, \"worfklowInputTextExample\": \"SAMPLE\", \"worfklowInputJsonExample\": { \"nestedKey\": \"nestedValue\" } } These values can be referred as inputs into your task using the following expression: { \"taskInput1Key\": \"${workflow.input.worfklowInputNumberExample}\", \"taskInput2Key\": \"${workflow.input.worfklowInputJsonExample.nestedKey}\" } In this example, the tasks will receive the following inputs after they are evaluated: { \"taskInput1Key\": 1, \"taskInput2Key\": \"nestedValue\" }","title":"Inputs referred from Workflow inputs"},{"location":"how-tos/Tasks/task-inputs/#inputs-referred-from-other-task-outputs","text":"Similar to how we can refer to workflow inputs, we can also refer to an output field that was generated by a task that executed before. Let's assume a task with the task reference name previousTaskReference executed and produced the following output: { \"taskOutputKey1\": \"outputValue\", \"taskOutputKey2\": { \"nestedKey1\": \"outputValue-1\" } } We can refer to these as the new task's input by using the following expression: { \"taskInput1Key\": \"${previousTaskReference.output.taskOutputKey1}\", \"taskInput2Key\": \"${previousTaskReference.output.taskOutputKey2.nestedKey1}\" } The expression format is based on Json Path and you can construct complex input params based on the syntax.","title":"Inputs referred from other Task outputs"},{"location":"how-tos/Tasks/task-inputs/#hard-coded-inputs","text":"Task inputs can also be hard coded in the workflow definitions. This is useful when you have a re-usable task which has configurable options that can be applied in different workflow contexts. { \"taskInput1\": \"OPTION_A\", \"taskInput2\": 100 }","title":"Hard coded inputs"},{"location":"how-tos/Tasks/task-timeouts/","text":"Task Timeouts \u00b6 Tasks can be configured to handle various scenarios of timeouts. Here are some scenarios and the relevance configuration fields. Scenario Configuration A task worker picked up the task, but fails to respond back with an update responseTimeoutSeconds A task worker picked up the task and updates progress, but fails to complete within an expected timeframe timeoutSeconds A task is stuck in a retry loop with repeated failures beyond an expected timeframe timeoutSeconds Task doesn't get picked by any workers for a specific amount of time pollTimeoutSeconds Task isn't completed within a specified amount of time despite being picked up by task workers timeoutSeconds timeoutSeconds should always be greater than responseTimeoutSeconds Timeout Seconds \u00b6 \"timeoutSeconds\" : 30 When configured with a value > 0 , the system will wait for this task to complete successfully up until this number of seconds from when the task is first polled. We can use this to fail a workflow when a task breaches the overall SLA for completion. Response Timeout Seconds \u00b6 \"responseTimeoutSeconds\" : 10 When configured with a value > 0 , the system will wait for this number of seconds from when the task is polled before the worker updates back with a status. The worker can keep the task in IN_PROGRESS state if it requires more time to complete. Poll Timeout Seconds \u00b6 \"pollTimeoutSeconds\" : 10 When configured with a value > 0 , the system will wait for this number of seconds for the task to be picked up by a task worker. Useful when you want to detect a backlogged task queue with not enough workers.","title":"Task Timeouts"},{"location":"how-tos/Tasks/task-timeouts/#task-timeouts","text":"Tasks can be configured to handle various scenarios of timeouts. Here are some scenarios and the relevance configuration fields. Scenario Configuration A task worker picked up the task, but fails to respond back with an update responseTimeoutSeconds A task worker picked up the task and updates progress, but fails to complete within an expected timeframe timeoutSeconds A task is stuck in a retry loop with repeated failures beyond an expected timeframe timeoutSeconds Task doesn't get picked by any workers for a specific amount of time pollTimeoutSeconds Task isn't completed within a specified amount of time despite being picked up by task workers timeoutSeconds timeoutSeconds should always be greater than responseTimeoutSeconds","title":"Task Timeouts"},{"location":"how-tos/Tasks/task-timeouts/#timeout-seconds","text":"\"timeoutSeconds\" : 30 When configured with a value > 0 , the system will wait for this task to complete successfully up until this number of seconds from when the task is first polled. We can use this to fail a workflow when a task breaches the overall SLA for completion.","title":"Timeout Seconds"},{"location":"how-tos/Tasks/task-timeouts/#response-timeout-seconds","text":"\"responseTimeoutSeconds\" : 10 When configured with a value > 0 , the system will wait for this number of seconds from when the task is polled before the worker updates back with a status. The worker can keep the task in IN_PROGRESS state if it requires more time to complete.","title":"Response Timeout Seconds"},{"location":"how-tos/Tasks/task-timeouts/#poll-timeout-seconds","text":"\"pollTimeoutSeconds\" : 10 When configured with a value > 0 , the system will wait for this number of seconds for the task to be picked up by a task worker. Useful when you want to detect a backlogged task queue with not enough workers.","title":"Poll Timeout Seconds"},{"location":"how-tos/Tasks/updating-tasks/","text":"Updating Task Definitions \u00b6 Updates to the task definitions can be made using the following API PUT /api/metadata/taskdefs This API takes a single task definition and updates itself. Example using curl \u00b6 curl 'http://localhost:8080/api/metadata/taskdefs' \\ -X 'PUT' \\ -H 'accept: */*' \\ -H 'content-type: application/json' \\ --data-raw '{\"createdBy\":\"user\",\"name\":\"sample_task_name_1\",\"description\":\"This is a sample task for demo\",\"responseTimeoutSeconds\":10,\"timeoutSeconds\":30,\"inputKeys\":[],\"outputKeys\":[],\"timeoutPolicy\":\"TIME_OUT_WF\",\"retryCount\":3,\"retryLogic\":\"FIXED\",\"retryDelaySeconds\":5,\"inputTemplate\":{},\"rateLimitPerFrequency\":0,\"rateLimitFrequencyInSeconds\":1}' Example using node fetch \u00b6 fetch(\"http://localhost:8080/api/metadata/taskdefs\", { \"headers\": { \"accept\": \"*/*\", \"content-type\": \"application/json\", }, \"body\": \"{\\\"createdBy\\\":\\\"user\\\",\\\"name\\\":\\\"sample_task_name_1\\\",\\\"description\\\":\\\"This is a sample task for demo\\\",\\\"responseTimeoutSeconds\\\":10,\\\"timeoutSeconds\\\":30,\\\"inputKeys\\\":[],\\\"outputKeys\\\":[],\\\"timeoutPolicy\\\":\\\"TIME_OUT_WF\\\",\\\"retryCount\\\":3,\\\"retryLogic\\\":\\\"FIXED\\\",\\\"retryDelaySeconds\\\":5,\\\"inputTemplate\\\":{},\\\"rateLimitPerFrequency\\\":0,\\\"rateLimitFrequencyInSeconds\\\":1}\", \"method\": \"PUT\" }); Best Practices \u00b6 You can also use the Conductor Swagger UI to update the tasks Task configurations are important attributes that controls the behavior of this task in a Workflow. Refer to Task Configurations for all the options and details'","title":"Updating Task Definitions"},{"location":"how-tos/Tasks/updating-tasks/#updating-task-definitions","text":"Updates to the task definitions can be made using the following API PUT /api/metadata/taskdefs This API takes a single task definition and updates itself.","title":"Updating Task Definitions"},{"location":"how-tos/Tasks/updating-tasks/#example-using-curl","text":"curl 'http://localhost:8080/api/metadata/taskdefs' \\ -X 'PUT' \\ -H 'accept: */*' \\ -H 'content-type: application/json' \\ --data-raw '{\"createdBy\":\"user\",\"name\":\"sample_task_name_1\",\"description\":\"This is a sample task for demo\",\"responseTimeoutSeconds\":10,\"timeoutSeconds\":30,\"inputKeys\":[],\"outputKeys\":[],\"timeoutPolicy\":\"TIME_OUT_WF\",\"retryCount\":3,\"retryLogic\":\"FIXED\",\"retryDelaySeconds\":5,\"inputTemplate\":{},\"rateLimitPerFrequency\":0,\"rateLimitFrequencyInSeconds\":1}'","title":"Example using curl"},{"location":"how-tos/Tasks/updating-tasks/#example-using-node-fetch","text":"fetch(\"http://localhost:8080/api/metadata/taskdefs\", { \"headers\": { \"accept\": \"*/*\", \"content-type\": \"application/json\", }, \"body\": \"{\\\"createdBy\\\":\\\"user\\\",\\\"name\\\":\\\"sample_task_name_1\\\",\\\"description\\\":\\\"This is a sample task for demo\\\",\\\"responseTimeoutSeconds\\\":10,\\\"timeoutSeconds\\\":30,\\\"inputKeys\\\":[],\\\"outputKeys\\\":[],\\\"timeoutPolicy\\\":\\\"TIME_OUT_WF\\\",\\\"retryCount\\\":3,\\\"retryLogic\\\":\\\"FIXED\\\",\\\"retryDelaySeconds\\\":5,\\\"inputTemplate\\\":{},\\\"rateLimitPerFrequency\\\":0,\\\"rateLimitFrequencyInSeconds\\\":1}\", \"method\": \"PUT\" });","title":"Example using node fetch"},{"location":"how-tos/Tasks/updating-tasks/#best-practices","text":"You can also use the Conductor Swagger UI to update the tasks Task configurations are important attributes that controls the behavior of this task in a Workflow. Refer to Task Configurations for all the options and details'","title":"Best Practices"},{"location":"how-tos/Workers/build-a-golang-task-worker/","text":"Build a Go Task Worker \u00b6 Install \u00b6 go get github.com/netflix/conductor/client/go This will create a Go project under $GOPATH/src and download any dependencies. Implementing a Task a Worker \u00b6 task package provies the types used to implement the worker. Here is a reference worker implementation: package task import ( \"fmt\" ) // Implementation for \"task_1\" func Task_1_Execution_Function(t *task.Task) (taskResult *task.TaskResult, err error) { log.Println(\"Executing Task_1_Execution_Function for\", t.TaskType) //Do some logic taskResult = task.NewTaskResult(t) output := map[string]interface{}{\"task\":\"task_1\", \"key2\":\"value2\", \"key3\":3, \"key4\":false} taskResult.OutputData = output taskResult.Status = \"COMPLETED\" err = nil return taskResult, err } Worker Polling \u00b6 Here is an example that shows how to start polling for tasks after defining the tasks. package main import ( \"github.com/netflix/conductor/client/go\" \"github.com/netflix/conductor/client/go/task/sample\" ) func main() { c := conductor.NewConductorWorker(\"http://localhost:8080\", 1, 10000) c.Start(\"task_1\", \"\", sample.Task_1_Execution_Function, false) c.Start(\"task_2\", \"mydomain\", sample.Task_2_Execution_Function, true) } NewConductorWoker parameters \u00b6 baseUrl: Server address. threadCount: No. of threads. Number of threads should be at-least same as the number of workers pollingInterval: Time in millisecond between subsequent polls","title":"-Build a Go Task Worker"},{"location":"how-tos/Workers/build-a-golang-task-worker/#build-a-go-task-worker","text":"","title":"Build a Go Task Worker"},{"location":"how-tos/Workers/build-a-golang-task-worker/#install","text":"go get github.com/netflix/conductor/client/go This will create a Go project under $GOPATH/src and download any dependencies.","title":"Install"},{"location":"how-tos/Workers/build-a-golang-task-worker/#implementing-a-task-a-worker","text":"task package provies the types used to implement the worker. Here is a reference worker implementation: package task import ( \"fmt\" ) // Implementation for \"task_1\" func Task_1_Execution_Function(t *task.Task) (taskResult *task.TaskResult, err error) { log.Println(\"Executing Task_1_Execution_Function for\", t.TaskType) //Do some logic taskResult = task.NewTaskResult(t) output := map[string]interface{}{\"task\":\"task_1\", \"key2\":\"value2\", \"key3\":3, \"key4\":false} taskResult.OutputData = output taskResult.Status = \"COMPLETED\" err = nil return taskResult, err }","title":"Implementing a Task a Worker"},{"location":"how-tos/Workers/build-a-golang-task-worker/#worker-polling","text":"Here is an example that shows how to start polling for tasks after defining the tasks. package main import ( \"github.com/netflix/conductor/client/go\" \"github.com/netflix/conductor/client/go/task/sample\" ) func main() { c := conductor.NewConductorWorker(\"http://localhost:8080\", 1, 10000) c.Start(\"task_1\", \"\", sample.Task_1_Execution_Function, false) c.Start(\"task_2\", \"mydomain\", sample.Task_2_Execution_Function, true) }","title":"Worker Polling"},{"location":"how-tos/Workers/build-a-golang-task-worker/#newconductorwoker-parameters","text":"baseUrl: Server address. threadCount: No. of threads. Number of threads should be at-least same as the number of workers pollingInterval: Time in millisecond between subsequent polls","title":"NewConductorWoker parameters"},{"location":"how-tos/Workers/build-a-java-task-worker/","text":"Build a Java Task Worker \u00b6 This guide provides introduction to building Task Workers in Java. Dependencies \u00b6 Conductor provides java client libraries, which we will use to build a simple task worker. Maven Dependency \u00b6 <dependency> <groupId>com.netflix.conductor</groupId> <artifactId>conductor-client</artifactId> <version>3.3.4</version> </dependency> Gradle \u00b6 implementation group: 'com.netflix.conductor', name: 'conductor-client', version: '3.3.4' Implementing a Task a Worker \u00b6 To create a worker, implement the Worker interface. public class SampleWorker implements Worker { private final String taskDefName; public SampleWorker(String taskDefName) { this.taskDefName = taskDefName; } @Override public String getTaskDefName() { return taskDefName; } @Override public TaskResult execute(Task task) { TaskResult result = new TaskResult(task); result.setStatus(Status.COMPLETED); //Register the output of the task result.getOutputData().put(\"outputKey1\", \"value\"); result.getOutputData().put(\"oddEven\", 1); result.getOutputData().put(\"mod\", 4); return result; } } Implementing worker's logic \u00b6 Worker's core implementation logic goes in the execute method. Upon completion, set the TaskResult with status as one of the following: 1. COMPLETED : If the task has completed successfully. 2. FAILED : If there are failures - business or system failures. Based on the task's configuration, when a task fails, it maybe retried. getTaskDefName() method returns the name of the task for which this worker provides the execution logic. See SampleWorker.java for the complete example. Configuring polling using TaskRunnerConfigurer \u00b6 The TaskRunnerConfigurer can be used to register the worker(s) and initialize the polling loop. Manages the task workers thread pool and server communication (poll and task update). Use the Builder to create an instance of the TaskRunnerConfigurer. The builder accepts the following parameters: TaskClient taskClient = new TaskClient(); taskClient.setRootURI(\"http://localhost:8080/api/\"); //Point this to the server API int threadCount = 2; //number of threads used to execute workers. To avoid starvation, should be same or more than number of workers Worker worker1 = new SampleWorker(\"task_1\"); Worker worker2 = new SampleWorker(\"task_5\"); // Create TaskRunnerConfigurer TaskRunnerConfigurer configurer = new TaskRunnerConfigurer.Builder(taskClient, Arrays.asList(worker1, worker2)) .withThreadCount(threadCount) .build(); // Start the polling and execution of tasks configurer.init(); See Sample for full example. Configuration Details \u00b6 Initialize the Builder with the following: TaskClient | TaskClient used to communicate to the Conductor server | | Workers | Workers that will be used for polling work and task execution. | Parameter Description Default withEurekaClient EurekaClient is used to identify if the server is in discovery or not. When the server goes out of discovery, the polling is stopped unless pollOutOfDiscovery is set to true. If passed null, discovery check is not done. provided by platform withThreadCount Number of threads assigned to the workers. Should be at-least the size of taskWorkers to avoid starvation in a busy system. Number of registered workers withSleepWhenRetry Time in milliseconds, for which the thread should sleep when task update call fails, before retrying the operation. 500 withUpdateRetryCount Number of attempts to be made when updating task status when update status call fails. 3 withWorkerNamePrefix String prefix that will be used for all the workers. workflow-worker- Once an instance is created, call init() method to initialize the TaskPollExecutor and begin the polling and execution of tasks. Note To ensure that the TaskRunnerConfigurer stops polling for tasks when the instance becomes unhealthy, call the provided shutdown() hook in a PreDestroy block. Properties The worker behavior can be further controlled by using these properties: Property Type Description Default paused boolean If set to true, the worker stops polling. false pollInterval int Interval in milliseconds at which the server should be polled for tasks. 1000 pollOutOfDiscovery boolean If set to true, the instance will poll for tasks regardless of the discovery status. This is useful while running on a dev machine. false Further, these properties can be set either by Worker implementation or by setting the following system properties in the JVM: Name Description conductor.worker.<property> Applies to ALL the workers in the JVM. conductor.worker.<taskDefName>.<property> Applies to the specified worker. Overrides the global property.","title":"-Build a Java Task Worker"},{"location":"how-tos/Workers/build-a-java-task-worker/#build-a-java-task-worker","text":"This guide provides introduction to building Task Workers in Java.","title":"Build a Java Task Worker"},{"location":"how-tos/Workers/build-a-java-task-worker/#dependencies","text":"Conductor provides java client libraries, which we will use to build a simple task worker.","title":"Dependencies"},{"location":"how-tos/Workers/build-a-java-task-worker/#maven-dependency","text":"<dependency> <groupId>com.netflix.conductor</groupId> <artifactId>conductor-client</artifactId> <version>3.3.4</version> </dependency>","title":"Maven Dependency"},{"location":"how-tos/Workers/build-a-java-task-worker/#gradle","text":"implementation group: 'com.netflix.conductor', name: 'conductor-client', version: '3.3.4'","title":"Gradle"},{"location":"how-tos/Workers/build-a-java-task-worker/#implementing-a-task-a-worker","text":"To create a worker, implement the Worker interface. public class SampleWorker implements Worker { private final String taskDefName; public SampleWorker(String taskDefName) { this.taskDefName = taskDefName; } @Override public String getTaskDefName() { return taskDefName; } @Override public TaskResult execute(Task task) { TaskResult result = new TaskResult(task); result.setStatus(Status.COMPLETED); //Register the output of the task result.getOutputData().put(\"outputKey1\", \"value\"); result.getOutputData().put(\"oddEven\", 1); result.getOutputData().put(\"mod\", 4); return result; } }","title":"Implementing a Task a Worker"},{"location":"how-tos/Workers/build-a-java-task-worker/#implementing-workers-logic","text":"Worker's core implementation logic goes in the execute method. Upon completion, set the TaskResult with status as one of the following: 1. COMPLETED : If the task has completed successfully. 2. FAILED : If there are failures - business or system failures. Based on the task's configuration, when a task fails, it maybe retried. getTaskDefName() method returns the name of the task for which this worker provides the execution logic. See SampleWorker.java for the complete example.","title":"Implementing worker's logic"},{"location":"how-tos/Workers/build-a-java-task-worker/#configuring-polling-using-taskrunnerconfigurer","text":"The TaskRunnerConfigurer can be used to register the worker(s) and initialize the polling loop. Manages the task workers thread pool and server communication (poll and task update). Use the Builder to create an instance of the TaskRunnerConfigurer. The builder accepts the following parameters: TaskClient taskClient = new TaskClient(); taskClient.setRootURI(\"http://localhost:8080/api/\"); //Point this to the server API int threadCount = 2; //number of threads used to execute workers. To avoid starvation, should be same or more than number of workers Worker worker1 = new SampleWorker(\"task_1\"); Worker worker2 = new SampleWorker(\"task_5\"); // Create TaskRunnerConfigurer TaskRunnerConfigurer configurer = new TaskRunnerConfigurer.Builder(taskClient, Arrays.asList(worker1, worker2)) .withThreadCount(threadCount) .build(); // Start the polling and execution of tasks configurer.init(); See Sample for full example.","title":"Configuring polling using TaskRunnerConfigurer"},{"location":"how-tos/Workers/build-a-java-task-worker/#configuration-details","text":"Initialize the Builder with the following: TaskClient | TaskClient used to communicate to the Conductor server | | Workers | Workers that will be used for polling work and task execution. | Parameter Description Default withEurekaClient EurekaClient is used to identify if the server is in discovery or not. When the server goes out of discovery, the polling is stopped unless pollOutOfDiscovery is set to true. If passed null, discovery check is not done. provided by platform withThreadCount Number of threads assigned to the workers. Should be at-least the size of taskWorkers to avoid starvation in a busy system. Number of registered workers withSleepWhenRetry Time in milliseconds, for which the thread should sleep when task update call fails, before retrying the operation. 500 withUpdateRetryCount Number of attempts to be made when updating task status when update status call fails. 3 withWorkerNamePrefix String prefix that will be used for all the workers. workflow-worker- Once an instance is created, call init() method to initialize the TaskPollExecutor and begin the polling and execution of tasks. Note To ensure that the TaskRunnerConfigurer stops polling for tasks when the instance becomes unhealthy, call the provided shutdown() hook in a PreDestroy block. Properties The worker behavior can be further controlled by using these properties: Property Type Description Default paused boolean If set to true, the worker stops polling. false pollInterval int Interval in milliseconds at which the server should be polled for tasks. 1000 pollOutOfDiscovery boolean If set to true, the instance will poll for tasks regardless of the discovery status. This is useful while running on a dev machine. false Further, these properties can be set either by Worker implementation or by setting the following system properties in the JVM: Name Description conductor.worker.<property> Applies to ALL the workers in the JVM. conductor.worker.<taskDefName>.<property> Applies to the specified worker. Overrides the global property.","title":"Configuration Details"},{"location":"how-tos/Workers/build-a-python-task-worker/","text":"Build a Python Task Worker \u00b6 Install the python client \u00b6 virtualenv conductorclient source conductorclient/bin/activate cd ../conductor/client/python python setup.py install Implement a Task Worker \u00b6 ConductorWorker class is used to implement task workers. The following script shows how to bring up two task workers named book_flight and book_car : from __future__ import print_function from conductor.ConductorWorker import ConductorWorker def book_flight_task(task): return {'status': 'COMPLETED', 'output': {'booking_ref': 2341111, 'airline': 'delta'}, 'logs': ['trying delta', 'skipping aa']} def book_car_task(task): return {'status': 'COMPLETED', 'output': {'booking_ref': \"84545fdfd\", 'agency': 'hertz'}, 'logs': ['trying hertz']} def main(): print('Starting Travel Booking workflows') cc = ConductorWorker('http://localhost:8080/api', 1, 0.1) cc.start('book_flight', book_flight_task, False) cc.start('book_car', book_car_task, True) if __name__ == '__main__': main() ConductorWorker parameters \u00b6 server_url: str The url to the server hosting the conductor api. Ex: 'http://localhost:8080/api' thread_count: int The number of threads that will be polling for and executing tasks in case of using the start method. polling_interval: float The number of seconds that each worker thread will wait between polls to the conductor server. worker_id: str, optional The worker_id of the worker that is going to execute the task. For further details, refer to the documentation By default, it is set to hostname of the machine start method parameters \u00b6 taskType: str The name of the task that the worker is looking to execute exec_function: function The function that the worker will execute. The function must return a dict with the `status`, `output` and `logs` keys present. If this is not present, an Exception will be raised wait: bool Whether the worker will block execution of further code. Since the workers are being run in daemon threads, when the program completes execution, all the threads are destroyed. Setting wait to True prevents the program from ending. If multiple workers are being called from the same program, all but the last start call but have wait set to False. The last start call must always set wait to True. If a single worker is being called, set wait to True. domain: str, optional The domain of the task under which the worker will run. For further details refer to the conductor server documentation By default, it is set to None See https://github.com/Netflix/conductor/tree/main/polyglot-clients/python for the source code.","title":"-Build a Python Task Worker"},{"location":"how-tos/Workers/build-a-python-task-worker/#build-a-python-task-worker","text":"","title":"Build a Python Task Worker"},{"location":"how-tos/Workers/build-a-python-task-worker/#install-the-python-client","text":"virtualenv conductorclient source conductorclient/bin/activate cd ../conductor/client/python python setup.py install","title":"Install the python client"},{"location":"how-tos/Workers/build-a-python-task-worker/#implement-a-task-worker","text":"ConductorWorker class is used to implement task workers. The following script shows how to bring up two task workers named book_flight and book_car : from __future__ import print_function from conductor.ConductorWorker import ConductorWorker def book_flight_task(task): return {'status': 'COMPLETED', 'output': {'booking_ref': 2341111, 'airline': 'delta'}, 'logs': ['trying delta', 'skipping aa']} def book_car_task(task): return {'status': 'COMPLETED', 'output': {'booking_ref': \"84545fdfd\", 'agency': 'hertz'}, 'logs': ['trying hertz']} def main(): print('Starting Travel Booking workflows') cc = ConductorWorker('http://localhost:8080/api', 1, 0.1) cc.start('book_flight', book_flight_task, False) cc.start('book_car', book_car_task, True) if __name__ == '__main__': main()","title":"Implement a Task Worker"},{"location":"how-tos/Workers/build-a-python-task-worker/#conductorworker-parameters","text":"server_url: str The url to the server hosting the conductor api. Ex: 'http://localhost:8080/api' thread_count: int The number of threads that will be polling for and executing tasks in case of using the start method. polling_interval: float The number of seconds that each worker thread will wait between polls to the conductor server. worker_id: str, optional The worker_id of the worker that is going to execute the task. For further details, refer to the documentation By default, it is set to hostname of the machine","title":"ConductorWorker parameters"},{"location":"how-tos/Workers/build-a-python-task-worker/#start-method-parameters","text":"taskType: str The name of the task that the worker is looking to execute exec_function: function The function that the worker will execute. The function must return a dict with the `status`, `output` and `logs` keys present. If this is not present, an Exception will be raised wait: bool Whether the worker will block execution of further code. Since the workers are being run in daemon threads, when the program completes execution, all the threads are destroyed. Setting wait to True prevents the program from ending. If multiple workers are being called from the same program, all but the last start call but have wait set to False. The last start call must always set wait to True. If a single worker is being called, set wait to True. domain: str, optional The domain of the task under which the worker will run. For further details refer to the conductor server documentation By default, it is set to None See https://github.com/Netflix/conductor/tree/main/polyglot-clients/python for the source code.","title":"start method parameters"},{"location":"how-tos/Workflows/create-workflow/","text":"Creating Workflows \u00b6 TODO Summary \u00b6 TODO","title":"Creating Workflows"},{"location":"how-tos/Workflows/create-workflow/#creating-workflows","text":"TODO","title":"Creating Workflows"},{"location":"how-tos/Workflows/create-workflow/#summary","text":"TODO","title":"Summary"},{"location":"how-tos/Workflows/debugging-workflows/","text":"Debugging Workflows \u00b6 Conductor UI is a tool that we can leverage for debugging issues. Refer to the following articles to search and view your workflow execution. Searching Workflows (coming soon!) View Workflow Executions (coming soon!) Debugging Executions \u00b6 Open the Tasks > Diagram tab to see the diagram of the overall workflow execution If there is a failure, you will them on the view marked as red. In most cases it should be clear what went wrong from the view itself. To see details of the failure, you can click on the failed task. The following fields are useful in debugging Field Name Description Task Detail > Summary > Reason for Incompletion If an exception was thrown by the worker, it will be captured and displayed here Task Detail > Summary > Worker The worker instance id where this failure last occurred. Useful to dig for detailed logs if not already captured by Conductor Task Detail > Input Verify if the task inputs were computed and provided correctly to the task Task Detail > Output If output of a previous task is used as an input to your next task, refer here for what was produced Task Detail > Logs If your task is supplying logs, we can look at that here Task Detail > Retried Task - Select an instance If your task was retried, we can see all the attempts and correponding details here Note: We can also access the task list from Tasks > Task List tab. Here is a screen grab of the fields referred above. Recovering From Failures \u00b6 Once we have resolved the underlying issue of workflow execution failure, we might want to replay or retry failed workflows. The UI has functions that would allow us to do this: The Actions button provides the following options: Action Name Description Restart with Current Definitions Restart this workflow from the beginning using the same version of the workflow definition that originally ran this workflow execution. This is useful if the workflow definition has changed and we want to retain this instance to the original version Restart with Latest Definitions Restart this workflow from the beginning using the latest definition of the workflow. If we made changes to definition, we can use this option to re-run this flow with the latest version Retry - From failed task Retry this workflow from the failed task Note: Conductor configurations allow your tasks to be retried automatically for transient failures. Refer to the task configuration options on how to leverage this.","title":"Debugging Workflows"},{"location":"how-tos/Workflows/debugging-workflows/#debugging-workflows","text":"Conductor UI is a tool that we can leverage for debugging issues. Refer to the following articles to search and view your workflow execution. Searching Workflows (coming soon!) View Workflow Executions (coming soon!)","title":"Debugging Workflows"},{"location":"how-tos/Workflows/debugging-workflows/#debugging-executions","text":"Open the Tasks > Diagram tab to see the diagram of the overall workflow execution If there is a failure, you will them on the view marked as red. In most cases it should be clear what went wrong from the view itself. To see details of the failure, you can click on the failed task. The following fields are useful in debugging Field Name Description Task Detail > Summary > Reason for Incompletion If an exception was thrown by the worker, it will be captured and displayed here Task Detail > Summary > Worker The worker instance id where this failure last occurred. Useful to dig for detailed logs if not already captured by Conductor Task Detail > Input Verify if the task inputs were computed and provided correctly to the task Task Detail > Output If output of a previous task is used as an input to your next task, refer here for what was produced Task Detail > Logs If your task is supplying logs, we can look at that here Task Detail > Retried Task - Select an instance If your task was retried, we can see all the attempts and correponding details here Note: We can also access the task list from Tasks > Task List tab. Here is a screen grab of the fields referred above.","title":"Debugging Executions"},{"location":"how-tos/Workflows/debugging-workflows/#recovering-from-failures","text":"Once we have resolved the underlying issue of workflow execution failure, we might want to replay or retry failed workflows. The UI has functions that would allow us to do this: The Actions button provides the following options: Action Name Description Restart with Current Definitions Restart this workflow from the beginning using the same version of the workflow definition that originally ran this workflow execution. This is useful if the workflow definition has changed and we want to retain this instance to the original version Restart with Latest Definitions Restart this workflow from the beginning using the latest definition of the workflow. If we made changes to definition, we can use this option to re-run this flow with the latest version Retry - From failed task Retry this workflow from the failed task Note: Conductor configurations allow your tasks to be retried automatically for transient failures. Refer to the task configuration options on how to leverage this.","title":"Recovering From Failures"},{"location":"how-tos/Workflows/handling-errors/","text":"Handling Errors \u00b6 When a workflow fails, there are 2 ways to handle the exception. failureWorkflow \u00b6 In your main workflow definition, you can configure a workflow to run upon failure. By default, three parameters are passed: reason workflowId: use this to pull the details of the failed workflow. failureStatus Set workflowStatusListenerEnabled \u00b6 When this is enabled, notifications are now possible, and by building a custome implementation of the Workflow Status Listener, a notifictaion can be sent to an external service. More details.","title":"Handliing Errors"},{"location":"how-tos/Workflows/handling-errors/#handling-errors","text":"When a workflow fails, there are 2 ways to handle the exception.","title":"Handling Errors"},{"location":"how-tos/Workflows/handling-errors/#failureworkflow","text":"In your main workflow definition, you can configure a workflow to run upon failure. By default, three parameters are passed: reason workflowId: use this to pull the details of the failed workflow. failureStatus","title":"failureWorkflow"},{"location":"how-tos/Workflows/handling-errors/#set-workflowstatuslistenerenabled","text":"When this is enabled, notifications are now possible, and by building a custome implementation of the Workflow Status Listener, a notifictaion can be sent to an external service. More details.","title":"Set workflowStatusListenerEnabled"},{"location":"how-tos/Workflows/searching-workflows/","text":"Searching Workflows \u00b6 In this article we will learn how to search through workflow executions via the UI. Prerequisites \u00b6 Conductor app and UI installed and running in an environment. If required we can look at the following options to get an environment up and running. Build and Run Conductor Locally Running via Docker Compose UI Workflows View \u00b6 Open the home page of the UI installation. It will take you to the Workflow Executions view. This is where we can look at available workflow executions. Basic Search \u00b6 The following fields are available for searching for workflows. Search Field Name Description Workflow Name Use this field to filter workflows by the configured name Workflow ID Use this field to filter to a specific workflow by its id Status Use this field to filter by status - available options are presented as a multi-select option Start Time - From Use this field to filter workflows that started on or after the time specified Start Time - To Use this field to filter workflows that started on or before the time specified Lookback (days) Use this field to filter workflows that ran in the last given number of days Free Text Query If you have indexing enabled, you can query by values that was part of your workflow inputs and outputs The table listing has options to 1. Select columns for display 2. Sort by column value At the bottom of the table, there are options to 1. Select number of rows per page 2. Navigating through pages Find by Tasks \u00b6 In addition to the options listed in Basic Search view, we have the following options in the Find by Tasks view. Search Field Name Description Include Task ID Use this field to filter workflows that contains a task with this id Include Task Name Use this field to filter workflows that contains a task with name Free Text in Tasks If you have indexing enabled, you can query by values that was part of your workflow task inputs and outputs","title":"Searching Workflows"},{"location":"how-tos/Workflows/searching-workflows/#searching-workflows","text":"In this article we will learn how to search through workflow executions via the UI.","title":"Searching Workflows"},{"location":"how-tos/Workflows/searching-workflows/#prerequisites","text":"Conductor app and UI installed and running in an environment. If required we can look at the following options to get an environment up and running. Build and Run Conductor Locally Running via Docker Compose","title":"Prerequisites"},{"location":"how-tos/Workflows/searching-workflows/#ui-workflows-view","text":"Open the home page of the UI installation. It will take you to the Workflow Executions view. This is where we can look at available workflow executions.","title":"UI Workflows View"},{"location":"how-tos/Workflows/searching-workflows/#basic-search","text":"The following fields are available for searching for workflows. Search Field Name Description Workflow Name Use this field to filter workflows by the configured name Workflow ID Use this field to filter to a specific workflow by its id Status Use this field to filter by status - available options are presented as a multi-select option Start Time - From Use this field to filter workflows that started on or after the time specified Start Time - To Use this field to filter workflows that started on or before the time specified Lookback (days) Use this field to filter workflows that ran in the last given number of days Free Text Query If you have indexing enabled, you can query by values that was part of your workflow inputs and outputs The table listing has options to 1. Select columns for display 2. Sort by column value At the bottom of the table, there are options to 1. Select number of rows per page 2. Navigating through pages","title":"Basic Search"},{"location":"how-tos/Workflows/searching-workflows/#find-by-tasks","text":"In addition to the options listed in Basic Search view, we have the following options in the Find by Tasks view. Search Field Name Description Include Task ID Use this field to filter workflows that contains a task with this id Include Task Name Use this field to filter workflows that contains a task with name Free Text in Tasks If you have indexing enabled, you can query by values that was part of your workflow task inputs and outputs","title":"Find by Tasks"},{"location":"how-tos/Workflows/starting-workflows/","text":"Starting Workflow Executions \u00b6 Workflow executions can be started by using the following API: ```http request POST /api/workflow/{name} `{name}` is the placeholder for workflow name. The POST API body is your workflow input parameters which can be empty if there are none. ### Using Client SDKs Conductor offers client SDKs for popular languages which has library methods that can be used to make this API call. Refer to the SDK documentation to configure a client in your selected language to invoke workflow executions. ### Example using curl ```shell curl 'https://localhost:8080/api/workflow/sample_workflow' \\ -H 'accept: text/plain' \\ -H 'content-type: application/json' \\ --data-raw '{\"service\":\"fedex\"}' In this example we are specifying one input param called service with a value of fedex and the name of the workflow is sample_workflow Example using node fetch \u00b6 fetch(\"https://localhost:8080/api/workflow/sample_workflow\", { \"headers\": { \"accept\": \"text/plain\", \"content-type\": \"application/json\", }, \"body\": \"{\\\"service\\\":\\\"fedex\\\"}\", \"method\": \"POST\", });","title":"Starting a Workflow"},{"location":"how-tos/Workflows/starting-workflows/#starting-workflow-executions","text":"Workflow executions can be started by using the following API: ```http request POST /api/workflow/{name} `{name}` is the placeholder for workflow name. The POST API body is your workflow input parameters which can be empty if there are none. ### Using Client SDKs Conductor offers client SDKs for popular languages which has library methods that can be used to make this API call. Refer to the SDK documentation to configure a client in your selected language to invoke workflow executions. ### Example using curl ```shell curl 'https://localhost:8080/api/workflow/sample_workflow' \\ -H 'accept: text/plain' \\ -H 'content-type: application/json' \\ --data-raw '{\"service\":\"fedex\"}' In this example we are specifying one input param called service with a value of fedex and the name of the workflow is sample_workflow","title":"Starting Workflow Executions"},{"location":"how-tos/Workflows/starting-workflows/#example-using-node-fetch","text":"fetch(\"https://localhost:8080/api/workflow/sample_workflow\", { \"headers\": { \"accept\": \"text/plain\", \"content-type\": \"application/json\", }, \"body\": \"{\\\"service\\\":\\\"fedex\\\"}\", \"method\": \"POST\", });","title":"Example using node fetch"},{"location":"how-tos/Workflows/updating-workflows/","text":"Updating Workflow Definitions \u00b6 Workflows can be created or updated using the workflow metadata API PUT /api/metadata/workflow Example using curl \u00b6 curl 'http://localhost:8080/api/metadata/workflow' \\ -X 'PUT' \\ -H 'accept: */*' \\ -H 'content-type: application/json' \\ --data-raw '[{\"name\":\"sample_workflow\",\"version\":1,\"tasks\":[{\"name\":\"ship_via_fedex\",\"taskReferenceName\":\"ship_via_fedex\",\"type\":\"SIMPLE\"}],\"schemaVersion\":2}]' Example using node fetch \u00b6 fetch(\"http://localhost:8080/api/metadata/workflow\", { \"headers\": { \"accept\": \"*/*\", \"content-type\": \"application/json\" }, \"body\": \"[{\\\"name\\\":\\\"sample_workflow\\\",\\\"version\\\":1,\\\"tasks\\\":[{\\\"name\\\":\\\"ship_via_fedex\\\",\\\"taskReferenceName\\\":\\\"ship_via_fedex\\\",\\\"type\\\":\\\"SIMPLE\\\"}],\\\"schemaVersion\\\":2}]\", \"method\": \"PUT\" }); Best Practices \u00b6 If you are updating the workflow with new tasks, remember to register the task definitions first You can also use the Conductor Swagger UI to update the workflows","title":"Updating Workflows"},{"location":"how-tos/Workflows/updating-workflows/#updating-workflow-definitions","text":"Workflows can be created or updated using the workflow metadata API PUT /api/metadata/workflow","title":"Updating Workflow Definitions"},{"location":"how-tos/Workflows/updating-workflows/#example-using-curl","text":"curl 'http://localhost:8080/api/metadata/workflow' \\ -X 'PUT' \\ -H 'accept: */*' \\ -H 'content-type: application/json' \\ --data-raw '[{\"name\":\"sample_workflow\",\"version\":1,\"tasks\":[{\"name\":\"ship_via_fedex\",\"taskReferenceName\":\"ship_via_fedex\",\"type\":\"SIMPLE\"}],\"schemaVersion\":2}]'","title":"Example using curl"},{"location":"how-tos/Workflows/updating-workflows/#example-using-node-fetch","text":"fetch(\"http://localhost:8080/api/metadata/workflow\", { \"headers\": { \"accept\": \"*/*\", \"content-type\": \"application/json\" }, \"body\": \"[{\\\"name\\\":\\\"sample_workflow\\\",\\\"version\\\":1,\\\"tasks\\\":[{\\\"name\\\":\\\"ship_via_fedex\\\",\\\"taskReferenceName\\\":\\\"ship_via_fedex\\\",\\\"type\\\":\\\"SIMPLE\\\"}],\\\"schemaVersion\\\":2}]\", \"method\": \"PUT\" });","title":"Example using node fetch"},{"location":"how-tos/Workflows/updating-workflows/#best-practices","text":"If you are updating the workflow with new tasks, remember to register the task definitions first You can also use the Conductor Swagger UI to update the workflows","title":"Best Practices"},{"location":"how-tos/Workflows/view-workflow-executions/","text":"View Workflow Executions \u00b6 In this article we will learn how to view workflow executions via the UI. Prerequisites \u00b6 Conductor app and UI installed and running in an environment. If required we can look at the following options to get an environment up and running. Build and Run Conductor Locally Running via Docker Compose Viewing a Workflow Execution \u00b6 Refer to Searching Workflows to filter and find an execution you want to view. Click on the workflow id hyperlink to open the Workflow Execution Details page. The following tabs are available to view the details of the Workflow Execution Tab Name Description Tasks Shows a view with the sub tabs Diagram , Task List and Timeline Tasks > Diagram Visual view of the workflow and its tasks. Tasks > Task List Tabular view of the task executions under this workflow. If there were failures, we will be able to see that here Tasks > Timeline Shows the time each of the tasks took for execution in a timeline view Summary Summary view of the workflow execution Workflow Input/Output Shows the input and output payloads of the workflow. Enable copy mode to copy all or parts of the payload JSON Full JSON payload of the workflow including all tasks, inputs and outputs. Useful for detailed debugging. Viewing a Workflow Task Detail \u00b6 From both the Tasks > Diagram and Tasks > Task List views, we can click to see a task execution detail. This opens a flyout panel from the side and contains the following tabs. Tab Name Description Summary Summary info of the task execution Input Task input payload - refer to this tab to see computed inputs passed into the task. Enable copy mode to copy all or parts of the payload Output Shows the output payload produced by the executed task. Enable copy mode to copy all or parts of the payload Log Any log messages logged by the task worked will show up here JSON Complete JSON payload for debugging issues Definition Task definition used when executing this task Execution Path \u00b6 An exciting feature of conductor is the ability to see the exact execution path of a workflow. The executed paths are shown in green and is easy to follow like the example below. The alternative paths are greyed out for reference Errors will be visible on the UI in ref such as the example below","title":"View Workflow Execution"},{"location":"how-tos/Workflows/view-workflow-executions/#view-workflow-executions","text":"In this article we will learn how to view workflow executions via the UI.","title":"View Workflow Executions"},{"location":"how-tos/Workflows/view-workflow-executions/#prerequisites","text":"Conductor app and UI installed and running in an environment. If required we can look at the following options to get an environment up and running. Build and Run Conductor Locally Running via Docker Compose","title":"Prerequisites"},{"location":"how-tos/Workflows/view-workflow-executions/#viewing-a-workflow-execution","text":"Refer to Searching Workflows to filter and find an execution you want to view. Click on the workflow id hyperlink to open the Workflow Execution Details page. The following tabs are available to view the details of the Workflow Execution Tab Name Description Tasks Shows a view with the sub tabs Diagram , Task List and Timeline Tasks > Diagram Visual view of the workflow and its tasks. Tasks > Task List Tabular view of the task executions under this workflow. If there were failures, we will be able to see that here Tasks > Timeline Shows the time each of the tasks took for execution in a timeline view Summary Summary view of the workflow execution Workflow Input/Output Shows the input and output payloads of the workflow. Enable copy mode to copy all or parts of the payload JSON Full JSON payload of the workflow including all tasks, inputs and outputs. Useful for detailed debugging.","title":"Viewing a Workflow Execution"},{"location":"how-tos/Workflows/view-workflow-executions/#viewing-a-workflow-task-detail","text":"From both the Tasks > Diagram and Tasks > Task List views, we can click to see a task execution detail. This opens a flyout panel from the side and contains the following tabs. Tab Name Description Summary Summary info of the task execution Input Task input payload - refer to this tab to see computed inputs passed into the task. Enable copy mode to copy all or parts of the payload Output Shows the output payload produced by the executed task. Enable copy mode to copy all or parts of the payload Log Any log messages logged by the task worked will show up here JSON Complete JSON payload for debugging issues Definition Task definition used when executing this task","title":"Viewing a Workflow Task Detail"},{"location":"how-tos/Workflows/view-workflow-executions/#execution-path","text":"An exciting feature of conductor is the ability to see the exact execution path of a workflow. The executed paths are shown in green and is easy to follow like the example below. The alternative paths are greyed out for reference Errors will be visible on the UI in ref such as the example below","title":"Execution Path"},{"location":"labs/beginner/","text":"Hands on mode \u00b6 Please feel free to follow along using any of these resources: Using cURL. Postman or similar REST client. Creating a Workflow \u00b6 Let's create a simple workflow that adds Netflix Idents to videos. We'll be mocking the adding Idents part and focusing on actually executing this process flow. What are Netflix Idents? Netflix Idents are those 4 second videos with Netflix logo, which appears at the beginning and end of shows. Learn more about them here . You might have also noticed they're different for Animation and several other genres. Disclaimer Obviously, this is not how Netflix adds Idents. Those Workflows are indeed very complex. But, it should give you an idea about how Conductor can be used to implement similar features. The workflow in this lab will look like this: This workflow contains the following: Worker Task verify_if_idents_are_added to verify if Idents are already added. Decision Task that takes output from the previous task, and decides whether to schedule the add_idents task. add_idents task which is another worker Task. Creating Task definitions \u00b6 Let's create the task definition for verify_if_idents_are_added in JSON. This task will be a SIMPLE task which is supposed to be executed by an Idents microservice. We'll be mocking the Idents microservice part. Note that at this point, we don't have to specify whether it is a System task or Worker task. We are only specifying the required configurations for the task, like number of times it should be retried, timeouts etc. We shall start by using name parameter for task name. { \"name\": \"verify_if_idents_are_added\" } We'd like this task to be retried 3 times on failure. { \"name\": \"verify_if_idents_are_added\", \"retryCount\": 3, \"retryLogic\": \"FIXED\", \"retryDelaySeconds\": 10 } And to timeout after 300 seconds. i.e. if the task doesn't finish execution within this time limit after transitioning to IN_PROGRESS state, the Conductor server cancels this task and schedules a new execution of this task in the queue. { \"name\": \"verify_if_idents_are_added\", \"retryCount\": 3, \"retryLogic\": \"FIXED\", \"retryDelaySeconds\": 10, \"timeoutSeconds\": 300, \"timeoutPolicy\": \"TIME_OUT_WF\" } And a responseTimeout of 180 seconds. { \"name\": \"verify_if_idents_are_added\", \"retryCount\": 3, \"retryLogic\": \"FIXED\", \"retryDelaySeconds\": 10, \"timeoutSeconds\": 300, \"timeoutPolicy\": \"TIME_OUT_WF\", \"responseTimeoutSeconds\": 180 } We can define several other fields defined here , but this is a good place to start with. Similarly, create another task definition: add_idents . { \"name\": \"add_idents\", \"retryCount\": 3, \"retryLogic\": \"FIXED\", \"retryDelaySeconds\": 10, \"timeoutSeconds\": 300, \"timeoutPolicy\": \"TIME_OUT_WF\", \"responseTimeoutSeconds\": 180 } Send a POST request to /metadata/taskdefs endpoint to register these tasks. You can use Swagger, Postman, CURL or similar tools. Why is the Decision Task not registered? System Tasks that are part of control flow do not need to be registered. However, some system tasks where the retries, rate limiting and other mechanisms are required, like HTTP Task, are to be registered though. Important Task and Workflow Definition names are unique. The names we use below might have already been registered. For this lab, add a prefix with your username, {my_username}_verify_if_idents_are_added for example. This is definitely not recommended for Production usage though. Example curl -X POST \\ http://localhost:8080/api/metadata/taskdefs \\ -H 'Content-Type: application/json' \\ -d '[ { \"name\": \"verify_if_idents_are_added\", \"retryCount\": 3, \"retryLogic\": \"FIXED\", \"retryDelaySeconds\": 10, \"timeoutSeconds\": 300, \"timeoutPolicy\": \"TIME_OUT_WF\", \"responseTimeoutSeconds\": 180, \"ownerEmail\": \"type your email here\" }, { \"name\": \"add_idents\", \"retryCount\": 3, \"retryLogic\": \"FIXED\", \"retryDelaySeconds\": 10, \"timeoutSeconds\": 300, \"timeoutPolicy\": \"TIME_OUT_WF\", \"responseTimeoutSeconds\": 180, \"ownerEmail\": \"type your email here\" } ]' Creating Workflow Definition \u00b6 Creating Workflow definition is almost similar. We shall use the Task definitions created above. Note that same Task definitions can be used in multiple workflows, or for multipe times in same Workflow (that's where taskReferenceName is useful). A workflow without any tasks looks like this: { \"name\": \"add_netflix_identation\", \"description\": \"Adds Netflix Identation to video files.\", \"version\": 1, \"schemaVersion\": 2, \"tasks\": [] } Add the first task that this workflow has to execute. All the tasks must be added to the tasks array. { \"name\": \"add_netflix_identation\", \"description\": \"Adds Netflix Identation to video files.\", \"version\": 1, \"schemaVersion\": 2, \"tasks\": [ { \"name\": \"verify_if_idents_are_added\", \"taskReferenceName\": \"ident_verification\", \"inputParameters\": { \"contentId\": \"${workflow.input.contentId}\" }, \"type\": \"SIMPLE\" } ] } Wiring Input/Outputs Notice how we were using ${workflow.input.contentId} to pass inputs to this task. Conductor can wire inputs between workflow and tasks, and between tasks. i.e The task verify_if_idents_are_added is wired to accept inputs from the workflow input using JSONPath expression ${workflow.input.param} . Learn more about wiring inputs and outputs here . Let's define decisionCases now. Checkout the Decision task structure here . A Decision task is specified by type:\"DECISION\" , caseValueParam and decisionCases which lists all the branches of Decision task. This is similar to a switch..case but written in Conductor JSON DSL. Adding the decision task: { \"name\": \"add_netflix_identation\", \"description\": \"Adds Netflix Identation to video files.\", \"version\": 2, \"schemaVersion\": 2, \"tasks\": [ { \"name\": \"verify_if_idents_are_added\", \"taskReferenceName\": \"ident_verification\", \"inputParameters\": { \"contentId\": \"${workflow.input.contentId}\" }, \"type\": \"SIMPLE\" }, { \"name\": \"decide_task\", \"taskReferenceName\": \"is_idents_added\", \"inputParameters\": { \"case_value_param\": \"${ident_verification.output.is_idents_added}\" }, \"type\": \"DECISION\", \"caseValueParam\": \"case_value_param\", \"decisionCases\": { } } ] } Each decision branch could have multiple tasks, so it has to be defined as an array. { \"name\": \"add_netflix_identation\", \"description\": \"Adds Netflix Identation to video files.\", \"version\": 2, \"schemaVersion\": 2, \"tasks\": [ { \"name\": \"verify_if_idents_are_added\", \"taskReferenceName\": \"ident_verification\", \"inputParameters\": { \"contentId\": \"${workflow.input.contentId}\" }, \"type\": \"SIMPLE\" }, { \"name\": \"decide_task\", \"taskReferenceName\": \"is_idents_added\", \"inputParameters\": { \"case_value_param\": \"${ident_verification.output.is_idents_added}\" }, \"type\": \"DECISION\", \"caseValueParam\": \"case_value_param\", \"decisionCases\": { \"false\": [ { \"name\": \"add_idents\", \"taskReferenceName\": \"add_idents_by_type\", \"inputParameters\": { \"identType\": \"${workflow.input.identType}\", \"contentId\": \"${workflow.input.contentId}\" }, \"type\": \"SIMPLE\" } ] } } ] } Just like the task definitions, register this workflow definition by sending a POST request to /workflow endpoint. Example curl -X POST \\ http://localhost:8080/api/metadata/workflow \\ -H 'Content-Type: application/json' \\ -d '{ \"name\": \"add_netflix_identation\", \"description\": \"Adds Netflix Identation to video files.\", \"version\": 2, \"schemaVersion\": 2, \"ownerEmail\": \"type your email here\", \"tasks\": [ { \"name\": \"verify_if_idents_are_added\", \"taskReferenceName\": \"ident_verification\", \"inputParameters\": { \"contentId\": \"${workflow.input.contentId}\" }, \"type\": \"SIMPLE\" }, { \"name\": \"decide_task\", \"taskReferenceName\": \"is_idents_added\", \"inputParameters\": { \"case_value_param\": \"${ident_verification.output.is_idents_added}\" }, \"type\": \"DECISION\", \"caseValueParam\": \"case_value_param\", \"decisionCases\": { \"false\": [ { \"name\": \"add_idents\", \"taskReferenceName\": \"add_idents_by_type\", \"inputParameters\": { \"identType\": \"${workflow.input.identType}\", \"contentId\": \"${workflow.input.contentId}\" }, \"type\": \"SIMPLE\" } ] } } ] }' Starting the Workflow \u00b6 Send a POST request to /workflow with: { \"name\": \"add_netflix_identation\", \"version\": 2, \"correlationId\": \"my_netflix_identation_workflows\", \"input\": { \"identType\": \"animation\", \"contentId\": \"my_unique_content_id\" } } Example: curl -X POST \\ http://localhost:8080/api/workflow/add_netflix_identation \\ -H 'Content-Type: application/json' \\ -d '{ \"identType\": \"animation\", \"contentId\": \"my_unique_content_id\" }' Successful POST request should return a workflow Id, which you can use to find the execution in the UI. Conductor User Interface \u00b6 Open the UI and navigate to the RUNNING tab, the Workflow should be in the state as below: Feel free to explore the various functionalities that the UI exposes. To elaborate on a few: Workflow Task modals (Opens on clicking any of the tasks in the workflow), which includes task I/O, logs and task JSON. Task Details tab, which shows the sequence of task execution, status, start/end time, and link to worker details which executed the task. Input/Output tab shows workflow input and output. Poll for Worker task \u00b6 Now that verify_if_idents_are_added task is in SCHEDULED state, it is the worker's turn to fetch the task, execute it and update Conductor with final status of the task. Ideally, the workers implementing the Client interface would do this process, executing the tasks on real microservices. But, let's mock this part. Send a GET request to /poll endpoint with your task type. For example: curl -X GET \\ http://localhost:8080/api/tasks/poll/verify_if_idents_are_added Return response, add logs \u00b6 We can respond to Conductor with any of the following states: Task has COMPLETED. Task has FAILED. Call back after seconds [Process the task at a later time]. Considering our Ident Service has verified that the Ident's are not yet added to given Content Id, let's return the task status by sending the below POST request to /tasks endpoint, with payload: { \"workflowInstanceId\": \"{workflowId}\", \"taskId\": \"{taskId}\", \"reasonForIncompletion\": \"\", \"callbackAfterSeconds\": 0, \"workerId\": \"localhost\", \"status\": \"COMPLETED\", \"outputData\": { \"is_idents_added\": false } } Example: curl -X POST \\ http://localhost:8080/api/tasks \\ -H 'Content-Type: application/json' \\ -d '{ \"workflowInstanceId\": \"cb7c5041-aa85-4940-acb4-3bdcfa9f5c5c\", \"taskId\": \"741f362b-ee9a-47b6-81b5-9bbbd5c4c992\", \"reasonForIncompletion\": \"\", \"callbackAfterSeconds\": 0, \"workerId\": \"string\", \"status\": \"COMPLETED\", \"outputData\": { \"is_idents_added\": false }, \"logs\": [ { \"log\": \"Ident verification successful for title: {some_title_name}, with Id: {some_id}\", \"createdTime\": 1550178825 } ] }' Check logs in UI You can find the logs we just sent by clicking the verify_if_idents_are_added , upon which a modal should open with Logs tab. Why is System task executed, but Worker task is Scheduled. \u00b6 You will notice that Workflow is in the state as below after sending the POST request: Conductor has executed is_idents_added all through it's lifecycle, without us polling, or returning the status of Task. If it is still unclear, is_idents_added is a System task, and System tasks are executed by Conductor Server. But, add_idents is a SIMPLE task. So, the complete lifecyle of this task (Poll, Update) should be handled by a worker to continue with W\\workflow execution. When Conductor has finished executing all the tasks in given flow, the workflow will reach Terminal state (COMPLETED, FAILED, TIMED_OUT etc.) Next steps \u00b6 You can play around this workflow by failing one of the Tasks, restarting or retrying the Workflow, or by tuning the number of retries, timeoutSeconds etc.","title":"Beginner"},{"location":"labs/beginner/#hands-on-mode","text":"Please feel free to follow along using any of these resources: Using cURL. Postman or similar REST client.","title":"Hands on mode"},{"location":"labs/beginner/#creating-a-workflow","text":"Let's create a simple workflow that adds Netflix Idents to videos. We'll be mocking the adding Idents part and focusing on actually executing this process flow. What are Netflix Idents? Netflix Idents are those 4 second videos with Netflix logo, which appears at the beginning and end of shows. Learn more about them here . You might have also noticed they're different for Animation and several other genres. Disclaimer Obviously, this is not how Netflix adds Idents. Those Workflows are indeed very complex. But, it should give you an idea about how Conductor can be used to implement similar features. The workflow in this lab will look like this: This workflow contains the following: Worker Task verify_if_idents_are_added to verify if Idents are already added. Decision Task that takes output from the previous task, and decides whether to schedule the add_idents task. add_idents task which is another worker Task.","title":"Creating a Workflow"},{"location":"labs/beginner/#creating-task-definitions","text":"Let's create the task definition for verify_if_idents_are_added in JSON. This task will be a SIMPLE task which is supposed to be executed by an Idents microservice. We'll be mocking the Idents microservice part. Note that at this point, we don't have to specify whether it is a System task or Worker task. We are only specifying the required configurations for the task, like number of times it should be retried, timeouts etc. We shall start by using name parameter for task name. { \"name\": \"verify_if_idents_are_added\" } We'd like this task to be retried 3 times on failure. { \"name\": \"verify_if_idents_are_added\", \"retryCount\": 3, \"retryLogic\": \"FIXED\", \"retryDelaySeconds\": 10 } And to timeout after 300 seconds. i.e. if the task doesn't finish execution within this time limit after transitioning to IN_PROGRESS state, the Conductor server cancels this task and schedules a new execution of this task in the queue. { \"name\": \"verify_if_idents_are_added\", \"retryCount\": 3, \"retryLogic\": \"FIXED\", \"retryDelaySeconds\": 10, \"timeoutSeconds\": 300, \"timeoutPolicy\": \"TIME_OUT_WF\" } And a responseTimeout of 180 seconds. { \"name\": \"verify_if_idents_are_added\", \"retryCount\": 3, \"retryLogic\": \"FIXED\", \"retryDelaySeconds\": 10, \"timeoutSeconds\": 300, \"timeoutPolicy\": \"TIME_OUT_WF\", \"responseTimeoutSeconds\": 180 } We can define several other fields defined here , but this is a good place to start with. Similarly, create another task definition: add_idents . { \"name\": \"add_idents\", \"retryCount\": 3, \"retryLogic\": \"FIXED\", \"retryDelaySeconds\": 10, \"timeoutSeconds\": 300, \"timeoutPolicy\": \"TIME_OUT_WF\", \"responseTimeoutSeconds\": 180 } Send a POST request to /metadata/taskdefs endpoint to register these tasks. You can use Swagger, Postman, CURL or similar tools. Why is the Decision Task not registered? System Tasks that are part of control flow do not need to be registered. However, some system tasks where the retries, rate limiting and other mechanisms are required, like HTTP Task, are to be registered though. Important Task and Workflow Definition names are unique. The names we use below might have already been registered. For this lab, add a prefix with your username, {my_username}_verify_if_idents_are_added for example. This is definitely not recommended for Production usage though. Example curl -X POST \\ http://localhost:8080/api/metadata/taskdefs \\ -H 'Content-Type: application/json' \\ -d '[ { \"name\": \"verify_if_idents_are_added\", \"retryCount\": 3, \"retryLogic\": \"FIXED\", \"retryDelaySeconds\": 10, \"timeoutSeconds\": 300, \"timeoutPolicy\": \"TIME_OUT_WF\", \"responseTimeoutSeconds\": 180, \"ownerEmail\": \"type your email here\" }, { \"name\": \"add_idents\", \"retryCount\": 3, \"retryLogic\": \"FIXED\", \"retryDelaySeconds\": 10, \"timeoutSeconds\": 300, \"timeoutPolicy\": \"TIME_OUT_WF\", \"responseTimeoutSeconds\": 180, \"ownerEmail\": \"type your email here\" } ]'","title":"Creating Task definitions"},{"location":"labs/beginner/#creating-workflow-definition","text":"Creating Workflow definition is almost similar. We shall use the Task definitions created above. Note that same Task definitions can be used in multiple workflows, or for multipe times in same Workflow (that's where taskReferenceName is useful). A workflow without any tasks looks like this: { \"name\": \"add_netflix_identation\", \"description\": \"Adds Netflix Identation to video files.\", \"version\": 1, \"schemaVersion\": 2, \"tasks\": [] } Add the first task that this workflow has to execute. All the tasks must be added to the tasks array. { \"name\": \"add_netflix_identation\", \"description\": \"Adds Netflix Identation to video files.\", \"version\": 1, \"schemaVersion\": 2, \"tasks\": [ { \"name\": \"verify_if_idents_are_added\", \"taskReferenceName\": \"ident_verification\", \"inputParameters\": { \"contentId\": \"${workflow.input.contentId}\" }, \"type\": \"SIMPLE\" } ] } Wiring Input/Outputs Notice how we were using ${workflow.input.contentId} to pass inputs to this task. Conductor can wire inputs between workflow and tasks, and between tasks. i.e The task verify_if_idents_are_added is wired to accept inputs from the workflow input using JSONPath expression ${workflow.input.param} . Learn more about wiring inputs and outputs here . Let's define decisionCases now. Checkout the Decision task structure here . A Decision task is specified by type:\"DECISION\" , caseValueParam and decisionCases which lists all the branches of Decision task. This is similar to a switch..case but written in Conductor JSON DSL. Adding the decision task: { \"name\": \"add_netflix_identation\", \"description\": \"Adds Netflix Identation to video files.\", \"version\": 2, \"schemaVersion\": 2, \"tasks\": [ { \"name\": \"verify_if_idents_are_added\", \"taskReferenceName\": \"ident_verification\", \"inputParameters\": { \"contentId\": \"${workflow.input.contentId}\" }, \"type\": \"SIMPLE\" }, { \"name\": \"decide_task\", \"taskReferenceName\": \"is_idents_added\", \"inputParameters\": { \"case_value_param\": \"${ident_verification.output.is_idents_added}\" }, \"type\": \"DECISION\", \"caseValueParam\": \"case_value_param\", \"decisionCases\": { } } ] } Each decision branch could have multiple tasks, so it has to be defined as an array. { \"name\": \"add_netflix_identation\", \"description\": \"Adds Netflix Identation to video files.\", \"version\": 2, \"schemaVersion\": 2, \"tasks\": [ { \"name\": \"verify_if_idents_are_added\", \"taskReferenceName\": \"ident_verification\", \"inputParameters\": { \"contentId\": \"${workflow.input.contentId}\" }, \"type\": \"SIMPLE\" }, { \"name\": \"decide_task\", \"taskReferenceName\": \"is_idents_added\", \"inputParameters\": { \"case_value_param\": \"${ident_verification.output.is_idents_added}\" }, \"type\": \"DECISION\", \"caseValueParam\": \"case_value_param\", \"decisionCases\": { \"false\": [ { \"name\": \"add_idents\", \"taskReferenceName\": \"add_idents_by_type\", \"inputParameters\": { \"identType\": \"${workflow.input.identType}\", \"contentId\": \"${workflow.input.contentId}\" }, \"type\": \"SIMPLE\" } ] } } ] } Just like the task definitions, register this workflow definition by sending a POST request to /workflow endpoint. Example curl -X POST \\ http://localhost:8080/api/metadata/workflow \\ -H 'Content-Type: application/json' \\ -d '{ \"name\": \"add_netflix_identation\", \"description\": \"Adds Netflix Identation to video files.\", \"version\": 2, \"schemaVersion\": 2, \"ownerEmail\": \"type your email here\", \"tasks\": [ { \"name\": \"verify_if_idents_are_added\", \"taskReferenceName\": \"ident_verification\", \"inputParameters\": { \"contentId\": \"${workflow.input.contentId}\" }, \"type\": \"SIMPLE\" }, { \"name\": \"decide_task\", \"taskReferenceName\": \"is_idents_added\", \"inputParameters\": { \"case_value_param\": \"${ident_verification.output.is_idents_added}\" }, \"type\": \"DECISION\", \"caseValueParam\": \"case_value_param\", \"decisionCases\": { \"false\": [ { \"name\": \"add_idents\", \"taskReferenceName\": \"add_idents_by_type\", \"inputParameters\": { \"identType\": \"${workflow.input.identType}\", \"contentId\": \"${workflow.input.contentId}\" }, \"type\": \"SIMPLE\" } ] } } ] }'","title":"Creating Workflow Definition"},{"location":"labs/beginner/#starting-the-workflow","text":"Send a POST request to /workflow with: { \"name\": \"add_netflix_identation\", \"version\": 2, \"correlationId\": \"my_netflix_identation_workflows\", \"input\": { \"identType\": \"animation\", \"contentId\": \"my_unique_content_id\" } } Example: curl -X POST \\ http://localhost:8080/api/workflow/add_netflix_identation \\ -H 'Content-Type: application/json' \\ -d '{ \"identType\": \"animation\", \"contentId\": \"my_unique_content_id\" }' Successful POST request should return a workflow Id, which you can use to find the execution in the UI.","title":"Starting the Workflow"},{"location":"labs/beginner/#conductor-user-interface","text":"Open the UI and navigate to the RUNNING tab, the Workflow should be in the state as below: Feel free to explore the various functionalities that the UI exposes. To elaborate on a few: Workflow Task modals (Opens on clicking any of the tasks in the workflow), which includes task I/O, logs and task JSON. Task Details tab, which shows the sequence of task execution, status, start/end time, and link to worker details which executed the task. Input/Output tab shows workflow input and output.","title":"Conductor User Interface"},{"location":"labs/beginner/#poll-for-worker-task","text":"Now that verify_if_idents_are_added task is in SCHEDULED state, it is the worker's turn to fetch the task, execute it and update Conductor with final status of the task. Ideally, the workers implementing the Client interface would do this process, executing the tasks on real microservices. But, let's mock this part. Send a GET request to /poll endpoint with your task type. For example: curl -X GET \\ http://localhost:8080/api/tasks/poll/verify_if_idents_are_added","title":"Poll for Worker task"},{"location":"labs/beginner/#return-response-add-logs","text":"We can respond to Conductor with any of the following states: Task has COMPLETED. Task has FAILED. Call back after seconds [Process the task at a later time]. Considering our Ident Service has verified that the Ident's are not yet added to given Content Id, let's return the task status by sending the below POST request to /tasks endpoint, with payload: { \"workflowInstanceId\": \"{workflowId}\", \"taskId\": \"{taskId}\", \"reasonForIncompletion\": \"\", \"callbackAfterSeconds\": 0, \"workerId\": \"localhost\", \"status\": \"COMPLETED\", \"outputData\": { \"is_idents_added\": false } } Example: curl -X POST \\ http://localhost:8080/api/tasks \\ -H 'Content-Type: application/json' \\ -d '{ \"workflowInstanceId\": \"cb7c5041-aa85-4940-acb4-3bdcfa9f5c5c\", \"taskId\": \"741f362b-ee9a-47b6-81b5-9bbbd5c4c992\", \"reasonForIncompletion\": \"\", \"callbackAfterSeconds\": 0, \"workerId\": \"string\", \"status\": \"COMPLETED\", \"outputData\": { \"is_idents_added\": false }, \"logs\": [ { \"log\": \"Ident verification successful for title: {some_title_name}, with Id: {some_id}\", \"createdTime\": 1550178825 } ] }' Check logs in UI You can find the logs we just sent by clicking the verify_if_idents_are_added , upon which a modal should open with Logs tab.","title":"Return response, add logs"},{"location":"labs/beginner/#why-is-system-task-executed-but-worker-task-is-scheduled","text":"You will notice that Workflow is in the state as below after sending the POST request: Conductor has executed is_idents_added all through it's lifecycle, without us polling, or returning the status of Task. If it is still unclear, is_idents_added is a System task, and System tasks are executed by Conductor Server. But, add_idents is a SIMPLE task. So, the complete lifecyle of this task (Poll, Update) should be handled by a worker to continue with W\\workflow execution. When Conductor has finished executing all the tasks in given flow, the workflow will reach Terminal state (COMPLETED, FAILED, TIMED_OUT etc.)","title":"Why is System task executed, but Worker task is Scheduled."},{"location":"labs/beginner/#next-steps","text":"You can play around this workflow by failing one of the Tasks, restarting or retrying the Workflow, or by tuning the number of retries, timeoutSeconds etc.","title":"Next steps"},{"location":"labs/eventhandlers/","text":"About \u00b6 In this Lab, we shall: Publish an Event to Conductor using Event task. Subscribe to Events, and perform actions: Start a Workflow Complete Task Conductor Supports Eventing with two Interfaces: Event Task Event Handlers We shall create a simple cyclic workflow similar to this: Create Workflow Definitions \u00b6 Let's create two workflows: test_workflow_for_eventHandler which will have an Event task to start another workflow, and a WAIT System task that will be completed by an event. test_workflow_startedBy_eventHandler which will have an Event task to generate an event to complete WAIT task in the above workflow. Send POST requests to /metadata/workflow endpoint with below payloads: { \"name\": \"test_workflow_for_eventHandler\", \"description\": \"A test workflow to start another workflow with EventHandler\", \"version\": 1, \"tasks\": [ { \"name\": \"test_start_workflow_event\", \"taskReferenceName\": \"start_workflow_with_event\", \"type\": \"EVENT\", \"sink\": \"conductor\" }, { \"name\": \"test_task_tobe_completed_by_eventHandler\", \"taskReferenceName\": \"test_task_tobe_completed_by_eventHandler\", \"type\": \"WAIT\" } ] } { \"name\": \"test_workflow_startedBy_eventHandler\", \"description\": \"A test workflow which is started by EventHandler, and then goes on to complete task in another workflow.\", \"version\": 1, \"tasks\": [ { \"name\": \"test_complete_task_event\", \"taskReferenceName\": \"complete_task_with_event\", \"inputParameters\": { \"sourceWorkflowId\": \"${workflow.input.sourceWorkflowId}\" }, \"type\": \"EVENT\", \"sink\": \"conductor\" } ] } Event Tasks in Workflow \u00b6 EVENT task is a System task, and we shall define it just like other Tasks in Workflow, with sink parameter. Also, EVENT task doesn't have to be registered before using in Workflow. This is also true for the WAIT task. Hence, we will not be registering any tasks for these workflows. Events are sent, but they're not handled (yet) \u00b6 Once you try to start test_workflow_for_eventHandler workflow, you would notice that the event is sent successfully, but the second worflow test_workflow_startedBy_eventHandler is not started. We have sent the Events, but we also need to define Event Handlers for Conductor to take any actions based on the Event. Let's create Event Handlers . Create Event Handlers \u00b6 Event Handler definitions are pretty much like Task or Workflow definitions. We start by name: { \"name\": \"test_start_workflow\" } Event Handler should know the Queue it has to listen to. This should be defined in event parameter. When using Conductor queues, define event with format: conductor:{workflow_name}:{taskReferenceName} And when using SQS, define with format: sqs:{my_sqs_queue_name} { \"name\": \"test_start_workflow\", \"event\": \"conductor:test_workflow_for_eventHandler:start_workflow_with_event\" } Event Handler can perform a list of actions defined in actions array parameter, for this particular event queue. { \"name\": \"test_start_workflow\", \"event\": \"conductor:test_workflow_for_eventHandler:start_workflow_with_event\", \"actions\": [ \"<insert-actions-here>\" ], \"active\": true } Let's define start_workflow action. We shall pass the name of workflow we would like to start. The start_workflow parameter can use any of the values from the general Start Workflow Request . Here we are passing in the workflowId, so that the Complete Task Event Handler can use it. { \"action\": \"start_workflow\", \"start_workflow\": { \"name\": \"test_workflow_startedBy_eventHandler\", \"input\": { \"sourceWorkflowId\": \"${workflowInstanceId}\" } } } Send a POST request to /event endpoint: { \"name\": \"test_start_workflow\", \"event\": \"conductor:test_workflow_for_eventHandler:start_workflow_with_event\", \"actions\": [ { \"action\": \"start_workflow\", \"start_workflow\": { \"name\": \"test_workflow_startedBy_eventHandler\", \"input\": { \"sourceWorkflowId\": \"${workflowInstanceId}\" } } } ], \"active\": true } Similarly, create another Event Handler to complete task. { \"name\": \"test_complete_task_event\", \"event\": \"conductor:test_workflow_startedBy_eventHandler:complete_task_with_event\", \"actions\": [ { \"action\": \"complete_task\", \"complete_task\": { \"workflowId\": \"${sourceWorkflowId}\", \"taskRefName\": \"test_task_tobe_completed_by_eventHandler\" } } ], \"active\": true } Final flow of Workflow \u00b6 After wiring all of the above, starting the test_workflow_for_eventHandler should: Start test_workflow_startedBy_eventHandler workflow. Sets test_task_tobe_completed_by_eventHandler WAIT task IN_PROGRESS . test_workflow_startedBy_eventHandler event task would publish an Event to complete the WAIT task above. Both the workflows would move to COMPLETED state.","title":"Events and Event Handlers"},{"location":"labs/eventhandlers/#about","text":"In this Lab, we shall: Publish an Event to Conductor using Event task. Subscribe to Events, and perform actions: Start a Workflow Complete Task Conductor Supports Eventing with two Interfaces: Event Task Event Handlers We shall create a simple cyclic workflow similar to this:","title":"About"},{"location":"labs/eventhandlers/#create-workflow-definitions","text":"Let's create two workflows: test_workflow_for_eventHandler which will have an Event task to start another workflow, and a WAIT System task that will be completed by an event. test_workflow_startedBy_eventHandler which will have an Event task to generate an event to complete WAIT task in the above workflow. Send POST requests to /metadata/workflow endpoint with below payloads: { \"name\": \"test_workflow_for_eventHandler\", \"description\": \"A test workflow to start another workflow with EventHandler\", \"version\": 1, \"tasks\": [ { \"name\": \"test_start_workflow_event\", \"taskReferenceName\": \"start_workflow_with_event\", \"type\": \"EVENT\", \"sink\": \"conductor\" }, { \"name\": \"test_task_tobe_completed_by_eventHandler\", \"taskReferenceName\": \"test_task_tobe_completed_by_eventHandler\", \"type\": \"WAIT\" } ] } { \"name\": \"test_workflow_startedBy_eventHandler\", \"description\": \"A test workflow which is started by EventHandler, and then goes on to complete task in another workflow.\", \"version\": 1, \"tasks\": [ { \"name\": \"test_complete_task_event\", \"taskReferenceName\": \"complete_task_with_event\", \"inputParameters\": { \"sourceWorkflowId\": \"${workflow.input.sourceWorkflowId}\" }, \"type\": \"EVENT\", \"sink\": \"conductor\" } ] }","title":"Create Workflow Definitions"},{"location":"labs/eventhandlers/#event-tasks-in-workflow","text":"EVENT task is a System task, and we shall define it just like other Tasks in Workflow, with sink parameter. Also, EVENT task doesn't have to be registered before using in Workflow. This is also true for the WAIT task. Hence, we will not be registering any tasks for these workflows.","title":"Event Tasks in Workflow"},{"location":"labs/eventhandlers/#events-are-sent-but-theyre-not-handled-yet","text":"Once you try to start test_workflow_for_eventHandler workflow, you would notice that the event is sent successfully, but the second worflow test_workflow_startedBy_eventHandler is not started. We have sent the Events, but we also need to define Event Handlers for Conductor to take any actions based on the Event. Let's create Event Handlers .","title":"Events are sent, but they're not handled (yet)"},{"location":"labs/eventhandlers/#create-event-handlers","text":"Event Handler definitions are pretty much like Task or Workflow definitions. We start by name: { \"name\": \"test_start_workflow\" } Event Handler should know the Queue it has to listen to. This should be defined in event parameter. When using Conductor queues, define event with format: conductor:{workflow_name}:{taskReferenceName} And when using SQS, define with format: sqs:{my_sqs_queue_name} { \"name\": \"test_start_workflow\", \"event\": \"conductor:test_workflow_for_eventHandler:start_workflow_with_event\" } Event Handler can perform a list of actions defined in actions array parameter, for this particular event queue. { \"name\": \"test_start_workflow\", \"event\": \"conductor:test_workflow_for_eventHandler:start_workflow_with_event\", \"actions\": [ \"<insert-actions-here>\" ], \"active\": true } Let's define start_workflow action. We shall pass the name of workflow we would like to start. The start_workflow parameter can use any of the values from the general Start Workflow Request . Here we are passing in the workflowId, so that the Complete Task Event Handler can use it. { \"action\": \"start_workflow\", \"start_workflow\": { \"name\": \"test_workflow_startedBy_eventHandler\", \"input\": { \"sourceWorkflowId\": \"${workflowInstanceId}\" } } } Send a POST request to /event endpoint: { \"name\": \"test_start_workflow\", \"event\": \"conductor:test_workflow_for_eventHandler:start_workflow_with_event\", \"actions\": [ { \"action\": \"start_workflow\", \"start_workflow\": { \"name\": \"test_workflow_startedBy_eventHandler\", \"input\": { \"sourceWorkflowId\": \"${workflowInstanceId}\" } } } ], \"active\": true } Similarly, create another Event Handler to complete task. { \"name\": \"test_complete_task_event\", \"event\": \"conductor:test_workflow_startedBy_eventHandler:complete_task_with_event\", \"actions\": [ { \"action\": \"complete_task\", \"complete_task\": { \"workflowId\": \"${sourceWorkflowId}\", \"taskRefName\": \"test_task_tobe_completed_by_eventHandler\" } } ], \"active\": true }","title":"Create Event Handlers"},{"location":"labs/eventhandlers/#final-flow-of-workflow","text":"After wiring all of the above, starting the test_workflow_for_eventHandler should: Start test_workflow_startedBy_eventHandler workflow. Sets test_task_tobe_completed_by_eventHandler WAIT task IN_PROGRESS . test_workflow_startedBy_eventHandler event task would publish an Event to complete the WAIT task above. Both the workflows would move to COMPLETED state.","title":"Final flow of Workflow"},{"location":"labs/kitchensink/","text":"An example kitchensink workflow that demonstrates the usage of all the schema constructs. Definition \u00b6 { \"name\": \"kitchensink\", \"description\": \"kitchensink workflow\", \"version\": 1, \"tasks\": [ { \"name\": \"task_1\", \"taskReferenceName\": \"task_1\", \"inputParameters\": { \"mod\": \"${workflow.input.mod}\", \"oddEven\": \"${workflow.input.oddEven}\" }, \"type\": \"SIMPLE\" }, { \"name\": \"event_task\", \"taskReferenceName\": \"event_0\", \"inputParameters\": { \"mod\": \"${workflow.input.mod}\", \"oddEven\": \"${workflow.input.oddEven}\" }, \"type\": \"EVENT\", \"sink\": \"conductor\" }, { \"name\": \"dyntask\", \"taskReferenceName\": \"task_2\", \"inputParameters\": { \"taskToExecute\": \"${workflow.input.task2Name}\" }, \"type\": \"DYNAMIC\", \"dynamicTaskNameParam\": \"taskToExecute\" }, { \"name\": \"oddEvenDecision\", \"taskReferenceName\": \"oddEvenDecision\", \"inputParameters\": { \"oddEven\": \"${task_2.output.oddEven}\" }, \"type\": \"DECISION\", \"caseValueParam\": \"oddEven\", \"decisionCases\": { \"0\": [ { \"name\": \"task_4\", \"taskReferenceName\": \"task_4\", \"inputParameters\": { \"mod\": \"${task_2.output.mod}\", \"oddEven\": \"${task_2.output.oddEven}\" }, \"type\": \"SIMPLE\" }, { \"name\": \"dynamic_fanout\", \"taskReferenceName\": \"fanout1\", \"inputParameters\": { \"dynamicTasks\": \"${task_4.output.dynamicTasks}\", \"input\": \"${task_4.output.inputs}\" }, \"type\": \"FORK_JOIN_DYNAMIC\", \"dynamicForkTasksParam\": \"dynamicTasks\", \"dynamicForkTasksInputParamName\": \"input\" }, { \"name\": \"dynamic_join\", \"taskReferenceName\": \"join1\", \"type\": \"JOIN\" } ], \"1\": [ { \"name\": \"fork_join\", \"taskReferenceName\": \"forkx\", \"type\": \"FORK_JOIN\", \"forkTasks\": [ [ { \"name\": \"task_10\", \"taskReferenceName\": \"task_10\", \"type\": \"SIMPLE\" }, { \"name\": \"sub_workflow_x\", \"taskReferenceName\": \"wf3\", \"inputParameters\": { \"mod\": \"${task_1.output.mod}\", \"oddEven\": \"${task_1.output.oddEven}\" }, \"type\": \"SUB_WORKFLOW\", \"subWorkflowParam\": { \"name\": \"sub_flow_1\", \"version\": 1 } } ], [ { \"name\": \"task_11\", \"taskReferenceName\": \"task_11\", \"type\": \"SIMPLE\" }, { \"name\": \"sub_workflow_x\", \"taskReferenceName\": \"wf4\", \"inputParameters\": { \"mod\": \"${task_1.output.mod}\", \"oddEven\": \"${task_1.output.oddEven}\" }, \"type\": \"SUB_WORKFLOW\", \"subWorkflowParam\": { \"name\": \"sub_flow_1\", \"version\": 1 } } ] ] }, { \"name\": \"join\", \"taskReferenceName\": \"join2\", \"type\": \"JOIN\", \"joinOn\": [ \"wf3\", \"wf4\" ] } ] } }, { \"name\": \"search_elasticsearch\", \"taskReferenceName\": \"get_es_1\", \"inputParameters\": { \"http_request\": { \"uri\": \"http://localhost:9200/conductor/_search?size=10\", \"method\": \"GET\" } }, \"type\": \"HTTP\" }, { \"name\": \"task_30\", \"taskReferenceName\": \"task_30\", \"inputParameters\": { \"statuses\": \"${get_es_1.output..status}\", \"workflowIds\": \"${get_es_1.output..workflowId}\" }, \"type\": \"SIMPLE\" } ], \"outputParameters\": { \"statues\": \"${get_es_1.output..status}\", \"workflowIds\": \"${get_es_1.output..workflowId}\" }, \"ownerEmail\": \"example@email.com\", \"schemaVersion\": 2 } Visual Flow \u00b6 Running Kitchensink Workflow \u00b6 Start the server as documented here . Use -DloadSample=true java system property when launching the server. This will create a kitchensink workflow, related task definitions and kick off an instance of kitchensink workflow. Once the workflow has started, the first task remains in the SCHEDULED state. This is because no workers are currently polling for the task. We will use the REST endpoints directly to poll for tasks and updating the status. Start workflow execution \u00b6 Start the execution of the kitchensink workflow by posting the following: curl -X POST --header 'Content-Type: application/json' --header 'Accept: text/plain' 'http://localhost:8080/api/workflow/kitchensink' -d ' { \"task2Name\": \"task_5\" } ' The response is a text string identifying the workflow instance id. Poll for the first task: \u00b6 curl http://localhost:8080/api/tasks/poll/task_1 The response should look something like: { \"taskType\": \"task_1\", \"status\": \"IN_PROGRESS\", \"inputData\": { \"mod\": null, \"oddEven\": null }, \"referenceTaskName\": \"task_1\", \"retryCount\": 0, \"seq\": 1, \"pollCount\": 1, \"taskDefName\": \"task_1\", \"scheduledTime\": 1486580932471, \"startTime\": 1486580933869, \"endTime\": 0, \"updateTime\": 1486580933902, \"startDelayInSeconds\": 0, \"retried\": false, \"callbackFromWorker\": true, \"responseTimeoutSeconds\": 3600, \"workflowInstanceId\": \"b0d1a935-3d74-46fd-92b2-0ca1e388659f\", \"taskId\": \"b9eea7dd-3fbd-46b9-a9ff-b00279459476\", \"callbackAfterSeconds\": 0, \"polledTime\": 1486580933902, \"queueWaitTime\": 1398 } Update the task status \u00b6 Note the values for taskId and workflowInstanceId fields from the poll response Update the status of the task as COMPLETED as below: curl -H 'Content-Type:application/json' -H 'Accept:application/json' -X POST http://localhost:8080/api/tasks/ -d ' { \"taskId\": \"b9eea7dd-3fbd-46b9-a9ff-b00279459476\", \"workflowInstanceId\": \"b0d1a935-3d74-46fd-92b2-0ca1e388659f\", \"status\": \"COMPLETED\", \"outputData\": { \"mod\": 5, \"taskToExecute\": \"task_1\", \"oddEven\": 0, \"dynamicTasks\": [ { \"name\": \"task_1\", \"taskReferenceName\": \"task_1_1\", \"type\": \"SIMPLE\" }, { \"name\": \"sub_workflow_4\", \"taskReferenceName\": \"wf_dyn\", \"type\": \"SUB_WORKFLOW\", \"subWorkflowParam\": { \"name\": \"sub_flow_1\" } } ], \"inputs\": { \"task_1_1\": {}, \"wf_dyn\": {} } } }' This will mark the task_1 as completed and schedule task_5 as the next task. Repeat the same process for the subsequently scheduled tasks until the completion.","title":"Kitchensink"},{"location":"labs/kitchensink/#definition","text":"{ \"name\": \"kitchensink\", \"description\": \"kitchensink workflow\", \"version\": 1, \"tasks\": [ { \"name\": \"task_1\", \"taskReferenceName\": \"task_1\", \"inputParameters\": { \"mod\": \"${workflow.input.mod}\", \"oddEven\": \"${workflow.input.oddEven}\" }, \"type\": \"SIMPLE\" }, { \"name\": \"event_task\", \"taskReferenceName\": \"event_0\", \"inputParameters\": { \"mod\": \"${workflow.input.mod}\", \"oddEven\": \"${workflow.input.oddEven}\" }, \"type\": \"EVENT\", \"sink\": \"conductor\" }, { \"name\": \"dyntask\", \"taskReferenceName\": \"task_2\", \"inputParameters\": { \"taskToExecute\": \"${workflow.input.task2Name}\" }, \"type\": \"DYNAMIC\", \"dynamicTaskNameParam\": \"taskToExecute\" }, { \"name\": \"oddEvenDecision\", \"taskReferenceName\": \"oddEvenDecision\", \"inputParameters\": { \"oddEven\": \"${task_2.output.oddEven}\" }, \"type\": \"DECISION\", \"caseValueParam\": \"oddEven\", \"decisionCases\": { \"0\": [ { \"name\": \"task_4\", \"taskReferenceName\": \"task_4\", \"inputParameters\": { \"mod\": \"${task_2.output.mod}\", \"oddEven\": \"${task_2.output.oddEven}\" }, \"type\": \"SIMPLE\" }, { \"name\": \"dynamic_fanout\", \"taskReferenceName\": \"fanout1\", \"inputParameters\": { \"dynamicTasks\": \"${task_4.output.dynamicTasks}\", \"input\": \"${task_4.output.inputs}\" }, \"type\": \"FORK_JOIN_DYNAMIC\", \"dynamicForkTasksParam\": \"dynamicTasks\", \"dynamicForkTasksInputParamName\": \"input\" }, { \"name\": \"dynamic_join\", \"taskReferenceName\": \"join1\", \"type\": \"JOIN\" } ], \"1\": [ { \"name\": \"fork_join\", \"taskReferenceName\": \"forkx\", \"type\": \"FORK_JOIN\", \"forkTasks\": [ [ { \"name\": \"task_10\", \"taskReferenceName\": \"task_10\", \"type\": \"SIMPLE\" }, { \"name\": \"sub_workflow_x\", \"taskReferenceName\": \"wf3\", \"inputParameters\": { \"mod\": \"${task_1.output.mod}\", \"oddEven\": \"${task_1.output.oddEven}\" }, \"type\": \"SUB_WORKFLOW\", \"subWorkflowParam\": { \"name\": \"sub_flow_1\", \"version\": 1 } } ], [ { \"name\": \"task_11\", \"taskReferenceName\": \"task_11\", \"type\": \"SIMPLE\" }, { \"name\": \"sub_workflow_x\", \"taskReferenceName\": \"wf4\", \"inputParameters\": { \"mod\": \"${task_1.output.mod}\", \"oddEven\": \"${task_1.output.oddEven}\" }, \"type\": \"SUB_WORKFLOW\", \"subWorkflowParam\": { \"name\": \"sub_flow_1\", \"version\": 1 } } ] ] }, { \"name\": \"join\", \"taskReferenceName\": \"join2\", \"type\": \"JOIN\", \"joinOn\": [ \"wf3\", \"wf4\" ] } ] } }, { \"name\": \"search_elasticsearch\", \"taskReferenceName\": \"get_es_1\", \"inputParameters\": { \"http_request\": { \"uri\": \"http://localhost:9200/conductor/_search?size=10\", \"method\": \"GET\" } }, \"type\": \"HTTP\" }, { \"name\": \"task_30\", \"taskReferenceName\": \"task_30\", \"inputParameters\": { \"statuses\": \"${get_es_1.output..status}\", \"workflowIds\": \"${get_es_1.output..workflowId}\" }, \"type\": \"SIMPLE\" } ], \"outputParameters\": { \"statues\": \"${get_es_1.output..status}\", \"workflowIds\": \"${get_es_1.output..workflowId}\" }, \"ownerEmail\": \"example@email.com\", \"schemaVersion\": 2 }","title":"Definition"},{"location":"labs/kitchensink/#visual-flow","text":"","title":"Visual Flow"},{"location":"labs/kitchensink/#running-kitchensink-workflow","text":"Start the server as documented here . Use -DloadSample=true java system property when launching the server. This will create a kitchensink workflow, related task definitions and kick off an instance of kitchensink workflow. Once the workflow has started, the first task remains in the SCHEDULED state. This is because no workers are currently polling for the task. We will use the REST endpoints directly to poll for tasks and updating the status.","title":"Running Kitchensink Workflow"},{"location":"labs/kitchensink/#start-workflow-execution","text":"Start the execution of the kitchensink workflow by posting the following: curl -X POST --header 'Content-Type: application/json' --header 'Accept: text/plain' 'http://localhost:8080/api/workflow/kitchensink' -d ' { \"task2Name\": \"task_5\" } ' The response is a text string identifying the workflow instance id.","title":"Start workflow execution"},{"location":"labs/kitchensink/#poll-for-the-first-task","text":"curl http://localhost:8080/api/tasks/poll/task_1 The response should look something like: { \"taskType\": \"task_1\", \"status\": \"IN_PROGRESS\", \"inputData\": { \"mod\": null, \"oddEven\": null }, \"referenceTaskName\": \"task_1\", \"retryCount\": 0, \"seq\": 1, \"pollCount\": 1, \"taskDefName\": \"task_1\", \"scheduledTime\": 1486580932471, \"startTime\": 1486580933869, \"endTime\": 0, \"updateTime\": 1486580933902, \"startDelayInSeconds\": 0, \"retried\": false, \"callbackFromWorker\": true, \"responseTimeoutSeconds\": 3600, \"workflowInstanceId\": \"b0d1a935-3d74-46fd-92b2-0ca1e388659f\", \"taskId\": \"b9eea7dd-3fbd-46b9-a9ff-b00279459476\", \"callbackAfterSeconds\": 0, \"polledTime\": 1486580933902, \"queueWaitTime\": 1398 }","title":"Poll for the first task:"},{"location":"labs/kitchensink/#update-the-task-status","text":"Note the values for taskId and workflowInstanceId fields from the poll response Update the status of the task as COMPLETED as below: curl -H 'Content-Type:application/json' -H 'Accept:application/json' -X POST http://localhost:8080/api/tasks/ -d ' { \"taskId\": \"b9eea7dd-3fbd-46b9-a9ff-b00279459476\", \"workflowInstanceId\": \"b0d1a935-3d74-46fd-92b2-0ca1e388659f\", \"status\": \"COMPLETED\", \"outputData\": { \"mod\": 5, \"taskToExecute\": \"task_1\", \"oddEven\": 0, \"dynamicTasks\": [ { \"name\": \"task_1\", \"taskReferenceName\": \"task_1_1\", \"type\": \"SIMPLE\" }, { \"name\": \"sub_workflow_4\", \"taskReferenceName\": \"wf_dyn\", \"type\": \"SUB_WORKFLOW\", \"subWorkflowParam\": { \"name\": \"sub_flow_1\" } } ], \"inputs\": { \"task_1_1\": {}, \"wf_dyn\": {} } } }' This will mark the task_1 as completed and schedule task_5 as the next task. Repeat the same process for the subsequently scheduled tasks until the completion.","title":"Update the task status"},{"location":"labs/running-first-worker/","text":"Running First Worker \u00b6 In this article we will explore how you can get your first worker task running. We are hosting the code used in this article in the following location. You can clone and use it as a reference locally. https://github.com/orkes-io/orkesworkers \u00b6 In the previous article, you used an HTTP task run your first simple workflow. Now it's time to explore how to run a custom worker that you will implement yourself. After completing the steps in this article, you will: Learn about a SIMPLE worker type which runs your custom code Learn about how a custom worker task runs from your environment and connects to Conductor Worker tasks are implemented by your application(s) and runs in a separate environment from Conductor. The worker tasks can be implemented in any language. These tasks talk to Conductor server via REST/gRPC to poll for tasks and update its status after execution. In our example we will be implementing a Java based worker by leveraging the official Java Client SDK. Worker tasks are identified by task type SIMPLE in the workflow JSON definition. Step 1 - Register the Worker Task \u00b6 First let's create task definition for \"simple_worker\". Send a POST request to /metadata/taskdefs API endpoint on your conductor server to register these tasks. [ { \"name\": \"simple_worker\", \"retryCount\": 3, \"retryLogic\": \"FIXED\", \"retryDelaySeconds\": 10, \"timeoutSeconds\": 300, \"timeoutPolicy\": \"TIME_OUT_WF\", \"responseTimeoutSeconds\": 180, \"ownerEmail\": \"example@gmail.com\" } ] Here is the curl command to do that curl 'http://localhost:8080/api/metadata/taskdefs' \\ -H 'accept: */*' \\ -H 'Referer: ' \\ -H 'Content-Type: application/json' \\ --data-raw '[{\"name\":\"simple_worker\",\"retryCount\":3,\"retryLogic\":\"FIXED\",\"retryDelaySeconds\":10,\"timeoutSeconds\":300,\"timeoutPolicy\":\"TIME_OUT_WF\",\"responseTimeoutSeconds\":180,\"ownerEmail\":\"example@gmail.com\"}]' You can also use the Conductor Swagger API UI to make this call. Here is an overview of the task fields that we just created \"name\" : Name of your worker. This should be unique. \"retryCount\" : The number of times Conductor should retry your worker task in the event of an unexpected failure \"retryLogic\" : FIXED - The retry logic - options are FIXED and EXPONENTIAL_BACKOFF \"retryDelaySeconds\" : Time to wait before retries \"timeoutSeconds\" : Time in seconds, after which the task is marked as TIMED_OUT if not completed after transitioning to IN_PROGRESS status for the first time \"timeoutPolicy\" : TIME_OUT_WF - Task's timeout policy. Options can be found here \"responseTimeoutSeconds\" : Must be greater than 0 and less than timeoutSeconds. The task is rescheduled if not updated with a status after this time (heartbeat mechanism). Useful when the worker polls for the task but fails to complete due to errors/network failure. Defaults to 3600 \"ownerEmail\" : Mandatory metadata to manage who created or owns this worker definition in a shared conductor environment. More details on the fields used and the remaining fields in the task definition can be found here . Step 2 - Create a Workflow definition \u00b6 Creating Workflow definition is similar to creating the task definition. In our workflow, we will use the task we defined earlier. Note that same Task definitions can be used in multiple workflows, or for multiple times in same Workflow (that's where taskReferenceName is useful). { \"createTime\": 1634021619147, \"updateTime\": 1630694890267, \"name\": \"first_sample_workflow_with_worker\", \"description\": \"First Sample Workflow With Worker\", \"version\": 1, \"tasks\": [ { \"name\": \"simple_worker\", \"taskReferenceName\": \"simple_worker_ref_1\", \"inputParameters\": {}, \"type\": \"SIMPLE\" } ], \"inputParameters\": [], \"outputParameters\": { \"currentTimeOnServer\": \"${simple_worker_ref_1.output.currentTimeOnServer}\", \"message\": \"${simple_worker_ref_1.output.message}\" }, \"schemaVersion\": 2, \"restartable\": true, \"ownerEmail\": \"example@email.com\", \"timeoutPolicy\": \"ALERT_ONLY\", \"timeoutSeconds\": 0 } Notice that in the workflow definition, we are using a single worker task using the task worker definition we created earlier. The task is of type SIMPLE . To create this workflow in your Conductor server using CURL, use the following: curl 'http://localhost:8080/api/metadata/workflow' \\ -H 'accept: */*' \\ -H 'Referer: ' \\ -H 'Content-Type: application/json' \\ --data-raw '{\"createTime\":1634021619147,\"updateTime\":1630694890267,\"name\":\"first_sample_workflow_with_worker\",\"description\":\"First Sample Workflow With Worker\",\"version\":1,\"tasks\":[{\"name\":\"simple_worker\",\"taskReferenceName\":\"simple_worker_ref_1\",\"inputParameters\":{},\"type\":\"SIMPLE\"}],\"inputParameters\":[],\"outputParameters\":{\"currentTimeOnServer\":\"${simple_worker_ref_1.output.currentTimeOnServer}\",\"message\":\"${simple_worker_ref_1.output.message}\"},\"schemaVersion\":2,\"restartable\":true,\"ownerEmail\":\"example@email.com\",\"timeoutPolicy\":\"ALERT_ONLY\",\"timeoutSeconds\":0}' Step 3 - Starting the Workflow \u00b6 This workflow doesn't need any inputs. So we can issue a curl command to start it. curl 'http://localhost:8080/api/workflow/first_sample_workflow_with_worker' \\ -H 'accept: text/plain' \\ -H 'Referer: ' \\ -H 'Content-Type: application/json' \\ --data-raw '{}' The API path contains the workflow name first_sample_workflow_with_worker and in our workflow since we don't need any inputs we will specify {} Successful POST request should return a workflow id, which you can use to find the execution in the UI by navigating to http://localhost:5000/execution/<workflowId> . Note: You can also run this using the Swagger UI instead of curl. Step 4 - Poll for Worker Task \u00b6 To get your Worker taskId, you first naviaget to http://localhost:5000/execution/<workflowId> . Next, click on the simple_worker_ref_1 in the UI. This will open a summary pane with the Task Execution ID If you look up the worker using the following URL http://localhost:8080/api/tasks/{taskId} , you will notice that the worker is in SCHEDULED state. This is because we haven't implemented the worker yet. Let's walk through the steps to implement the worker. Prerequisite \u00b6 Setup a Java project on your local. You can also use an existing Java project if you already have one Adding worker implementation \u00b6 In your project, add the following dependencies. We are showing here how you will do this in gradle: implementation(\"com.netflix.conductor:conductor-client:${versions.conductor}\") { exclude group: 'com.github.vmg.protogen', module: 'protogen-annotations' } implementation(\"com.netflix.conductor:conductor-common:${versions.conductor}\") { exclude group: 'com.github.vmg.protogen', module: 'protogen-annotations' } See full example on GitHub You can do this for Maven as well, just need to use the appropriate syntax. We will need the following two libraries available in maven repo and you can use the latest version if required. com.netflix.conductor:conductor-client:3.0.6 com.netflix.conductor:conductor-common:3.0.6 Now \"simple_worker\" task is in SCHEDULED state, it is worker's turn to fetch the task, execute it and update Conductor with final status of the task. A class needs to be created which implements Worker and defines its methods. Note : Make sure the method getTaskDefName returns string same as the task name we defined in step 1 ( simple_worker ). ```js reference https://github.com/orkes-io/orkesworkers/blob/main/src/main/java/io/orkes/samples/workers/SimpleWorker.java#L11-L30 Awesome, you have implemented your first worker in code! Amazing. #### Connecting, Polling and Executing your Worker In your main method or where ever your application starts, you will need to configure a class called `TaskRunnerConfigurer` and initialize it. This is the step that makes your code connect to a Conductor server. Here we have used a SpringBoot based Java application and we are showing you how to create a Bean for this class. ```js reference https://github.com/orkes-io/orkesworkers/blob/main/src/main/java/io/orkes/samples/OrkesWorkersApplication.java#L18-L45 This is the place where you define your conductor server URL: env.getProperty(\"conductor.server.url\") We have defined this in a property file as shown below. You can also hard code this. ```javascript reference https://github.com/orkes-io/orkesworkers/blob/main/src/main/resources/application.properties#L1-L1 That's it. You are all set. Run your Java application to start running your worker. After running your application, it will be able to poll and run your worker. Let's go back and load up the previously created workflow in your UI. ![Conductor UI - Workflow Run](../img/tutorial/successfulWorkerExecution.png) In your worker you had this implementation: ```js result.addOutputData(\"currentTimeOnServer\", currentTimeOnServer); result.addOutputData(\"message\", \"Hello World!\"); As you can see in the screenshot above the worker has added these outputs to the workflow run. Play around with this, change the outputs and re-run and see how it works. Summary \u00b6 In this blog post \u2014 we learned how to run a sample workflow in our Conductor installation with a custom worker. Concepts we touched on: Adding Task (worker) definition Adding Workflow definition with a custom SIMPLE task Running Conductor client using the Java SDK Thank you for reading, and we hope you found this helpful. Please feel free to reach out to us for any questions and we are happy to help in any way we can.","title":"A First Worker"},{"location":"labs/running-first-worker/#running-first-worker","text":"In this article we will explore how you can get your first worker task running. We are hosting the code used in this article in the following location. You can clone and use it as a reference locally.","title":"Running First Worker"},{"location":"labs/running-first-worker/#httpsgithubcomorkes-ioorkesworkers","text":"In the previous article, you used an HTTP task run your first simple workflow. Now it's time to explore how to run a custom worker that you will implement yourself. After completing the steps in this article, you will: Learn about a SIMPLE worker type which runs your custom code Learn about how a custom worker task runs from your environment and connects to Conductor Worker tasks are implemented by your application(s) and runs in a separate environment from Conductor. The worker tasks can be implemented in any language. These tasks talk to Conductor server via REST/gRPC to poll for tasks and update its status after execution. In our example we will be implementing a Java based worker by leveraging the official Java Client SDK. Worker tasks are identified by task type SIMPLE in the workflow JSON definition.","title":"https://github.com/orkes-io/orkesworkers"},{"location":"labs/running-first-worker/#step-1-register-the-worker-task","text":"First let's create task definition for \"simple_worker\". Send a POST request to /metadata/taskdefs API endpoint on your conductor server to register these tasks. [ { \"name\": \"simple_worker\", \"retryCount\": 3, \"retryLogic\": \"FIXED\", \"retryDelaySeconds\": 10, \"timeoutSeconds\": 300, \"timeoutPolicy\": \"TIME_OUT_WF\", \"responseTimeoutSeconds\": 180, \"ownerEmail\": \"example@gmail.com\" } ] Here is the curl command to do that curl 'http://localhost:8080/api/metadata/taskdefs' \\ -H 'accept: */*' \\ -H 'Referer: ' \\ -H 'Content-Type: application/json' \\ --data-raw '[{\"name\":\"simple_worker\",\"retryCount\":3,\"retryLogic\":\"FIXED\",\"retryDelaySeconds\":10,\"timeoutSeconds\":300,\"timeoutPolicy\":\"TIME_OUT_WF\",\"responseTimeoutSeconds\":180,\"ownerEmail\":\"example@gmail.com\"}]' You can also use the Conductor Swagger API UI to make this call. Here is an overview of the task fields that we just created \"name\" : Name of your worker. This should be unique. \"retryCount\" : The number of times Conductor should retry your worker task in the event of an unexpected failure \"retryLogic\" : FIXED - The retry logic - options are FIXED and EXPONENTIAL_BACKOFF \"retryDelaySeconds\" : Time to wait before retries \"timeoutSeconds\" : Time in seconds, after which the task is marked as TIMED_OUT if not completed after transitioning to IN_PROGRESS status for the first time \"timeoutPolicy\" : TIME_OUT_WF - Task's timeout policy. Options can be found here \"responseTimeoutSeconds\" : Must be greater than 0 and less than timeoutSeconds. The task is rescheduled if not updated with a status after this time (heartbeat mechanism). Useful when the worker polls for the task but fails to complete due to errors/network failure. Defaults to 3600 \"ownerEmail\" : Mandatory metadata to manage who created or owns this worker definition in a shared conductor environment. More details on the fields used and the remaining fields in the task definition can be found here .","title":"Step 1 - Register the Worker Task"},{"location":"labs/running-first-worker/#step-2-create-a-workflow-definition","text":"Creating Workflow definition is similar to creating the task definition. In our workflow, we will use the task we defined earlier. Note that same Task definitions can be used in multiple workflows, or for multiple times in same Workflow (that's where taskReferenceName is useful). { \"createTime\": 1634021619147, \"updateTime\": 1630694890267, \"name\": \"first_sample_workflow_with_worker\", \"description\": \"First Sample Workflow With Worker\", \"version\": 1, \"tasks\": [ { \"name\": \"simple_worker\", \"taskReferenceName\": \"simple_worker_ref_1\", \"inputParameters\": {}, \"type\": \"SIMPLE\" } ], \"inputParameters\": [], \"outputParameters\": { \"currentTimeOnServer\": \"${simple_worker_ref_1.output.currentTimeOnServer}\", \"message\": \"${simple_worker_ref_1.output.message}\" }, \"schemaVersion\": 2, \"restartable\": true, \"ownerEmail\": \"example@email.com\", \"timeoutPolicy\": \"ALERT_ONLY\", \"timeoutSeconds\": 0 } Notice that in the workflow definition, we are using a single worker task using the task worker definition we created earlier. The task is of type SIMPLE . To create this workflow in your Conductor server using CURL, use the following: curl 'http://localhost:8080/api/metadata/workflow' \\ -H 'accept: */*' \\ -H 'Referer: ' \\ -H 'Content-Type: application/json' \\ --data-raw '{\"createTime\":1634021619147,\"updateTime\":1630694890267,\"name\":\"first_sample_workflow_with_worker\",\"description\":\"First Sample Workflow With Worker\",\"version\":1,\"tasks\":[{\"name\":\"simple_worker\",\"taskReferenceName\":\"simple_worker_ref_1\",\"inputParameters\":{},\"type\":\"SIMPLE\"}],\"inputParameters\":[],\"outputParameters\":{\"currentTimeOnServer\":\"${simple_worker_ref_1.output.currentTimeOnServer}\",\"message\":\"${simple_worker_ref_1.output.message}\"},\"schemaVersion\":2,\"restartable\":true,\"ownerEmail\":\"example@email.com\",\"timeoutPolicy\":\"ALERT_ONLY\",\"timeoutSeconds\":0}'","title":"Step 2 - Create a Workflow definition"},{"location":"labs/running-first-worker/#step-3-starting-the-workflow","text":"This workflow doesn't need any inputs. So we can issue a curl command to start it. curl 'http://localhost:8080/api/workflow/first_sample_workflow_with_worker' \\ -H 'accept: text/plain' \\ -H 'Referer: ' \\ -H 'Content-Type: application/json' \\ --data-raw '{}' The API path contains the workflow name first_sample_workflow_with_worker and in our workflow since we don't need any inputs we will specify {} Successful POST request should return a workflow id, which you can use to find the execution in the UI by navigating to http://localhost:5000/execution/<workflowId> . Note: You can also run this using the Swagger UI instead of curl.","title":"Step 3 - Starting the Workflow"},{"location":"labs/running-first-worker/#step-4-poll-for-worker-task","text":"To get your Worker taskId, you first naviaget to http://localhost:5000/execution/<workflowId> . Next, click on the simple_worker_ref_1 in the UI. This will open a summary pane with the Task Execution ID If you look up the worker using the following URL http://localhost:8080/api/tasks/{taskId} , you will notice that the worker is in SCHEDULED state. This is because we haven't implemented the worker yet. Let's walk through the steps to implement the worker.","title":"Step 4 - Poll for Worker Task"},{"location":"labs/running-first-worker/#prerequisite","text":"Setup a Java project on your local. You can also use an existing Java project if you already have one","title":"Prerequisite"},{"location":"labs/running-first-worker/#adding-worker-implementation","text":"In your project, add the following dependencies. We are showing here how you will do this in gradle: implementation(\"com.netflix.conductor:conductor-client:${versions.conductor}\") { exclude group: 'com.github.vmg.protogen', module: 'protogen-annotations' } implementation(\"com.netflix.conductor:conductor-common:${versions.conductor}\") { exclude group: 'com.github.vmg.protogen', module: 'protogen-annotations' } See full example on GitHub You can do this for Maven as well, just need to use the appropriate syntax. We will need the following two libraries available in maven repo and you can use the latest version if required. com.netflix.conductor:conductor-client:3.0.6 com.netflix.conductor:conductor-common:3.0.6 Now \"simple_worker\" task is in SCHEDULED state, it is worker's turn to fetch the task, execute it and update Conductor with final status of the task. A class needs to be created which implements Worker and defines its methods. Note : Make sure the method getTaskDefName returns string same as the task name we defined in step 1 ( simple_worker ). ```js reference https://github.com/orkes-io/orkesworkers/blob/main/src/main/java/io/orkes/samples/workers/SimpleWorker.java#L11-L30 Awesome, you have implemented your first worker in code! Amazing. #### Connecting, Polling and Executing your Worker In your main method or where ever your application starts, you will need to configure a class called `TaskRunnerConfigurer` and initialize it. This is the step that makes your code connect to a Conductor server. Here we have used a SpringBoot based Java application and we are showing you how to create a Bean for this class. ```js reference https://github.com/orkes-io/orkesworkers/blob/main/src/main/java/io/orkes/samples/OrkesWorkersApplication.java#L18-L45 This is the place where you define your conductor server URL: env.getProperty(\"conductor.server.url\") We have defined this in a property file as shown below. You can also hard code this. ```javascript reference https://github.com/orkes-io/orkesworkers/blob/main/src/main/resources/application.properties#L1-L1 That's it. You are all set. Run your Java application to start running your worker. After running your application, it will be able to poll and run your worker. Let's go back and load up the previously created workflow in your UI. ![Conductor UI - Workflow Run](../img/tutorial/successfulWorkerExecution.png) In your worker you had this implementation: ```js result.addOutputData(\"currentTimeOnServer\", currentTimeOnServer); result.addOutputData(\"message\", \"Hello World!\"); As you can see in the screenshot above the worker has added these outputs to the workflow run. Play around with this, change the outputs and re-run and see how it works.","title":"Adding worker implementation"},{"location":"labs/running-first-worker/#summary","text":"In this blog post \u2014 we learned how to run a sample workflow in our Conductor installation with a custom worker. Concepts we touched on: Adding Task (worker) definition Adding Workflow definition with a custom SIMPLE task Running Conductor client using the Java SDK Thank you for reading, and we hope you found this helpful. Please feel free to reach out to us for any questions and we are happy to help in any way we can.","title":"Summary"},{"location":"labs/running-first-workflow/","text":"Running First Workflow \u00b6 In this article we will explore how we can run a really simple workflow that runs without deploying any new microservice. Conductor can orchestrate HTTP services out of the box without implementing any code. We will use that to create and run the first workflow. See System Task for the list of such built-in tasks. Using system tasks is a great way to run a lot of our code in production. To bring up a local instance of Conductor follow one of the recommended steps: 1. Running Locally - From Code 2. Running Locally - Docker Compose Configuring our First Workflow \u00b6 This is a sample workflow that we can leverage for our test. { \"name\": \"first_sample_workflow\", \"description\": \"First Sample Workflow\", \"version\": 1, \"tasks\": [ { \"name\": \"get_population_data\", \"taskReferenceName\": \"get_population_data\", \"inputParameters\": { \"http_request\": { \"uri\": \"https://datausa.io/api/data?drilldowns=Nation&measures=Population\", \"method\": \"GET\" } }, \"type\": \"HTTP\" } ], \"inputParameters\": [], \"outputParameters\": { \"data\": \"${get_population_data.output.response.body.data}\", \"source\": \"${get_population_data.output.response.body.source}\" }, \"schemaVersion\": 2, \"restartable\": true, \"workflowStatusListenerEnabled\": false, \"ownerEmail\": \"example@email.com\", \"timeoutPolicy\": \"ALERT_ONLY\", \"timeoutSeconds\": 0 } This is an example workflow that queries a publicly available JSON API to retrieve some data. This workflow doesn\u2019t require any worker implementation as the tasks in this workflow are managed by the system itself. This is an awesome feature of Conductor. For a lot of typical work, we won\u2019t have to write any code at all. Let's talk about this workflow a little more so that we can gain some context. \"name\" : \"first_sample_workflow\" This line here is how we name our workflow. In this case our workflow name is first_sample_workflow This workflow contains just one worker. The workers are defined under the key tasks . Here is the worker definition with the most important values: { \"name\": \"get_population_data\", \"taskReferenceName\": \"get_population_data\", \"inputParameters\": { \"http_request\": { \"uri\": \"https://datausa.io/api/data?drilldowns=Nation&measures=Population\", \"method\": \"GET\" } }, \"type\": \"HTTP\" } Here is a list of fields and what it does: \"name\" : Name of our worker \"taskReferenceName\" : This is a reference to this worker in this specific workflow implementation. We can have multiple workers of the same name in our workflow, but we will need a unique task reference name for each of them. Task reference name should be unique across our entire workflow. \"inputParameters\" : These are the inputs into our worker. We can hard code inputs as we have done here. We can also provide dynamic inputs such as from the workflow input or based on the output of another worker. We can find examples of this in our documentation. \"type\" : This is what defines what the type of worker is. In our example - this is HTTP . There are more task types which we can find in the Conductor documentation. \"http_request\" : This is an input that is required for tasks of type HTTP . In our example we have provided a well known internet JSON API url and the type of HTTP method to invoke - GET We haven't talked about the other fields that we can use in our definitions as these are either just metadata or more advanced concepts which we can learn more in the detailed documentation. Ok, now that we have walked through our workflow details, let's run this and see how it works. To configure the workflow, head over to the swagger API of conductor server and access the metadata workflow create API: http://localhost:8080/swagger-ui/index.html?configUrl=/api-docs/swagger-config#/metadata-resource/create If the link doesn\u2019t open the right Swagger section, we can navigate to Metadata-Resource \u2192 POST /api/metadata/workflow Paste the workflow payload into the Swagger API and hit Execute. Now if we head over to the UI, we can see this workflow definition created: If we click through we can see a visual representation of the workflow: 2. Running our First Workflow \u00b6 Let\u2019s run this workflow. To do that we can use the swagger API under the workflow-resources http://localhost:8080/swagger-ui/index.html?configUrl=/api-docs/swagger-config#/workflow-resource/startWorkflow_1 Hit Execute ! Conductor will return a workflow id. We will need to use this id to load this up on the UI. If our UI installation has search enabled we wouldn't need to copy this. If we don't have search enabled (using Elasticsearch) copy it from the Swagger UI. Ok, we should see this running and get completed soon. Let\u2019s go to the UI to see what happened. To load the workflow directly, use this URL format: http://localhost:5000/execution/<WORKFLOW_ID> Replace <WORKFLOW_ID> with our workflow id from the previous step. We should see a screen like below. Click on the different tabs to see all inputs and outputs and task list etc. Explore away! Summary \u00b6 In this blog post \u2014 we learned how to run a sample workflow in our Conductor installation. Concepts we touched on: Workflow creation System tasks such as HTTP Running a workflow via API Thank you for reading, and we hope you found this helpful. Please feel free to reach out to us for any questions and we are happy to help in any way we can.","title":"A First Workflow"},{"location":"labs/running-first-workflow/#running-first-workflow","text":"In this article we will explore how we can run a really simple workflow that runs without deploying any new microservice. Conductor can orchestrate HTTP services out of the box without implementing any code. We will use that to create and run the first workflow. See System Task for the list of such built-in tasks. Using system tasks is a great way to run a lot of our code in production. To bring up a local instance of Conductor follow one of the recommended steps: 1. Running Locally - From Code 2. Running Locally - Docker Compose","title":"Running First Workflow"},{"location":"labs/running-first-workflow/#configuring-our-first-workflow","text":"This is a sample workflow that we can leverage for our test. { \"name\": \"first_sample_workflow\", \"description\": \"First Sample Workflow\", \"version\": 1, \"tasks\": [ { \"name\": \"get_population_data\", \"taskReferenceName\": \"get_population_data\", \"inputParameters\": { \"http_request\": { \"uri\": \"https://datausa.io/api/data?drilldowns=Nation&measures=Population\", \"method\": \"GET\" } }, \"type\": \"HTTP\" } ], \"inputParameters\": [], \"outputParameters\": { \"data\": \"${get_population_data.output.response.body.data}\", \"source\": \"${get_population_data.output.response.body.source}\" }, \"schemaVersion\": 2, \"restartable\": true, \"workflowStatusListenerEnabled\": false, \"ownerEmail\": \"example@email.com\", \"timeoutPolicy\": \"ALERT_ONLY\", \"timeoutSeconds\": 0 } This is an example workflow that queries a publicly available JSON API to retrieve some data. This workflow doesn\u2019t require any worker implementation as the tasks in this workflow are managed by the system itself. This is an awesome feature of Conductor. For a lot of typical work, we won\u2019t have to write any code at all. Let's talk about this workflow a little more so that we can gain some context. \"name\" : \"first_sample_workflow\" This line here is how we name our workflow. In this case our workflow name is first_sample_workflow This workflow contains just one worker. The workers are defined under the key tasks . Here is the worker definition with the most important values: { \"name\": \"get_population_data\", \"taskReferenceName\": \"get_population_data\", \"inputParameters\": { \"http_request\": { \"uri\": \"https://datausa.io/api/data?drilldowns=Nation&measures=Population\", \"method\": \"GET\" } }, \"type\": \"HTTP\" } Here is a list of fields and what it does: \"name\" : Name of our worker \"taskReferenceName\" : This is a reference to this worker in this specific workflow implementation. We can have multiple workers of the same name in our workflow, but we will need a unique task reference name for each of them. Task reference name should be unique across our entire workflow. \"inputParameters\" : These are the inputs into our worker. We can hard code inputs as we have done here. We can also provide dynamic inputs such as from the workflow input or based on the output of another worker. We can find examples of this in our documentation. \"type\" : This is what defines what the type of worker is. In our example - this is HTTP . There are more task types which we can find in the Conductor documentation. \"http_request\" : This is an input that is required for tasks of type HTTP . In our example we have provided a well known internet JSON API url and the type of HTTP method to invoke - GET We haven't talked about the other fields that we can use in our definitions as these are either just metadata or more advanced concepts which we can learn more in the detailed documentation. Ok, now that we have walked through our workflow details, let's run this and see how it works. To configure the workflow, head over to the swagger API of conductor server and access the metadata workflow create API: http://localhost:8080/swagger-ui/index.html?configUrl=/api-docs/swagger-config#/metadata-resource/create If the link doesn\u2019t open the right Swagger section, we can navigate to Metadata-Resource \u2192 POST /api/metadata/workflow Paste the workflow payload into the Swagger API and hit Execute. Now if we head over to the UI, we can see this workflow definition created: If we click through we can see a visual representation of the workflow:","title":"Configuring our First Workflow"},{"location":"labs/running-first-workflow/#2-running-our-first-workflow","text":"Let\u2019s run this workflow. To do that we can use the swagger API under the workflow-resources http://localhost:8080/swagger-ui/index.html?configUrl=/api-docs/swagger-config#/workflow-resource/startWorkflow_1 Hit Execute ! Conductor will return a workflow id. We will need to use this id to load this up on the UI. If our UI installation has search enabled we wouldn't need to copy this. If we don't have search enabled (using Elasticsearch) copy it from the Swagger UI. Ok, we should see this running and get completed soon. Let\u2019s go to the UI to see what happened. To load the workflow directly, use this URL format: http://localhost:5000/execution/<WORKFLOW_ID> Replace <WORKFLOW_ID> with our workflow id from the previous step. We should see a screen like below. Click on the different tabs to see all inputs and outputs and task list etc. Explore away!","title":"2. Running our First Workflow"},{"location":"labs/running-first-workflow/#summary","text":"In this blog post \u2014 we learned how to run a sample workflow in our Conductor installation. Concepts we touched on: Workflow creation System tasks such as HTTP Running a workflow via API Thank you for reading, and we hope you found this helpful. Please feel free to reach out to us for any questions and we are happy to help in any way we can.","title":"Summary"},{"location":"metrics/client/","text":"When using the Java client, the following metrics are published: Name Purpose Tags task_execution_queue_full Counter to record execution queue has saturated taskType task_poll_error Client error when polling for a task queue taskType, includeRetries, status task_paused Counter for number of times the task has been polled, when the worker has been paused taskType task_execute_error Execution error taskType task_ack_failed Task ack failed taskType task_ack_error Task ack has encountered an exception taskType task_update_error Task status cannot be updated back to server taskType task_poll_counter Incremented each time polling is done taskType task_poll_time Time to poll for a batch of tasks taskType task_execute_time Time to execute a task taskType task_result_size Records output payload size of a task taskType workflow_input_size Records input payload size of a workflow workflowType, workflowVersion external_payload_used Incremented each time external payload storage is used name, operation, payloadType Metrics on client side supplements the one collected from server in identifying the network as well as client side issues.","title":"Client Metrics"},{"location":"metrics/server/","text":"Publishing metrics \u00b6 Conductor uses spectator to collect the metrics. To enable conductor serve to publish metrics, add this dependency to your build.gradle. Conductor Server enables you to load additional modules dynamically, this feature can be controlled using this configuration . Create your own AbstractModule that overides configure function and registers the Spectator metrics registry. Initialize the Registry and add it to the global registry via ((CompositeRegistry)Spectator.globalRegistry()).add(...) . The following metrics are published by the server. You can use these metrics to configure alerts for your workflows and tasks. Name Purpose Tags workflow_server_error Rate at which server side error is happening methodName workflow_failure Counter for failing workflows workflowName, status workflow_start_error Counter for failing to start a workflow workflowName workflow_running Counter for no. of running workflows workflowName, version workflow_execution Timer for Workflow completion workflowName, ownerApp task_queue_wait Time spent by a task in queue taskType task_execution Time taken to execute a task taskType, includeRetries, status task_poll Time taken to poll for a task taskType task_poll_count Counter for number of times the task is being polled taskType, domain task_queue_depth Pending tasks queue depth taskType, ownerApp task_rate_limited Current number of tasks being rate limited taskType task_concurrent_execution_limited Current number of tasks being limited by concurrent execution limit taskType task_timeout Counter for timed out tasks taskType task_response_timeout Counter for tasks timedout due to responseTimeout taskType task_update_conflict Counter for task update conflicts. Eg: when the workflow is in terminal state workflowName, taskType, taskStatus, workflowStatus event_queue_messages_processed Counter for number of messages fetched from an event queue queueType, queueName observable_queue_error Counter for number of errors encountered when fetching messages from an event queue queueType event_queue_messages_handled Counter for number of messages executed from an event queue queueType, queueName external_payload_storage_usage Counter for number of times external payload storage was used name, operation, payloadType Collecting metrics with Log4j \u00b6 One way of collecting metrics is to push them into the logging framework (log4j). Log4j supports various appenders that can print metrics into a console/file or even send them to remote metrics collectors over e.g. syslog channel. Conductor provides optional modules that connect metrics registry with the logging framework. To enable these modules, configure following additional modules property in config.properties: conductor.metrics-logger.enabled = true conductor.metrics-logger.reportPeriodSeconds = 15 This will push all available metrics into log4j every 15 seconds. By default, the metrics will be handled as a regular log message (just printed to console with default log4j.properties). In order to change that, you can use following log4j configuration that prints metrics into a dedicated file: log4j.rootLogger=INFO,console,file log4j.appender.console=org.apache.log4j.ConsoleAppender log4j.appender.console.layout=org.apache.log4j.PatternLayout log4j.appender.console.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n log4j.appender.file=org.apache.log4j.RollingFileAppender log4j.appender.file.File=/app/logs/conductor.log log4j.appender.file.MaxFileSize=10MB log4j.appender.file.MaxBackupIndex=10 log4j.appender.file.layout=org.apache.log4j.PatternLayout log4j.appender.file.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n # Dedicated file appender for metrics log4j.appender.fileMetrics=org.apache.log4j.RollingFileAppender log4j.appender.fileMetrics.File=/app/logs/metrics.log log4j.appender.fileMetrics.MaxFileSize=10MB log4j.appender.fileMetrics.MaxBackupIndex=10 log4j.appender.fileMetrics.layout=org.apache.log4j.PatternLayout log4j.appender.fileMetrics.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n log4j.logger.ConductorMetrics=INFO,console,fileMetrics log4j.additivity.ConductorMetrics=false This configuration is bundled with conductor-server in file: log4j-file-appender.properties and can be utilized by setting env var: LOG4J_PROP=log4j-file-appender.properties This variable is used by startup.sh script. Integration with logstash using a log file \u00b6 The metrics collected by log4j can be further processed and pushed into a central collector such as ElasticSearch. One way of achieving this is to use: log4j file appender -> logstash -> ElasticSearch. Considering the above setup, you can deploy logstash to consume the contents of /app/logs/metrics.log file, process it and send further to elasticsearch. Following configuration needs to be used in logstash to achieve it: pipeline.yml: - pipeline.id: conductor_metrics path.config: \"/usr/share/logstash/pipeline/logstash_metrics.conf\" pipeline.workers: 2 logstash_metrics.conf input { file { path => [\"/conductor-server-logs/metrics.log\"] codec => multiline { pattern => \"^%{TIMESTAMP_ISO8601} \" negate => true what => previous } } } filter { kv { field_split => \", \" include_keys => [ \"name\", \"type\", \"count\", \"value\" ] } mutate { convert => { \"count\" => \"integer\" \"value\" => \"float\" } } } output { elasticsearch { hosts => [\"elasticsearch:9200\"] } } Note: In addition to forwarding the metrics into ElasticSearch, logstash will extract following fields from each metric: name, type, count, value and set proper types Integration with fluentd using a syslog channel \u00b6 Another example of metrics collection uses: log4j syslog appender -> fluentd -> prometheus. In this case, a specific log4j properties file needs to be used so that metrics are pushed into a syslog channel: log4j.rootLogger=INFO,console,file log4j.appender.console=org.apache.log4j.ConsoleAppender log4j.appender.console.layout=org.apache.log4j.PatternLayout log4j.appender.console.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n log4j.appender.file=org.apache.log4j.RollingFileAppender log4j.appender.file.File=/app/logs/conductor.log log4j.appender.file.MaxFileSize=10MB log4j.appender.file.MaxBackupIndex=10 log4j.appender.file.layout=org.apache.log4j.PatternLayout log4j.appender.file.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n # Syslog based appender streaming metrics into fluentd log4j.appender.server=org.apache.log4j.net.SyslogAppender log4j.appender.server.syslogHost=fluentd:5170 log4j.appender.server.facility=LOCAL1 log4j.appender.server.layout=org.apache.log4j.PatternLayout log4j.appender.server.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n log4j.logger.ConductorMetrics=INFO,console,server log4j.additivity.ConductorMetrics=false And on the fluentd side you need following configuration: <source> @type prometheus </source> <source> @type syslog port 5170 bind 0.0.0.0 tag conductor <parse> ; only allow TIMER metrics of workflow execution and extract tenant ID @type regexp expression /^.*type=TIMER, name=workflow_execution.class-WorkflowMonitor.+workflowName-(?<tenant>.*)_(?<workflow>.+), count=(?<count>\\d+), min=(?<min>[\\d.]+), max=(?<max>[\\d.]+), mean=(?<mean>[\\d.]+).*$/ types count:integer,min:float,max:float,mean:float </parse> </source> <filter conductor.local1.info> @type prometheus <metric> name conductor_workflow_count type gauge desc The total number of executed workflows key count <labels> workflow ${workflow} tenant ${tenant} user ${email} </labels> </metric> <metric> name conductor_workflow_max_duration type gauge desc Max duration in millis for a workflow key max <labels> workflow ${workflow} tenant ${tenant} user ${email} </labels> </metric> <metric> name conductor_workflow_mean_duration type gauge desc Mean duration in millis for a workflow key mean <labels> workflow ${workflow} tenant ${tenant} user ${email} </labels> </metric> </filter> <match **> @type stdout </match> With above configuration, fluentd will: - Listen to raw metrics on 0.0.0.0:5170 - Collect only workflow_execution TIMER metrics - Process the raw metrics and expose 3 prometheus specific metrics - Expose prometheus metrics on http://fluentd:24231/metrics Collecting metrics with Prometheus \u00b6 Another way to collect metrics is using Prometheus client to push them to Prometheus server. Conductor provides optional modules that connect metrics registry with Prometheus. To enable these modules, configure following additional module property in config.properties: conductor.metrics-prometheus.enabled = true This will simply push these metrics via Prometheus collector. However, you need to configure your own Prometheus collector and expose the metrics via an endpoint.","title":"Server Metrics"},{"location":"metrics/server/#publishing-metrics","text":"Conductor uses spectator to collect the metrics. To enable conductor serve to publish metrics, add this dependency to your build.gradle. Conductor Server enables you to load additional modules dynamically, this feature can be controlled using this configuration . Create your own AbstractModule that overides configure function and registers the Spectator metrics registry. Initialize the Registry and add it to the global registry via ((CompositeRegistry)Spectator.globalRegistry()).add(...) . The following metrics are published by the server. You can use these metrics to configure alerts for your workflows and tasks. Name Purpose Tags workflow_server_error Rate at which server side error is happening methodName workflow_failure Counter for failing workflows workflowName, status workflow_start_error Counter for failing to start a workflow workflowName workflow_running Counter for no. of running workflows workflowName, version workflow_execution Timer for Workflow completion workflowName, ownerApp task_queue_wait Time spent by a task in queue taskType task_execution Time taken to execute a task taskType, includeRetries, status task_poll Time taken to poll for a task taskType task_poll_count Counter for number of times the task is being polled taskType, domain task_queue_depth Pending tasks queue depth taskType, ownerApp task_rate_limited Current number of tasks being rate limited taskType task_concurrent_execution_limited Current number of tasks being limited by concurrent execution limit taskType task_timeout Counter for timed out tasks taskType task_response_timeout Counter for tasks timedout due to responseTimeout taskType task_update_conflict Counter for task update conflicts. Eg: when the workflow is in terminal state workflowName, taskType, taskStatus, workflowStatus event_queue_messages_processed Counter for number of messages fetched from an event queue queueType, queueName observable_queue_error Counter for number of errors encountered when fetching messages from an event queue queueType event_queue_messages_handled Counter for number of messages executed from an event queue queueType, queueName external_payload_storage_usage Counter for number of times external payload storage was used name, operation, payloadType","title":"Publishing metrics"},{"location":"metrics/server/#collecting-metrics-with-log4j","text":"One way of collecting metrics is to push them into the logging framework (log4j). Log4j supports various appenders that can print metrics into a console/file or even send them to remote metrics collectors over e.g. syslog channel. Conductor provides optional modules that connect metrics registry with the logging framework. To enable these modules, configure following additional modules property in config.properties: conductor.metrics-logger.enabled = true conductor.metrics-logger.reportPeriodSeconds = 15 This will push all available metrics into log4j every 15 seconds. By default, the metrics will be handled as a regular log message (just printed to console with default log4j.properties). In order to change that, you can use following log4j configuration that prints metrics into a dedicated file: log4j.rootLogger=INFO,console,file log4j.appender.console=org.apache.log4j.ConsoleAppender log4j.appender.console.layout=org.apache.log4j.PatternLayout log4j.appender.console.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n log4j.appender.file=org.apache.log4j.RollingFileAppender log4j.appender.file.File=/app/logs/conductor.log log4j.appender.file.MaxFileSize=10MB log4j.appender.file.MaxBackupIndex=10 log4j.appender.file.layout=org.apache.log4j.PatternLayout log4j.appender.file.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n # Dedicated file appender for metrics log4j.appender.fileMetrics=org.apache.log4j.RollingFileAppender log4j.appender.fileMetrics.File=/app/logs/metrics.log log4j.appender.fileMetrics.MaxFileSize=10MB log4j.appender.fileMetrics.MaxBackupIndex=10 log4j.appender.fileMetrics.layout=org.apache.log4j.PatternLayout log4j.appender.fileMetrics.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n log4j.logger.ConductorMetrics=INFO,console,fileMetrics log4j.additivity.ConductorMetrics=false This configuration is bundled with conductor-server in file: log4j-file-appender.properties and can be utilized by setting env var: LOG4J_PROP=log4j-file-appender.properties This variable is used by startup.sh script.","title":"Collecting metrics with Log4j"},{"location":"metrics/server/#integration-with-logstash-using-a-log-file","text":"The metrics collected by log4j can be further processed and pushed into a central collector such as ElasticSearch. One way of achieving this is to use: log4j file appender -> logstash -> ElasticSearch. Considering the above setup, you can deploy logstash to consume the contents of /app/logs/metrics.log file, process it and send further to elasticsearch. Following configuration needs to be used in logstash to achieve it: pipeline.yml: - pipeline.id: conductor_metrics path.config: \"/usr/share/logstash/pipeline/logstash_metrics.conf\" pipeline.workers: 2 logstash_metrics.conf input { file { path => [\"/conductor-server-logs/metrics.log\"] codec => multiline { pattern => \"^%{TIMESTAMP_ISO8601} \" negate => true what => previous } } } filter { kv { field_split => \", \" include_keys => [ \"name\", \"type\", \"count\", \"value\" ] } mutate { convert => { \"count\" => \"integer\" \"value\" => \"float\" } } } output { elasticsearch { hosts => [\"elasticsearch:9200\"] } } Note: In addition to forwarding the metrics into ElasticSearch, logstash will extract following fields from each metric: name, type, count, value and set proper types","title":"Integration with logstash using a log file"},{"location":"metrics/server/#integration-with-fluentd-using-a-syslog-channel","text":"Another example of metrics collection uses: log4j syslog appender -> fluentd -> prometheus. In this case, a specific log4j properties file needs to be used so that metrics are pushed into a syslog channel: log4j.rootLogger=INFO,console,file log4j.appender.console=org.apache.log4j.ConsoleAppender log4j.appender.console.layout=org.apache.log4j.PatternLayout log4j.appender.console.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n log4j.appender.file=org.apache.log4j.RollingFileAppender log4j.appender.file.File=/app/logs/conductor.log log4j.appender.file.MaxFileSize=10MB log4j.appender.file.MaxBackupIndex=10 log4j.appender.file.layout=org.apache.log4j.PatternLayout log4j.appender.file.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n # Syslog based appender streaming metrics into fluentd log4j.appender.server=org.apache.log4j.net.SyslogAppender log4j.appender.server.syslogHost=fluentd:5170 log4j.appender.server.facility=LOCAL1 log4j.appender.server.layout=org.apache.log4j.PatternLayout log4j.appender.server.layout.ConversionPattern=%d{ISO8601} %5p [%t] (%C) - %m%n log4j.logger.ConductorMetrics=INFO,console,server log4j.additivity.ConductorMetrics=false And on the fluentd side you need following configuration: <source> @type prometheus </source> <source> @type syslog port 5170 bind 0.0.0.0 tag conductor <parse> ; only allow TIMER metrics of workflow execution and extract tenant ID @type regexp expression /^.*type=TIMER, name=workflow_execution.class-WorkflowMonitor.+workflowName-(?<tenant>.*)_(?<workflow>.+), count=(?<count>\\d+), min=(?<min>[\\d.]+), max=(?<max>[\\d.]+), mean=(?<mean>[\\d.]+).*$/ types count:integer,min:float,max:float,mean:float </parse> </source> <filter conductor.local1.info> @type prometheus <metric> name conductor_workflow_count type gauge desc The total number of executed workflows key count <labels> workflow ${workflow} tenant ${tenant} user ${email} </labels> </metric> <metric> name conductor_workflow_max_duration type gauge desc Max duration in millis for a workflow key max <labels> workflow ${workflow} tenant ${tenant} user ${email} </labels> </metric> <metric> name conductor_workflow_mean_duration type gauge desc Mean duration in millis for a workflow key mean <labels> workflow ${workflow} tenant ${tenant} user ${email} </labels> </metric> </filter> <match **> @type stdout </match> With above configuration, fluentd will: - Listen to raw metrics on 0.0.0.0:5170 - Collect only workflow_execution TIMER metrics - Process the raw metrics and expose 3 prometheus specific metrics - Expose prometheus metrics on http://fluentd:24231/metrics","title":"Integration with fluentd using a syslog channel"},{"location":"metrics/server/#collecting-metrics-with-prometheus","text":"Another way to collect metrics is using Prometheus client to push them to Prometheus server. Conductor provides optional modules that connect metrics registry with Prometheus. To enable these modules, configure following additional module property in config.properties: conductor.metrics-prometheus.enabled = true This will simply push these metrics via Prometheus collector. However, you need to configure your own Prometheus collector and expose the metrics via an endpoint.","title":"Collecting metrics with Prometheus"},{"location":"reference-docs/do-while-task/","text":"Do While \u00b6 \"type\" : \"DO_WHILE\" Introduction \u00b6 Sequentially execute a list of task as long as a condition is true. The list of tasks is executed first, before the condition is checked (even for the first iteration). When scheduled, each task of this loop will see its taskReferenceName concatenated with __i, with i being the iteration number, starting at 1. Warning: taskReferenceName containing arithmetic operators must not be used. Each task output is stored as part of the DO_WHILE task, indexed by the iteration value (see example below), allowing the condition to reference the output of a task for a specific iteration (eg. $.LoopTask['iteration']['first_task']) The DO_WHILE task is set to FAILED as soon as one of the loopTask fails. In such case retry, iteration starts from 1. Limitations \u00b6 Domain or isolation group execution is unsupported; - Nested DO_WHILE is unsupported; SUB_WORKFLOW is unsupported; Since loopover tasks will be executed in loop inside scope of parent do while task, crossing branching outside of DO_WHILE task is not respected. Branching inside loopover task is supported. Configuration \u00b6 Parameters: name type description loopCondition String Condition to be evaluated after every iteration. This is a Javascript expression, evaluated using the Nashorn engine. If an exception occurs during evaluation, the DO_WHILE task is set to FAILED_WITH_TERMINAL_ERROR. loopOver List[Task] List of tasks that needs to be executed as long as the condition is true. Outputs: name type description iteration Integer Iteration number: the current one while executing; the final one once the loop is finished i Map[String, Any] Iteration number as a string, mapped to the task references names and their output. * Any Any state can be stored here if the loopCondition does so. For example storage will exist if loopCondition is if ($.LoopTask['iteration'] <= 10) {$.LoopTask.storage = 3; true } else {false} Examples \u00b6 The following definition: { \"name\": \"Loop Task\", \"taskReferenceName\": \"LoopTask\", \"type\": \"DO_WHILE\", \"inputParameters\": { \"value\": \"${workflow.input.value}\" }, \"loopCondition\": \"if ( ($.LoopTask['iteration'] < $.value ) || ( $.first_task['response']['body'] > 10)) { false; } else { true; }\", \"loopOver\": [ { \"name\": \"first task\", \"taskReferenceName\": \"first_task\", \"inputParameters\": { \"http_request\": { \"uri\": \"http://localhost:8082\", \"method\": \"POST\" } }, \"type\": \"HTTP\" },{ \"name\": \"second task\", \"taskReferenceName\": \"second_task\", \"inputParameters\": { \"http_request\": { \"uri\": \"http://localhost:8082\", \"method\": \"POST\" } }, \"type\": \"HTTP\" } ], \"startDelay\": 0, \"optional\": false } will produce the following execution, assuming 3 executions occurred (alongside first_task__1 , first_task__2 , first_task__3 , second_task__1 , second_task__2 and second_task__3 ): { \"taskType\": \"DO_WHILE\", \"outputData\": { \"iteration\": 3, \"1\": { \"first_task\": { \"response\": {}, \"headers\": { \"Content-Type\": \"application/json\" } }, \"second_task\": { \"response\": {}, \"headers\": { \"Content-Type\": \"application/json\" } } }, \"2\": { \"first_task\": { \"response\": {}, \"headers\": { \"Content-Type\": \"application/json\" } }, \"second_task\": { \"response\": {}, \"headers\": { \"Content-Type\": \"application/json\" } } }, \"3\": { \"first_task\": { \"response\": {}, \"headers\": { \"Content-Type\": \"application/json\" } }, \"second_task\": { \"response\": {}, \"headers\": { \"Content-Type\": \"application/json\" } } } } }","title":"Do/While"},{"location":"reference-docs/do-while-task/#do-while","text":"\"type\" : \"DO_WHILE\"","title":"Do While"},{"location":"reference-docs/do-while-task/#introduction","text":"Sequentially execute a list of task as long as a condition is true. The list of tasks is executed first, before the condition is checked (even for the first iteration). When scheduled, each task of this loop will see its taskReferenceName concatenated with __i, with i being the iteration number, starting at 1. Warning: taskReferenceName containing arithmetic operators must not be used. Each task output is stored as part of the DO_WHILE task, indexed by the iteration value (see example below), allowing the condition to reference the output of a task for a specific iteration (eg. $.LoopTask['iteration']['first_task']) The DO_WHILE task is set to FAILED as soon as one of the loopTask fails. In such case retry, iteration starts from 1.","title":"Introduction"},{"location":"reference-docs/do-while-task/#limitations","text":"Domain or isolation group execution is unsupported; - Nested DO_WHILE is unsupported; SUB_WORKFLOW is unsupported; Since loopover tasks will be executed in loop inside scope of parent do while task, crossing branching outside of DO_WHILE task is not respected. Branching inside loopover task is supported.","title":"Limitations"},{"location":"reference-docs/do-while-task/#configuration","text":"Parameters: name type description loopCondition String Condition to be evaluated after every iteration. This is a Javascript expression, evaluated using the Nashorn engine. If an exception occurs during evaluation, the DO_WHILE task is set to FAILED_WITH_TERMINAL_ERROR. loopOver List[Task] List of tasks that needs to be executed as long as the condition is true. Outputs: name type description iteration Integer Iteration number: the current one while executing; the final one once the loop is finished i Map[String, Any] Iteration number as a string, mapped to the task references names and their output. * Any Any state can be stored here if the loopCondition does so. For example storage will exist if loopCondition is if ($.LoopTask['iteration'] <= 10) {$.LoopTask.storage = 3; true } else {false}","title":"Configuration"},{"location":"reference-docs/do-while-task/#examples","text":"The following definition: { \"name\": \"Loop Task\", \"taskReferenceName\": \"LoopTask\", \"type\": \"DO_WHILE\", \"inputParameters\": { \"value\": \"${workflow.input.value}\" }, \"loopCondition\": \"if ( ($.LoopTask['iteration'] < $.value ) || ( $.first_task['response']['body'] > 10)) { false; } else { true; }\", \"loopOver\": [ { \"name\": \"first task\", \"taskReferenceName\": \"first_task\", \"inputParameters\": { \"http_request\": { \"uri\": \"http://localhost:8082\", \"method\": \"POST\" } }, \"type\": \"HTTP\" },{ \"name\": \"second task\", \"taskReferenceName\": \"second_task\", \"inputParameters\": { \"http_request\": { \"uri\": \"http://localhost:8082\", \"method\": \"POST\" } }, \"type\": \"HTTP\" } ], \"startDelay\": 0, \"optional\": false } will produce the following execution, assuming 3 executions occurred (alongside first_task__1 , first_task__2 , first_task__3 , second_task__1 , second_task__2 and second_task__3 ): { \"taskType\": \"DO_WHILE\", \"outputData\": { \"iteration\": 3, \"1\": { \"first_task\": { \"response\": {}, \"headers\": { \"Content-Type\": \"application/json\" } }, \"second_task\": { \"response\": {}, \"headers\": { \"Content-Type\": \"application/json\" } } }, \"2\": { \"first_task\": { \"response\": {}, \"headers\": { \"Content-Type\": \"application/json\" } }, \"second_task\": { \"response\": {}, \"headers\": { \"Content-Type\": \"application/json\" } } }, \"3\": { \"first_task\": { \"response\": {}, \"headers\": { \"Content-Type\": \"application/json\" } }, \"second_task\": { \"response\": {}, \"headers\": { \"Content-Type\": \"application/json\" } } } } }","title":"Examples"},{"location":"reference-docs/dynamic-fork-task/","text":"Dynamic Fork \u00b6 \"type\" : \"FORK_JOIN_DYNAMIC\" Introduction \u00b6 Dynamic Forks are an extension of the Fork operation in conductor. In a regular fork operation ( FORK_JOIN task), the size of the fork is defined at the time of workflow definition. For dynamic forks the list of tasks is provided at runtime using the task's input. There are four things that are needed to configure a FORK_JOIN_DYNAMIC task: A list of tasks or sub-workflows that needs to be forked and run in parallel. A list of inputs to each of these forked tasks or sub-workflows A task prior to the FORK_JOIN_DYNAMIC tasks outputs 1 and 2 above that can be wired in as in input to the FORK_JOIN_DYNAMIC tasks. A join task to accept the results of the dynamic forks. This join will wait for ALL the forked branches to complete before completing. Use Cases \u00b6 A FORK_JOIN_DYNAMIC is useful when a set of tasks or sub-workflows need to be executed and the number of tasks or sub-workflows are determined at run time. Note: Unlike FORK , which can execute parallel flows with each fork executing a series of tasks in sequence, FORK_JOIN_DYNAMIC is limited to only one task per fork. However, forked task can be a Sub Workflow, allowing for more complex execution flows. Configuration \u00b6 Input Configuration \u00b6 Attribute Description name Task Name. A unique name that is descriptive of the task function taskReferenceName Task Reference Name. A unique reference to this task. There can be multiple references of a task within the same workflow definition type FORK_JOIN_DYNAMIC inputParameters The input parameters that will be supplied to this task. dynamicForkTasksParam This is a JSON array of tasks or sub-workflow objects that needs to be forked and run in parallel dynamicForkTasksInputParamName A JSON map, where the keys are task or sub-workflow names, and the values are its corresponding inputParameters Example \u00b6 Let's say we have a task that resizes an image, and we need to create a workflow that will resize an image into multiple sizes. In this case, a task can be created prior to the FORK_JOIN_DYNAMIC task that will prepare the input that needs to be passed into the FORK_JOIN_DYNAMIC task. These will be: dynamicForkTasksParam the JSON array of tasks/subworkflows to be run in parallel. dynamicForkTasksInputParamName a JSON map of input parameters for each task. The keys will be the tasks/subworkflows, and the values will be the input parameters for the tasks. The single image resize task does one job. The FORK_JOIN_DYNAMIC and the following JOIN will manage the multiple invokes of the single image resize task. Here, the responsibilities are clearly broken out, where the single image resize task does the core job and FORK_JOIN_DYNAMIC manages the orchestration and fault tolerance aspects. The workflow \u00b6 Here is an example of a FORK_JOIN_DYNAMIC task followed by a JOIN task: { \"inputParameters\": { \"dynamicTasks\": \"${fooBarTask.output.dynamicTasksJSON}\", \"dynamicTasksInput\": \"${fooBarTask.output.dynamicTasksInputJSON}\" }, \"type\": \"FORK_JOIN_DYNAMIC\", \"dynamicForkTasksParam\": \"dynamicTasks\", \"dynamicForkTasksInputParamName\": \"dynamicTasksInput\" }, { \"name\": \"image_multiple_convert_resize_join\", \"taskReferenceName\": \"image_multiple_convert_resize_join_ref\", \"type\": \"JOIN\" } This appears in the UI as follows: Let's assume this data is sent to the workflow: { \"fileLocation\": \"https://pbs.twimg.com/media/FJY7ud0XEAYVCS8?format=png&name=900x900\", \"outputFormats\": [\"png\",\"jpg\"], \"outputSizes\": [ {\"width\":300, \"height\":300}, {\"width\":200, \"height\":200} ], \"maintainAspectRatio\": \"true\" } With 2 file formats and 2 sizes in the input, we'll be creating 4 images total. The first task will generate the tasks and the parameters for these tasks: dynamicForkTasksParam This is a JSON array of task or sub-workflow objects that specifies the list of tasks or sub-workflows that needs to be forked and run in parallel. This will have the form: { \"dynamicTasks\": [ 0: { \"name\": :\"image_convert_resize\", \"taskReferenceName\": \"image_convert_resize_png_300x300_0\", ... }, 1: { \"name\": :\"image_convert_resize\", \"taskReferenceName\": \"image_convert_resize_png_200x200_1\", ... }, 2: { \"name\": :\"image_convert_resize\", \"taskReferenceName\": \"image_convert_resize_jpg_300x300_2\", ... }, 3: { \"name\": :\"image_convert_resize\", \"taskReferenceName\": \"image_convert_resize_jpg_200x200_3\", ... } ]} dynamicForkTasksInputParamName This is a JSON map of task or sub-workflow objects and all the input parameters that these tasks will need to run. \"dynamicTasksInput\":{ \"image_convert_resize_jpg_300x300_2\":{ \"outputWidth\":300 \"outputHeight\":300 \"fileLocation\":\"https://pbs.twimg.com/media/FJY7ud0XEAYVCS8?format=png&name=900x900\" \"outputFormat\":\"jpg\" \"maintainAspectRatio\":true } \"image_convert_resize_jpg_200x200_3\":{ \"outputWidth\":200 \"outputHeight\":200 \"fileLocation\":\"https://pbs.twimg.com/media/FJY7ud0XEAYVCS8?format=png&name=900x900\" \"outputFormat\":\"jpg\" \"maintainAspectRatio\":true } \"image_convert_resize_png_200x200_1\":{ \"outputWidth\":200 \"outputHeight\":200 \"fileLocation\":\"https://pbs.twimg.com/media/FJY7ud0XEAYVCS8?format=png&name=900x900\" \"outputFormat\":\"png\" \"maintainAspectRatio\":true } \"image_convert_resize_png_300x300_0\":{ \"outputWidth\":300 \"outputHeight\":300 \"fileLocation\":\"https://pbs.twimg.com/media/FJY7ud0XEAYVCS8?format=png&name=900x900\" \"outputFormat\":\"png\" \"maintainAspectRatio\":true } The Join \u00b6 The JOIN task will run after all of the dynamic tasks, collecting the output for all of the tasks.","title":"Dynamic Fork"},{"location":"reference-docs/dynamic-fork-task/#dynamic-fork","text":"\"type\" : \"FORK_JOIN_DYNAMIC\"","title":"Dynamic Fork"},{"location":"reference-docs/dynamic-fork-task/#introduction","text":"Dynamic Forks are an extension of the Fork operation in conductor. In a regular fork operation ( FORK_JOIN task), the size of the fork is defined at the time of workflow definition. For dynamic forks the list of tasks is provided at runtime using the task's input. There are four things that are needed to configure a FORK_JOIN_DYNAMIC task: A list of tasks or sub-workflows that needs to be forked and run in parallel. A list of inputs to each of these forked tasks or sub-workflows A task prior to the FORK_JOIN_DYNAMIC tasks outputs 1 and 2 above that can be wired in as in input to the FORK_JOIN_DYNAMIC tasks. A join task to accept the results of the dynamic forks. This join will wait for ALL the forked branches to complete before completing.","title":"Introduction"},{"location":"reference-docs/dynamic-fork-task/#use-cases","text":"A FORK_JOIN_DYNAMIC is useful when a set of tasks or sub-workflows need to be executed and the number of tasks or sub-workflows are determined at run time. Note: Unlike FORK , which can execute parallel flows with each fork executing a series of tasks in sequence, FORK_JOIN_DYNAMIC is limited to only one task per fork. However, forked task can be a Sub Workflow, allowing for more complex execution flows.","title":"Use Cases"},{"location":"reference-docs/dynamic-fork-task/#configuration","text":"","title":"Configuration"},{"location":"reference-docs/dynamic-fork-task/#input-configuration","text":"Attribute Description name Task Name. A unique name that is descriptive of the task function taskReferenceName Task Reference Name. A unique reference to this task. There can be multiple references of a task within the same workflow definition type FORK_JOIN_DYNAMIC inputParameters The input parameters that will be supplied to this task. dynamicForkTasksParam This is a JSON array of tasks or sub-workflow objects that needs to be forked and run in parallel dynamicForkTasksInputParamName A JSON map, where the keys are task or sub-workflow names, and the values are its corresponding inputParameters","title":"Input Configuration"},{"location":"reference-docs/dynamic-fork-task/#example","text":"Let's say we have a task that resizes an image, and we need to create a workflow that will resize an image into multiple sizes. In this case, a task can be created prior to the FORK_JOIN_DYNAMIC task that will prepare the input that needs to be passed into the FORK_JOIN_DYNAMIC task. These will be: dynamicForkTasksParam the JSON array of tasks/subworkflows to be run in parallel. dynamicForkTasksInputParamName a JSON map of input parameters for each task. The keys will be the tasks/subworkflows, and the values will be the input parameters for the tasks. The single image resize task does one job. The FORK_JOIN_DYNAMIC and the following JOIN will manage the multiple invokes of the single image resize task. Here, the responsibilities are clearly broken out, where the single image resize task does the core job and FORK_JOIN_DYNAMIC manages the orchestration and fault tolerance aspects.","title":"Example"},{"location":"reference-docs/dynamic-fork-task/#the-workflow","text":"Here is an example of a FORK_JOIN_DYNAMIC task followed by a JOIN task: { \"inputParameters\": { \"dynamicTasks\": \"${fooBarTask.output.dynamicTasksJSON}\", \"dynamicTasksInput\": \"${fooBarTask.output.dynamicTasksInputJSON}\" }, \"type\": \"FORK_JOIN_DYNAMIC\", \"dynamicForkTasksParam\": \"dynamicTasks\", \"dynamicForkTasksInputParamName\": \"dynamicTasksInput\" }, { \"name\": \"image_multiple_convert_resize_join\", \"taskReferenceName\": \"image_multiple_convert_resize_join_ref\", \"type\": \"JOIN\" } This appears in the UI as follows: Let's assume this data is sent to the workflow: { \"fileLocation\": \"https://pbs.twimg.com/media/FJY7ud0XEAYVCS8?format=png&name=900x900\", \"outputFormats\": [\"png\",\"jpg\"], \"outputSizes\": [ {\"width\":300, \"height\":300}, {\"width\":200, \"height\":200} ], \"maintainAspectRatio\": \"true\" } With 2 file formats and 2 sizes in the input, we'll be creating 4 images total. The first task will generate the tasks and the parameters for these tasks: dynamicForkTasksParam This is a JSON array of task or sub-workflow objects that specifies the list of tasks or sub-workflows that needs to be forked and run in parallel. This will have the form: { \"dynamicTasks\": [ 0: { \"name\": :\"image_convert_resize\", \"taskReferenceName\": \"image_convert_resize_png_300x300_0\", ... }, 1: { \"name\": :\"image_convert_resize\", \"taskReferenceName\": \"image_convert_resize_png_200x200_1\", ... }, 2: { \"name\": :\"image_convert_resize\", \"taskReferenceName\": \"image_convert_resize_jpg_300x300_2\", ... }, 3: { \"name\": :\"image_convert_resize\", \"taskReferenceName\": \"image_convert_resize_jpg_200x200_3\", ... } ]} dynamicForkTasksInputParamName This is a JSON map of task or sub-workflow objects and all the input parameters that these tasks will need to run. \"dynamicTasksInput\":{ \"image_convert_resize_jpg_300x300_2\":{ \"outputWidth\":300 \"outputHeight\":300 \"fileLocation\":\"https://pbs.twimg.com/media/FJY7ud0XEAYVCS8?format=png&name=900x900\" \"outputFormat\":\"jpg\" \"maintainAspectRatio\":true } \"image_convert_resize_jpg_200x200_3\":{ \"outputWidth\":200 \"outputHeight\":200 \"fileLocation\":\"https://pbs.twimg.com/media/FJY7ud0XEAYVCS8?format=png&name=900x900\" \"outputFormat\":\"jpg\" \"maintainAspectRatio\":true } \"image_convert_resize_png_200x200_1\":{ \"outputWidth\":200 \"outputHeight\":200 \"fileLocation\":\"https://pbs.twimg.com/media/FJY7ud0XEAYVCS8?format=png&name=900x900\" \"outputFormat\":\"png\" \"maintainAspectRatio\":true } \"image_convert_resize_png_300x300_0\":{ \"outputWidth\":300 \"outputHeight\":300 \"fileLocation\":\"https://pbs.twimg.com/media/FJY7ud0XEAYVCS8?format=png&name=900x900\" \"outputFormat\":\"png\" \"maintainAspectRatio\":true }","title":"The workflow"},{"location":"reference-docs/dynamic-fork-task/#the-join","text":"The JOIN task will run after all of the dynamic tasks, collecting the output for all of the tasks.","title":"The Join"},{"location":"reference-docs/dynamic-task/","text":"Dynamic \u00b6 \"type\" : \"DYNAMIC\" Introduction \u00b6 Dynamic Task allows to execute one of the registered Tasks dynamically at run-time. It accepts the task name to execute as taskToExecute in inputParameters . Use Cases \u00b6 Consider a scenario, when we have to make decision of executing a task dynamically i.e. while the workflow is still running. In such cases, Dynamic Task would be useful. Configuration \u00b6 Dynamic task is defined directly inside the workflow with type DYNAMIC . Inputs \u00b6 Following are the input parameters : name description dynamicTaskNameParam Name of the parameter from the task input whose value is used to schedule the task. e.g. if the value of the parameter is ABC, the next task scheduled is of type 'ABC'. Output \u00b6 TODO: Talk about output of the task, what to expect Examples \u00b6 Suppose in a workflow, we have to take decision to ship the courier with the shipping service providers on the basis of Post Code. Following task shipping_info generates an output on the basis of which decision would be taken to run the next task. { \"name\": \"shipping_info\", \"retryCount\": 3, \"timeoutSeconds\": 600, \"pollTimeoutSeconds\": 1200, \"timeoutPolicy\": \"TIME_OUT_WF\", \"retryLogic\": \"FIXED\", \"retryDelaySeconds\": 300, \"responseTimeoutSeconds\": 300, \"concurrentExecLimit\": 100, \"rateLimitFrequencyInSeconds\": 60, \"ownerEmail\":\"abc@example.com\", \"rateLimitPerFrequency\": 1 } Following are the two worker tasks, one among them would execute on the basis of output generated by the shipping_info task : { \"name\": \"ship_via_fedex\", \"retryCount\": 3, \"timeoutSeconds\": 600, \"pollTimeoutSeconds\": 1200, \"timeoutPolicy\": \"TIME_OUT_WF\", \"retryLogic\": \"FIXED\", \"retryDelaySeconds\": 300, \"responseTimeoutSeconds\": 300, \"concurrentExecLimit\": 100, \"rateLimitFrequencyInSeconds\": 60, \"ownerEmail\":\"abc@example.com\", \"rateLimitPerFrequency\": 2 }, { \"name\": \"ship_via_ups\", \"retryCount\": 3, \"timeoutSeconds\": 600, \"pollTimeoutSeconds\": 1200, \"timeoutPolicy\": \"TIME_OUT_WF\", \"retryLogic\": \"FIXED\", \"retryDelaySeconds\": 300, \"responseTimeoutSeconds\": 300, \"concurrentExecLimit\": 100, \"rateLimitFrequencyInSeconds\": 60, \"ownerEmail\":\"abc@example.com\", \"rateLimitPerFrequency\": 2 } We will create the Workflow with the following definition : { \"name\": \"Shipping_Flow\", \"description\": \"Ships smartly on the basis of Shipping info\", \"version\": 1, \"tasks\": [ { \"name\": \"shipping_info\", \"taskReferenceName\": \"shipping_info\", \"inputParameters\": { }, \"type\": \"SIMPLE\" }, { \"name\": \"shipping_task\", \"taskReferenceName\": \"shipping_task\", \"inputParameters\": { \"taskToExecute\": \"${shipping_info.output.shipping_service}\" }, \"type\": \"DYNAMIC\", \"dynamicTaskNameParam\": \"taskToExecute\" } ], \"restartable\": true, \"ownerEmail\":\"abc@example.com\", \"workflowStatusListenerEnabled\": true, \"schemaVersion\": 2 } Workflow is the created as shown in the below diagram. Note : shipping_task is a DYNAMIC task and the taskToExecute parameter can be set with input value provided while running the workflow or with the output of previous tasks. Here, it is set to the output provided by the previous task i.e. ${shipping_info.output.shipping_service} . If the input value is provided while running the workflow it can be accessed by ${workflow.input.shipping_service} . { \"shipping_service\": \"ship_via_fedex\" } We can see in the below example that on the basis of Post Code the shipping service is being decided. js reference https://github.com/orkes-io/orkesworkers/blob/main/src/main/java/io/orkes/samples/workers/ShippingInfoWorker.java#L10-L36 Based on given set of inputs i.e. Post Code starts with '9' hence, ship_via_fedex is executed - If the Post Code started with anything other than 9 ship_via_ups is executed - If the incorrect task name or the task that doesn't exist is provided then the workflow fails and we get the error \"Invalid task specified. Cannot find task by name in the task definitions.\" If the null reference is provided in the task name then also the workflow fails and we get the error \"Cannot map a dynamic task based on the parameter and input. Parameter= taskToExecute, input= {taskToExecute=null}\"","title":"Dynamic"},{"location":"reference-docs/dynamic-task/#dynamic","text":"\"type\" : \"DYNAMIC\"","title":"Dynamic"},{"location":"reference-docs/dynamic-task/#introduction","text":"Dynamic Task allows to execute one of the registered Tasks dynamically at run-time. It accepts the task name to execute as taskToExecute in inputParameters .","title":"Introduction"},{"location":"reference-docs/dynamic-task/#use-cases","text":"Consider a scenario, when we have to make decision of executing a task dynamically i.e. while the workflow is still running. In such cases, Dynamic Task would be useful.","title":"Use Cases"},{"location":"reference-docs/dynamic-task/#configuration","text":"Dynamic task is defined directly inside the workflow with type DYNAMIC .","title":"Configuration"},{"location":"reference-docs/dynamic-task/#inputs","text":"Following are the input parameters : name description dynamicTaskNameParam Name of the parameter from the task input whose value is used to schedule the task. e.g. if the value of the parameter is ABC, the next task scheduled is of type 'ABC'.","title":"Inputs"},{"location":"reference-docs/dynamic-task/#output","text":"TODO: Talk about output of the task, what to expect","title":"Output"},{"location":"reference-docs/dynamic-task/#examples","text":"Suppose in a workflow, we have to take decision to ship the courier with the shipping service providers on the basis of Post Code. Following task shipping_info generates an output on the basis of which decision would be taken to run the next task. { \"name\": \"shipping_info\", \"retryCount\": 3, \"timeoutSeconds\": 600, \"pollTimeoutSeconds\": 1200, \"timeoutPolicy\": \"TIME_OUT_WF\", \"retryLogic\": \"FIXED\", \"retryDelaySeconds\": 300, \"responseTimeoutSeconds\": 300, \"concurrentExecLimit\": 100, \"rateLimitFrequencyInSeconds\": 60, \"ownerEmail\":\"abc@example.com\", \"rateLimitPerFrequency\": 1 } Following are the two worker tasks, one among them would execute on the basis of output generated by the shipping_info task : { \"name\": \"ship_via_fedex\", \"retryCount\": 3, \"timeoutSeconds\": 600, \"pollTimeoutSeconds\": 1200, \"timeoutPolicy\": \"TIME_OUT_WF\", \"retryLogic\": \"FIXED\", \"retryDelaySeconds\": 300, \"responseTimeoutSeconds\": 300, \"concurrentExecLimit\": 100, \"rateLimitFrequencyInSeconds\": 60, \"ownerEmail\":\"abc@example.com\", \"rateLimitPerFrequency\": 2 }, { \"name\": \"ship_via_ups\", \"retryCount\": 3, \"timeoutSeconds\": 600, \"pollTimeoutSeconds\": 1200, \"timeoutPolicy\": \"TIME_OUT_WF\", \"retryLogic\": \"FIXED\", \"retryDelaySeconds\": 300, \"responseTimeoutSeconds\": 300, \"concurrentExecLimit\": 100, \"rateLimitFrequencyInSeconds\": 60, \"ownerEmail\":\"abc@example.com\", \"rateLimitPerFrequency\": 2 } We will create the Workflow with the following definition : { \"name\": \"Shipping_Flow\", \"description\": \"Ships smartly on the basis of Shipping info\", \"version\": 1, \"tasks\": [ { \"name\": \"shipping_info\", \"taskReferenceName\": \"shipping_info\", \"inputParameters\": { }, \"type\": \"SIMPLE\" }, { \"name\": \"shipping_task\", \"taskReferenceName\": \"shipping_task\", \"inputParameters\": { \"taskToExecute\": \"${shipping_info.output.shipping_service}\" }, \"type\": \"DYNAMIC\", \"dynamicTaskNameParam\": \"taskToExecute\" } ], \"restartable\": true, \"ownerEmail\":\"abc@example.com\", \"workflowStatusListenerEnabled\": true, \"schemaVersion\": 2 } Workflow is the created as shown in the below diagram. Note : shipping_task is a DYNAMIC task and the taskToExecute parameter can be set with input value provided while running the workflow or with the output of previous tasks. Here, it is set to the output provided by the previous task i.e. ${shipping_info.output.shipping_service} . If the input value is provided while running the workflow it can be accessed by ${workflow.input.shipping_service} . { \"shipping_service\": \"ship_via_fedex\" } We can see in the below example that on the basis of Post Code the shipping service is being decided. js reference https://github.com/orkes-io/orkesworkers/blob/main/src/main/java/io/orkes/samples/workers/ShippingInfoWorker.java#L10-L36 Based on given set of inputs i.e. Post Code starts with '9' hence, ship_via_fedex is executed - If the Post Code started with anything other than 9 ship_via_ups is executed - If the incorrect task name or the task that doesn't exist is provided then the workflow fails and we get the error \"Invalid task specified. Cannot find task by name in the task definitions.\" If the null reference is provided in the task name then also the workflow fails and we get the error \"Cannot map a dynamic task based on the parameter and input. Parameter= taskToExecute, input= {taskToExecute=null}\"","title":"Examples"},{"location":"reference-docs/dynamic/","text":"Dynamic Task \u00b6 Dynamic Tasks allow you to execute a registered task dynamically at run-time. It accepts the task name to execute in inputParameters. Parameters: name description dynamicTaskNameParam Name of the parameter from the task input whose value is used to schedule the task. e.g. if the value of the parameter is ABC, the next task scheduled is of type 'ABC'. Example { \"name\": \"user_task\", \"taskReferenceName\": \"t1\", \"inputParameters\": { \"files\": \"${workflow.input.files}\", \"taskToExecute\": \"${workflow.input.user_supplied_task}\" }, \"type\": \"DYNAMIC\", \"dynamicTaskNameParam\": \"taskToExecute\" } If the workflow is started with input parameter user_supplied_task's value as user_task_2 , Conductor will schedule user_task_2 when scheduling this dynamic task.","title":"Dynamic"},{"location":"reference-docs/dynamic/#dynamic-task","text":"Dynamic Tasks allow you to execute a registered task dynamically at run-time. It accepts the task name to execute in inputParameters. Parameters: name description dynamicTaskNameParam Name of the parameter from the task input whose value is used to schedule the task. e.g. if the value of the parameter is ABC, the next task scheduled is of type 'ABC'. Example { \"name\": \"user_task\", \"taskReferenceName\": \"t1\", \"inputParameters\": { \"files\": \"${workflow.input.files}\", \"taskToExecute\": \"${workflow.input.user_supplied_task}\" }, \"type\": \"DYNAMIC\", \"dynamicTaskNameParam\": \"taskToExecute\" } If the workflow is started with input parameter user_supplied_task's value as user_task_2 , Conductor will schedule user_task_2 when scheduling this dynamic task.","title":"Dynamic Task"},{"location":"reference-docs/event-task/","text":"Event Task \u00b6 \"type\" : \"EVENT\" Introduction \u00b6 EVENT is a task used to publish an event into one of the supported eventing systems in Conductor. Conductor supports the the following eventing models: Conductor internal events (type: conductor) SQL (type: sqs) Use Cases \u00b6 Consider a use case where at some point in the execution, an event is published to an external eventing system such as SQS. Event tasks are useful for creating event based dependencies for workflows and tasks. Consider an example where we want to publish an event into SQS to notify an external system. { \"type\": \"EVENT\", \"sink\": \"sqs:sqs_queue_name\", \"asyncComplete\": false } An example where we want to publish a messase to conductor's internal queuing system. { \"type\": \"EVENT\", \"sink\": \"conductor:internal_event_name\", \"asyncComplete\": false } Configuration \u00b6 Input Configuration \u00b6 Attribute Description name Task Name. A unique name that is descriptive of the task function taskReferenceName Task Reference Name. A unique reference to this task. There can be multiple references of a task within the same workflow definition type Task Type. In this case, EVENT sink External event queue in the format of prefix:location . Prefix is either sqs or conductor and location specifies the actual queue name. e.g. \"sqs:send_email_queue\" asyncComplete Boolean asyncComplete \u00b6 false to mark status COMPLETED upon execution true to keep it IN_PROGRESS, wait for an external event (via Conductor or SQS or EventHandler) to complete it. Output Configuration \u00b6 Tasks's output are sent as a payload to the external event. In case of SQS the task's output is sent to the SQS message a a payload. name type description workflowInstanceId String Workflow id workflowType String Workflow Name workflowVersion Integer Workflow Version correlationId String Workflow CorrelationId sink String Copy of the input data \"sink\" asyncComplete Boolean Copy of the input data \"asyncComplete event_produced String Name of the event produced The published event's payload is identical to the output of the task (except \"event_produced\"). When producing an event with Conductor as sink, the event name follows the structure: conductor:<workflow_name>:<task_reference_name> For SQS, use the name of the queue and NOT the URI. Conductor looks up the URI based on the name. Warning When using SQS add the ContribsModule to the deployment. The module needs to be configured with AWSCredentialsProvider for Conductor to be able to use AWS APIs. Warning When using Conductor as sink, you have two options: defining the sink as conductor in which case the queue name will default to the taskReferenceName of the Event Task, or specifying the queue name in the sink, as conductor:<queue_name> . The queue name is in the event value of the event Handler, as conductor:<workflow_name>:<queue_name> . Supported Queuing Systems \u00b6 Conductor has support for the following external event queueing systems as part of the OSS build SQS (prefix: sqs) NATS (prefix: nats) AMQP (prefix: amqp_queue or amqp_exchange) Internal Conductor (prefix: conductor) To add support for other","title":"Event Task"},{"location":"reference-docs/event-task/#event-task","text":"\"type\" : \"EVENT\"","title":"Event Task"},{"location":"reference-docs/event-task/#introduction","text":"EVENT is a task used to publish an event into one of the supported eventing systems in Conductor. Conductor supports the the following eventing models: Conductor internal events (type: conductor) SQL (type: sqs)","title":"Introduction"},{"location":"reference-docs/event-task/#use-cases","text":"Consider a use case where at some point in the execution, an event is published to an external eventing system such as SQS. Event tasks are useful for creating event based dependencies for workflows and tasks. Consider an example where we want to publish an event into SQS to notify an external system. { \"type\": \"EVENT\", \"sink\": \"sqs:sqs_queue_name\", \"asyncComplete\": false } An example where we want to publish a messase to conductor's internal queuing system. { \"type\": \"EVENT\", \"sink\": \"conductor:internal_event_name\", \"asyncComplete\": false }","title":"Use Cases"},{"location":"reference-docs/event-task/#configuration","text":"","title":"Configuration"},{"location":"reference-docs/event-task/#input-configuration","text":"Attribute Description name Task Name. A unique name that is descriptive of the task function taskReferenceName Task Reference Name. A unique reference to this task. There can be multiple references of a task within the same workflow definition type Task Type. In this case, EVENT sink External event queue in the format of prefix:location . Prefix is either sqs or conductor and location specifies the actual queue name. e.g. \"sqs:send_email_queue\" asyncComplete Boolean","title":"Input Configuration"},{"location":"reference-docs/event-task/#asynccomplete","text":"false to mark status COMPLETED upon execution true to keep it IN_PROGRESS, wait for an external event (via Conductor or SQS or EventHandler) to complete it.","title":"asyncComplete"},{"location":"reference-docs/event-task/#output-configuration","text":"Tasks's output are sent as a payload to the external event. In case of SQS the task's output is sent to the SQS message a a payload. name type description workflowInstanceId String Workflow id workflowType String Workflow Name workflowVersion Integer Workflow Version correlationId String Workflow CorrelationId sink String Copy of the input data \"sink\" asyncComplete Boolean Copy of the input data \"asyncComplete event_produced String Name of the event produced The published event's payload is identical to the output of the task (except \"event_produced\"). When producing an event with Conductor as sink, the event name follows the structure: conductor:<workflow_name>:<task_reference_name> For SQS, use the name of the queue and NOT the URI. Conductor looks up the URI based on the name. Warning When using SQS add the ContribsModule to the deployment. The module needs to be configured with AWSCredentialsProvider for Conductor to be able to use AWS APIs. Warning When using Conductor as sink, you have two options: defining the sink as conductor in which case the queue name will default to the taskReferenceName of the Event Task, or specifying the queue name in the sink, as conductor:<queue_name> . The queue name is in the event value of the event Handler, as conductor:<workflow_name>:<queue_name> .","title":"Output Configuration"},{"location":"reference-docs/event-task/#supported-queuing-systems","text":"Conductor has support for the following external event queueing systems as part of the OSS build SQS (prefix: sqs) NATS (prefix: nats) AMQP (prefix: amqp_queue or amqp_exchange) Internal Conductor (prefix: conductor) To add support for other","title":"Supported Queuing Systems"},{"location":"reference-docs/exclusive-join-task/","text":"Exclusive Join Task \u00b6 TODO Summary \u00b6 TODO","title":"Exclusive Join Task"},{"location":"reference-docs/exclusive-join-task/#exclusive-join-task","text":"TODO","title":"Exclusive Join Task"},{"location":"reference-docs/exclusive-join-task/#summary","text":"TODO","title":"Summary"},{"location":"reference-docs/fork-task/","text":"Fork \u00b6 \"type\" : \"FORK_JOIN\" Introduction \u00b6 A Fork operation lets you run a specified list of tasks or sub workflows in parallel. A fork task is followed by a join operation that waits on the forked tasks or sub workflows to finish. The JOIN task also collects outputs from each of the forked tasks or sub workflows. Use Cases \u00b6 FORK_JOIN tasks are typically used when a list of tasks can be run in parallel. E.g In a notification workflow, there could be multiple ways of sending notifications, i,e e-mail, SMS, HTTP etc.. These notifications are not dependent on each other, and so they can be run in parallel. In such cases, you can create 3 sub-lists of forked tasks for each of these operations. Configuration \u00b6 A FORK_JOIN task has a forkTasks attribute that expects an array. Each array is a sub-list of tasks. Each of these sub-lists are then invoked in parallel. The tasks defined within each sublist can be sequential or any other way as desired. A FORK_JOIN task has to be followed by a JOIN operation. The JOIN operator specifies which of the forked tasks to joinOn (wait for completion) before moving to the next stage in the workflow. Input Configuration \u00b6 Attribute Description name Task Name. A unique name that is descriptive of the task function taskReferenceName Task Reference Name. A unique reference to this task. There can be multiple references of a task within the same workflow definition type Task Type. In this case, FORK_JOIN inputParameters The input parameters that will be supplied to this task forkTasks A list of a list of tasks. Each of the outer list will be invoked in parallel. The inner list can be a graph of other tasks and sub-workflows Output Configuration \u00b6 This is the output configuration of the JOIN task that is used in conjunction with the FORK_JOIN task. The output of the JOIN task is a map, where the keys are the names of the task reference names where were being joinOn and the keys are the corresponding outputs of those tasks. Attribute Description task_ref_name_1 A task reference name that was being joinOn . The value is the output of that task task_ref_name_2 A task reference name that was being joinOn . The value is the output of that task ... ... task_ref_name_N A task reference name that was being joinOn . The value is the output of that task Example \u00b6 Imagine a workflow that sends 3 notifications: email, SMS and HTTP. Since none of these steps are dependant on the others, they can be run in parallel with a fork. The diagram will appear as: Here's the JSON definition for the workflow: [ { \"name\": \"fork_join\", \"taskReferenceName\": \"my_fork_join_ref\", \"type\": \"FORK_JOIN\", \"forkTasks\": [ [ { \"name\": \"process_notification_payload\", \"taskReferenceName\": \"process_notification_payload_email\", \"type\": \"SIMPLE\" }, { \"name\": \"email_notification\", \"taskReferenceName\": \"email_notification_ref\", \"type\": \"SIMPLE\" } ], [ { \"name\": \"process_notification_payload\", \"taskReferenceName\": \"process_notification_payload_sms\", \"type\": \"SIMPLE\" }, { \"name\": \"sms_notification\", \"taskReferenceName\": \"sms_notification_ref\", \"type\": \"SIMPLE\" } ], [ { \"name\": \"process_notification_payload\", \"taskReferenceName\": \"process_notification_payload_http\", \"type\": \"SIMPLE\" }, { \"name\": \"http_notification\", \"taskReferenceName\": \"http_notification_ref\", \"type\": \"SIMPLE\" } ] ] }, { \"name\": \"notification_join\", \"taskReferenceName\": \"notification_join_ref\", \"type\": \"JOIN\", \"joinOn\": [ \"email_notification_ref\", \"sms_notification_ref\" ] } ] Note: There are three parallel 'tines' to this fork, but only two of the outputs are required for the JOIN to continue. The diagram does draw an arrow from http_notification_ref to the notification_join , but it is not required for the workflow to continue. Here is how the output of notification_join will look like. The output is a map, where the keys are the names of task references that were being joinOn . The corresponding values are the outputs of those tasks. { \"email_notification_ref\": { \"email_sent_at\": \"2021-11-06T07:37:17+0000\", \"email_sent_to\": \"test@example.com\" }, \"sms_notification_ref\": { \"smm_sent_at\": \"2021-11-06T07:37:17+0129\", \"sms_sen\": \"+1-425-555-0189\" } } See JOIN for more details ni the JOIN aspect of the FORK.","title":"Fork"},{"location":"reference-docs/fork-task/#fork","text":"\"type\" : \"FORK_JOIN\"","title":"Fork"},{"location":"reference-docs/fork-task/#introduction","text":"A Fork operation lets you run a specified list of tasks or sub workflows in parallel. A fork task is followed by a join operation that waits on the forked tasks or sub workflows to finish. The JOIN task also collects outputs from each of the forked tasks or sub workflows.","title":"Introduction"},{"location":"reference-docs/fork-task/#use-cases","text":"FORK_JOIN tasks are typically used when a list of tasks can be run in parallel. E.g In a notification workflow, there could be multiple ways of sending notifications, i,e e-mail, SMS, HTTP etc.. These notifications are not dependent on each other, and so they can be run in parallel. In such cases, you can create 3 sub-lists of forked tasks for each of these operations.","title":"Use Cases"},{"location":"reference-docs/fork-task/#configuration","text":"A FORK_JOIN task has a forkTasks attribute that expects an array. Each array is a sub-list of tasks. Each of these sub-lists are then invoked in parallel. The tasks defined within each sublist can be sequential or any other way as desired. A FORK_JOIN task has to be followed by a JOIN operation. The JOIN operator specifies which of the forked tasks to joinOn (wait for completion) before moving to the next stage in the workflow.","title":"Configuration"},{"location":"reference-docs/fork-task/#input-configuration","text":"Attribute Description name Task Name. A unique name that is descriptive of the task function taskReferenceName Task Reference Name. A unique reference to this task. There can be multiple references of a task within the same workflow definition type Task Type. In this case, FORK_JOIN inputParameters The input parameters that will be supplied to this task forkTasks A list of a list of tasks. Each of the outer list will be invoked in parallel. The inner list can be a graph of other tasks and sub-workflows","title":"Input Configuration"},{"location":"reference-docs/fork-task/#output-configuration","text":"This is the output configuration of the JOIN task that is used in conjunction with the FORK_JOIN task. The output of the JOIN task is a map, where the keys are the names of the task reference names where were being joinOn and the keys are the corresponding outputs of those tasks. Attribute Description task_ref_name_1 A task reference name that was being joinOn . The value is the output of that task task_ref_name_2 A task reference name that was being joinOn . The value is the output of that task ... ... task_ref_name_N A task reference name that was being joinOn . The value is the output of that task","title":"Output Configuration"},{"location":"reference-docs/fork-task/#example","text":"Imagine a workflow that sends 3 notifications: email, SMS and HTTP. Since none of these steps are dependant on the others, they can be run in parallel with a fork. The diagram will appear as: Here's the JSON definition for the workflow: [ { \"name\": \"fork_join\", \"taskReferenceName\": \"my_fork_join_ref\", \"type\": \"FORK_JOIN\", \"forkTasks\": [ [ { \"name\": \"process_notification_payload\", \"taskReferenceName\": \"process_notification_payload_email\", \"type\": \"SIMPLE\" }, { \"name\": \"email_notification\", \"taskReferenceName\": \"email_notification_ref\", \"type\": \"SIMPLE\" } ], [ { \"name\": \"process_notification_payload\", \"taskReferenceName\": \"process_notification_payload_sms\", \"type\": \"SIMPLE\" }, { \"name\": \"sms_notification\", \"taskReferenceName\": \"sms_notification_ref\", \"type\": \"SIMPLE\" } ], [ { \"name\": \"process_notification_payload\", \"taskReferenceName\": \"process_notification_payload_http\", \"type\": \"SIMPLE\" }, { \"name\": \"http_notification\", \"taskReferenceName\": \"http_notification_ref\", \"type\": \"SIMPLE\" } ] ] }, { \"name\": \"notification_join\", \"taskReferenceName\": \"notification_join_ref\", \"type\": \"JOIN\", \"joinOn\": [ \"email_notification_ref\", \"sms_notification_ref\" ] } ] Note: There are three parallel 'tines' to this fork, but only two of the outputs are required for the JOIN to continue. The diagram does draw an arrow from http_notification_ref to the notification_join , but it is not required for the workflow to continue. Here is how the output of notification_join will look like. The output is a map, where the keys are the names of task references that were being joinOn . The corresponding values are the outputs of those tasks. { \"email_notification_ref\": { \"email_sent_at\": \"2021-11-06T07:37:17+0000\", \"email_sent_to\": \"test@example.com\" }, \"sms_notification_ref\": { \"smm_sent_at\": \"2021-11-06T07:37:17+0129\", \"sms_sen\": \"+1-425-555-0189\" } } See JOIN for more details ni the JOIN aspect of the FORK.","title":"Example"},{"location":"reference-docs/http-task/","text":"HTTP Task \u00b6 \"type\" : \"HTTP\" Introduction \u00b6 An HTTP task is useful when you have a requirements such as: Making calls to another service that exposes an API via HTTP Fetch any resource or data present on an endpoint Use Cases \u00b6 If we have a scenario where we need to make an HTTP call into another service, we can make use of HTTP tasks. You can use the data returned from the HTTP call in your subsequent tasks as inputs. Using HTTP tasks you can avoid having to write the code that talks to these services and instead let Conductor manage it directly. This can reduce the code you have to maintain and allows for a lot of flexibility. Configuration \u00b6 HTTP task is defined directly inside the workflow with the task type HTTP . name type description http_request HttpRequest JSON object (see below) Inputs \u00b6 Name Type Description uri String URI for the service. Can be a partial when using vipAddress or includes the server address. method String HTTP method. GET, PUT, POST, DELETE, OPTIONS, HEAD accept String Accept header. Default: application/json contentType String Content Type - supported types are text/plain , text/html , and application/json (Default) headers Map[String, Any] A map of additional http headers to be sent along with the request. body Map[] Request body vipAddress String When using discovery based service URLs. asyncComplete Boolean false to mark status COMPLETED upon execution ; true to keep it IN_PROGRESS, wait for an external event (via Conductor or SQS or EventHandler) to complete it. oauthConsumerKey String OAuth client consumer key oauthConsumerSecret String OAuth client consumer secret connectionTimeOut Integer Connection Time Out in milliseconds. If set to 0, equivalent to infinity. Default: 100. readTimeOut Integer Read Time Out in milliseconds. If set to 0, equivalent to infinity. Default: 150. Output \u00b6 name type description response Map JSON body containing the response if one is present headers Map[String, Any] Response Headers statusCode Integer Http Status Code reasonPhrase String Http Status Code's reason phrase Examples \u00b6 Following is the example of HTTP task with GET method. We can use variables in our URI as show in the example below. { \"name\": \"Get Example\", \"taskReferenceName\": \"get_example\", \"inputParameters\": { \"http_request\": { \"uri\": \"https://jsonplaceholder.typicode.com/posts/${workflow.input.queryid}\", \"method\": \"GET\" } }, \"type\": \"HTTP\" } Following is the example of HTTP task with POST method. Here we are using variables for our POST body which happens to be data from a previous task. This is an example of how you can chain HTTP calls to make complex flows happen without writing any additional code. { \"name\": \"http_post_example\", \"taskReferenceName\": \"post_example\", \"inputParameters\": { \"http_request\": { \"uri\": \"https://jsonplaceholder.typicode.com/posts/\", \"method\": \"POST\", \"body\": { \"title\": \"${get_example.output.response.body.title}\", \"userId\": \"${get_example.output.response.body.userId}\", \"action\": \"doSomething\" } } }, \"type\": \"HTTP\" } Following is the example of HTTP task with PUT method. { \"name\": \"http_put_example\", \"taskReferenceName\": \"put_example\", \"inputParameters\": { \"http_request\": { \"uri\": \"https://jsonplaceholder.typicode.com/posts/1\", \"method\": \"PUT\", \"body\": { \"title\": \"${get_example.output.response.body.title}\", \"userId\": \"${get_example.output.response.body.userId}\", \"action\": \"doSomethingDifferent\" } } }, \"type\": \"HTTP\" } Following is the example of HTTP task with DELETE method. { \"name\": \"DELETE Example\", \"taskReferenceName\": \"delete_example\", \"inputParameters\": { \"http_request\": { \"uri\": \"https://jsonplaceholder.typicode.com/posts/1\", \"method\": \"DELETE\" } }, \"type\": \"HTTP\" } Best Practices \u00b6 Why are my HTTP tasks not getting picked up? We might have too many HTTP tasks in the queue. There is a concept called Isolation Groups that you can rely on for prioritizing certain HTTP tasks over others. Read more here: Isolation Groups","title":"HTTP Task"},{"location":"reference-docs/http-task/#http-task","text":"\"type\" : \"HTTP\"","title":"HTTP Task"},{"location":"reference-docs/http-task/#introduction","text":"An HTTP task is useful when you have a requirements such as: Making calls to another service that exposes an API via HTTP Fetch any resource or data present on an endpoint","title":"Introduction"},{"location":"reference-docs/http-task/#use-cases","text":"If we have a scenario where we need to make an HTTP call into another service, we can make use of HTTP tasks. You can use the data returned from the HTTP call in your subsequent tasks as inputs. Using HTTP tasks you can avoid having to write the code that talks to these services and instead let Conductor manage it directly. This can reduce the code you have to maintain and allows for a lot of flexibility.","title":"Use Cases"},{"location":"reference-docs/http-task/#configuration","text":"HTTP task is defined directly inside the workflow with the task type HTTP . name type description http_request HttpRequest JSON object (see below)","title":"Configuration"},{"location":"reference-docs/http-task/#inputs","text":"Name Type Description uri String URI for the service. Can be a partial when using vipAddress or includes the server address. method String HTTP method. GET, PUT, POST, DELETE, OPTIONS, HEAD accept String Accept header. Default: application/json contentType String Content Type - supported types are text/plain , text/html , and application/json (Default) headers Map[String, Any] A map of additional http headers to be sent along with the request. body Map[] Request body vipAddress String When using discovery based service URLs. asyncComplete Boolean false to mark status COMPLETED upon execution ; true to keep it IN_PROGRESS, wait for an external event (via Conductor or SQS or EventHandler) to complete it. oauthConsumerKey String OAuth client consumer key oauthConsumerSecret String OAuth client consumer secret connectionTimeOut Integer Connection Time Out in milliseconds. If set to 0, equivalent to infinity. Default: 100. readTimeOut Integer Read Time Out in milliseconds. If set to 0, equivalent to infinity. Default: 150.","title":"Inputs"},{"location":"reference-docs/http-task/#output","text":"name type description response Map JSON body containing the response if one is present headers Map[String, Any] Response Headers statusCode Integer Http Status Code reasonPhrase String Http Status Code's reason phrase","title":"Output"},{"location":"reference-docs/http-task/#examples","text":"Following is the example of HTTP task with GET method. We can use variables in our URI as show in the example below. { \"name\": \"Get Example\", \"taskReferenceName\": \"get_example\", \"inputParameters\": { \"http_request\": { \"uri\": \"https://jsonplaceholder.typicode.com/posts/${workflow.input.queryid}\", \"method\": \"GET\" } }, \"type\": \"HTTP\" } Following is the example of HTTP task with POST method. Here we are using variables for our POST body which happens to be data from a previous task. This is an example of how you can chain HTTP calls to make complex flows happen without writing any additional code. { \"name\": \"http_post_example\", \"taskReferenceName\": \"post_example\", \"inputParameters\": { \"http_request\": { \"uri\": \"https://jsonplaceholder.typicode.com/posts/\", \"method\": \"POST\", \"body\": { \"title\": \"${get_example.output.response.body.title}\", \"userId\": \"${get_example.output.response.body.userId}\", \"action\": \"doSomething\" } } }, \"type\": \"HTTP\" } Following is the example of HTTP task with PUT method. { \"name\": \"http_put_example\", \"taskReferenceName\": \"put_example\", \"inputParameters\": { \"http_request\": { \"uri\": \"https://jsonplaceholder.typicode.com/posts/1\", \"method\": \"PUT\", \"body\": { \"title\": \"${get_example.output.response.body.title}\", \"userId\": \"${get_example.output.response.body.userId}\", \"action\": \"doSomethingDifferent\" } } }, \"type\": \"HTTP\" } Following is the example of HTTP task with DELETE method. { \"name\": \"DELETE Example\", \"taskReferenceName\": \"delete_example\", \"inputParameters\": { \"http_request\": { \"uri\": \"https://jsonplaceholder.typicode.com/posts/1\", \"method\": \"DELETE\" } }, \"type\": \"HTTP\" }","title":"Examples"},{"location":"reference-docs/http-task/#best-practices","text":"Why are my HTTP tasks not getting picked up? We might have too many HTTP tasks in the queue. There is a concept called Isolation Groups that you can rely on for prioritizing certain HTTP tasks over others. Read more here: Isolation Groups","title":"Best Practices"},{"location":"reference-docs/inline-task/","text":"Inline Task \u00b6 \"type\": \"INLINE\" Introduction \u00b6 Inline Task helps execute necessary logic at Workflow run-time, using an evaluator. There are two supported evaluators as of now: Configuration \u00b6 name description value-param Use a parameter directly as the value javascript Evaluate Javascript expressions and compute value Use Cases \u00b6 Consider a scenario, we have to run simple evaluations in Conductor server while creating Workers. Inline task can be used to run these evaluations using an evaluator engine. Example 1 \u00b6 { \"name\": \"inline_task_example\", \"taskReferenceName\": \"inline_task_example\", \"type\": \"INLINE\", \"inputParameters\": { \"value\": \"${workflow.input.value}\", \"evaluatorType\": \"javascript\", \"expression\": \"function e() { if ($.value == 1){return {\\\"result\\\": true}} else { return {\\\"result\\\": false}}} e();\" } } Following are the parameters in the above example : \"evaluatorType\" - Type of the evaluator. Supported evaluators: value-param, javascript which evaluates javascript expression. \"expression\" - Expression associated with the type of evaluator. For javascript evaluator, Javascript evaluation engine is used to evaluate expression defined as a string. Must return a value. Besides expression, any of the properties in the input values is accessible as $.value for the expression to evaluate. The task output can then be referenced in downstream tasks like: \"${inline_test.output.result}\" Example 2 \u00b6 Perhaps a weather API sometimes returns Celcius, and sometimes returns Farenheit temperature values. This task ensures that the downstream tasks ONLY receive Celcius values: { \"name\": \"INLINE_TASK\", \"taskReferenceName\": \"inline_test\", \"type\": \"INLINE\", \"inputParameters\": { \"scale\": \"${workflow.input.tempScale}\", \"temperature\": \"${workflow.input.temperature}\", \"evaluatorType\": \"javascript\", \"expression\": \"function SIvaluesOnly(){if ($.scale === \"F\"){ centigrade = ($.temperature -32)*5/9; return {temperature: centigrade} } else { return {temperature: $.temperature} }} SIvaluesOnly();\" } }","title":"Inline Task"},{"location":"reference-docs/inline-task/#inline-task","text":"\"type\": \"INLINE\"","title":"Inline Task"},{"location":"reference-docs/inline-task/#introduction","text":"Inline Task helps execute necessary logic at Workflow run-time, using an evaluator. There are two supported evaluators as of now:","title":"Introduction"},{"location":"reference-docs/inline-task/#configuration","text":"name description value-param Use a parameter directly as the value javascript Evaluate Javascript expressions and compute value","title":"Configuration"},{"location":"reference-docs/inline-task/#use-cases","text":"Consider a scenario, we have to run simple evaluations in Conductor server while creating Workers. Inline task can be used to run these evaluations using an evaluator engine.","title":"Use Cases"},{"location":"reference-docs/inline-task/#example-1","text":"{ \"name\": \"inline_task_example\", \"taskReferenceName\": \"inline_task_example\", \"type\": \"INLINE\", \"inputParameters\": { \"value\": \"${workflow.input.value}\", \"evaluatorType\": \"javascript\", \"expression\": \"function e() { if ($.value == 1){return {\\\"result\\\": true}} else { return {\\\"result\\\": false}}} e();\" } } Following are the parameters in the above example : \"evaluatorType\" - Type of the evaluator. Supported evaluators: value-param, javascript which evaluates javascript expression. \"expression\" - Expression associated with the type of evaluator. For javascript evaluator, Javascript evaluation engine is used to evaluate expression defined as a string. Must return a value. Besides expression, any of the properties in the input values is accessible as $.value for the expression to evaluate. The task output can then be referenced in downstream tasks like: \"${inline_test.output.result}\"","title":"Example 1"},{"location":"reference-docs/inline-task/#example-2","text":"Perhaps a weather API sometimes returns Celcius, and sometimes returns Farenheit temperature values. This task ensures that the downstream tasks ONLY receive Celcius values: { \"name\": \"INLINE_TASK\", \"taskReferenceName\": \"inline_test\", \"type\": \"INLINE\", \"inputParameters\": { \"scale\": \"${workflow.input.tempScale}\", \"temperature\": \"${workflow.input.temperature}\", \"evaluatorType\": \"javascript\", \"expression\": \"function SIvaluesOnly(){if ($.scale === \"F\"){ centigrade = ($.temperature -32)*5/9; return {temperature: centigrade} } else { return {temperature: $.temperature} }} SIvaluesOnly();\" } }","title":"Example 2"},{"location":"reference-docs/join-task/","text":"Join \u00b6 \"type\" : \"JOIN\" Introduction \u00b6 A JOIN task is used in conjunction with a FORK_JOIN or FORK_JOIN_DYNAMIC task. When JOIN is used along with a FORK_JOIN task, tt waits for a list of zero or more of the forked tasks to be completed. However, when used with a FORK_JOIN_DYNAMIC task, it implicitly waits for all of the dynamically forked tasks to complete. Use Cases \u00b6 FORK_JOIN and FORK_JOIN_DYNAMIC task are used to execute a collection of other tasks or sub workflows in parallel. In such cases, there is a need for these forked tasks to complete before moving to the next stage in the workflow. Configuration \u00b6 Input Configuration \u00b6 Attribute Description name Task Name. A unique name that is descriptive of the task function taskReferenceName Task Reference Name. A unique reference to this task. There can be multiple references of a task within the same workflow definition type Task Type. In this case, JOIN joinOn A list of task reference names, that this JOIN task will wait for completion Output Configuration \u00b6 Attribute Description task_ref_name_1 A task reference name that was being joinOn . The value is the output of that task task_ref_name_2 A task reference name that was being joinOn . The value is the output of that task ... ... task_ref_name_N A task reference name that was being joinOn . The value is the output of that task Examples \u00b6 Simple Example \u00b6 Here is an example of a JOIN task. This task will wait for the completion of tasks my_task_ref_1 and my_task_ref_2 as specified by the joinOn attribute. { \"name\": \"join_task\", \"taskReferenceName\": \"my_join_task_ref\", \"type\": \"JOIN\", \"joinOn\": [ \"my_task_ref_1\", \"my_task_ref_2\" ] } Example - ignoring one fork \u00b6 Here is an example of a JOIN task used in conjunction with a FORK_JOIN task. The 'FORK_JOIN' spawns 3 tasks. An email_notification task, a sms_notification task and a http_notification task. Email and SMS are usually best effort delivery systems. However, in case of a http based notification you get a return code and you can retry until it succeeds or eventually give up. When you setup a notification workflow, you may decide to continue ,if you kicked off an email and sms notification. Im that case, you can decide to joinOn those specific tasks. However, the http_notification task will still continue to execute, but it will not block the rest of the workflow from proceeding. [ { \"name\": \"fork_join\", \"taskReferenceName\": \"my_fork_join_ref\", \"type\": \"FORK_JOIN\", \"forkTasks\": [ [ { \"name\": \"email_notification\", \"taskReferenceName\": \"email_notification_ref\", \"type\": \"SIMPLE\" } ], [ { \"name\": \"sms_notification\", \"taskReferenceName\": \"sms_notification_ref\", \"type\": \"SIMPLE\" } ], [ { \"name\": \"http_notification\", \"taskReferenceName\": \"http_notification_ref\", \"type\": \"SIMPLE\" } ] ] }, { \"name\": \"notification_join\", \"taskReferenceName\": \"notification_join_ref\", \"type\": \"JOIN\", \"joinOn\": [ \"email_notification_ref\", \"sms_notification_ref\" ] } ] Here is how the output of notification_join will look like. The output is a map, where the keys are the names of task references that were being joinOn . The corresponding values are the outputs of those tasks. { \"email_notification_ref\": { \"email_sent_at\": \"2021-11-06T07:37:17+0000\", \"email_sent_to\": \"test@example.com\" }, \"sms_notification_ref\": { \"smm_sent_at\": \"2021-11-06T07:37:17+0129\", \"sms_sen\": \"+1-425-555-0189\" } }","title":"Join"},{"location":"reference-docs/join-task/#join","text":"\"type\" : \"JOIN\"","title":"Join"},{"location":"reference-docs/join-task/#introduction","text":"A JOIN task is used in conjunction with a FORK_JOIN or FORK_JOIN_DYNAMIC task. When JOIN is used along with a FORK_JOIN task, tt waits for a list of zero or more of the forked tasks to be completed. However, when used with a FORK_JOIN_DYNAMIC task, it implicitly waits for all of the dynamically forked tasks to complete.","title":"Introduction"},{"location":"reference-docs/join-task/#use-cases","text":"FORK_JOIN and FORK_JOIN_DYNAMIC task are used to execute a collection of other tasks or sub workflows in parallel. In such cases, there is a need for these forked tasks to complete before moving to the next stage in the workflow.","title":"Use Cases"},{"location":"reference-docs/join-task/#configuration","text":"","title":"Configuration"},{"location":"reference-docs/join-task/#input-configuration","text":"Attribute Description name Task Name. A unique name that is descriptive of the task function taskReferenceName Task Reference Name. A unique reference to this task. There can be multiple references of a task within the same workflow definition type Task Type. In this case, JOIN joinOn A list of task reference names, that this JOIN task will wait for completion","title":"Input Configuration"},{"location":"reference-docs/join-task/#output-configuration","text":"Attribute Description task_ref_name_1 A task reference name that was being joinOn . The value is the output of that task task_ref_name_2 A task reference name that was being joinOn . The value is the output of that task ... ... task_ref_name_N A task reference name that was being joinOn . The value is the output of that task","title":"Output Configuration"},{"location":"reference-docs/join-task/#examples","text":"","title":"Examples"},{"location":"reference-docs/join-task/#simple-example","text":"Here is an example of a JOIN task. This task will wait for the completion of tasks my_task_ref_1 and my_task_ref_2 as specified by the joinOn attribute. { \"name\": \"join_task\", \"taskReferenceName\": \"my_join_task_ref\", \"type\": \"JOIN\", \"joinOn\": [ \"my_task_ref_1\", \"my_task_ref_2\" ] }","title":"Simple Example"},{"location":"reference-docs/join-task/#example-ignoring-one-fork","text":"Here is an example of a JOIN task used in conjunction with a FORK_JOIN task. The 'FORK_JOIN' spawns 3 tasks. An email_notification task, a sms_notification task and a http_notification task. Email and SMS are usually best effort delivery systems. However, in case of a http based notification you get a return code and you can retry until it succeeds or eventually give up. When you setup a notification workflow, you may decide to continue ,if you kicked off an email and sms notification. Im that case, you can decide to joinOn those specific tasks. However, the http_notification task will still continue to execute, but it will not block the rest of the workflow from proceeding. [ { \"name\": \"fork_join\", \"taskReferenceName\": \"my_fork_join_ref\", \"type\": \"FORK_JOIN\", \"forkTasks\": [ [ { \"name\": \"email_notification\", \"taskReferenceName\": \"email_notification_ref\", \"type\": \"SIMPLE\" } ], [ { \"name\": \"sms_notification\", \"taskReferenceName\": \"sms_notification_ref\", \"type\": \"SIMPLE\" } ], [ { \"name\": \"http_notification\", \"taskReferenceName\": \"http_notification_ref\", \"type\": \"SIMPLE\" } ] ] }, { \"name\": \"notification_join\", \"taskReferenceName\": \"notification_join_ref\", \"type\": \"JOIN\", \"joinOn\": [ \"email_notification_ref\", \"sms_notification_ref\" ] } ] Here is how the output of notification_join will look like. The output is a map, where the keys are the names of task references that were being joinOn . The corresponding values are the outputs of those tasks. { \"email_notification_ref\": { \"email_sent_at\": \"2021-11-06T07:37:17+0000\", \"email_sent_to\": \"test@example.com\" }, \"sms_notification_ref\": { \"smm_sent_at\": \"2021-11-06T07:37:17+0129\", \"sms_sen\": \"+1-425-555-0189\" } }","title":"Example - ignoring one fork"},{"location":"reference-docs/json-jq-transform-task/","text":"JSON JQ Transform Task \u00b6 \"type\" : \"JSON_JQ_TRANSFORM_TASK\" Introduction \u00b6 JSON_JQ_TRANSFORM_TASK is a System task that allows processing of JSON data that is supplied to the task, by using the popular JQ processing tool\u2019s query expression language. Check the JQ Manual , and the JQ Playground for more information on JQ Use Cases \u00b6 JSON is a popular format of choice for data-interchange. It is widely used in web and server applications, document storage, API I/O etc. It\u2019s also used within Conductor to define workflow and task definitions and passing data and state between tasks and workflows. This makes a tool like JQ a natural fit for processing task related data. Some common usages within Conductor includes, working with HTTP task, JOIN tasks or standalone tasks that try to transform data from the output of one task to the input of another. Configuration \u00b6 Attribute Description name Task Name. A unique name that is descriptive of the task function taskReferenceName Task Reference Name. A unique reference to this task. There can be multiple references of a task within the same workflow definition type Task Type. In this case, JSON_JQ_TRANSFORM inputParameters The input parameters that will be supplied to this task. The parameters will be a JSON object of atleast 2 attributes, one of which will be called queryExpression. The others are user named attributes. These attributes will be accessible by the JQ query processor inputParameters/user-defined-key(s) User defined key(s) along with values. inputParameters/queryExpression A JQ query expression Output Configuration \u00b6 Attribute Description result The first results returned by the JQ expression resultList A List of results returned by the JQ expression error An optional error message, indicating that the JQ query failed processing Example \u00b6 Here is an example of a JSON_JQ_TRANSFORM task. The inputParameters attribute is expected to have a value object that has the following A list of key value pair objects denoted key1/value1, key2/value2 in the example below. Note the key1/value1 are arbitrary names used in this example. A key with the name queryExpression , whose value is a JQ expression. The expression will operate on the value of the inputParameters attribute. In the example below, the inputParameters has 2 inner objects named by attributes key1 and key2 , each of which has an object that is named value1 and value2 . They have an associated array of strings as values, \"a\", \"b\" and \"c\", \"d\" . The expression key3: (.key1.value1 + .key2.value2) concat's the 2 string arrays into a single array against an attribute named key3 { \"name\": \"jq_example_task\", \"taskReferenceName\": \"my_jq_example_task\", \"type\": \"JSON_JQ_TRANSFORM\", \"inputParameters\": { \"key1\": { \"value1\": [ \"a\", \"b\" ] }, \"key2\": { \"value2\": [ \"c\", \"d\" ] }, \"queryExpression\": \"{ key3: (.key1.value1 + .key2.value2) }\" } } The execution of this example task above will provide the following output. The resultList attribute stores the full list of the queryExpression result. The result attribute stores the first element of the resultList. An optional error attribute along with a string message will be returned if there was an error processing the query expression. { \"result\": { \"key3\": [ \"a\", \"b\", \"c\", \"d\" ] }, \"resultList\": [ { \"key3\": [ \"a\", \"b\", \"c\", \"d\" ] } ] }","title":"JSON JQ Transform Task"},{"location":"reference-docs/json-jq-transform-task/#json-jq-transform-task","text":"\"type\" : \"JSON_JQ_TRANSFORM_TASK\"","title":"JSON JQ Transform Task"},{"location":"reference-docs/json-jq-transform-task/#introduction","text":"JSON_JQ_TRANSFORM_TASK is a System task that allows processing of JSON data that is supplied to the task, by using the popular JQ processing tool\u2019s query expression language. Check the JQ Manual , and the JQ Playground for more information on JQ","title":"Introduction"},{"location":"reference-docs/json-jq-transform-task/#use-cases","text":"JSON is a popular format of choice for data-interchange. It is widely used in web and server applications, document storage, API I/O etc. It\u2019s also used within Conductor to define workflow and task definitions and passing data and state between tasks and workflows. This makes a tool like JQ a natural fit for processing task related data. Some common usages within Conductor includes, working with HTTP task, JOIN tasks or standalone tasks that try to transform data from the output of one task to the input of another.","title":"Use Cases"},{"location":"reference-docs/json-jq-transform-task/#configuration","text":"Attribute Description name Task Name. A unique name that is descriptive of the task function taskReferenceName Task Reference Name. A unique reference to this task. There can be multiple references of a task within the same workflow definition type Task Type. In this case, JSON_JQ_TRANSFORM inputParameters The input parameters that will be supplied to this task. The parameters will be a JSON object of atleast 2 attributes, one of which will be called queryExpression. The others are user named attributes. These attributes will be accessible by the JQ query processor inputParameters/user-defined-key(s) User defined key(s) along with values. inputParameters/queryExpression A JQ query expression","title":"Configuration"},{"location":"reference-docs/json-jq-transform-task/#output-configuration","text":"Attribute Description result The first results returned by the JQ expression resultList A List of results returned by the JQ expression error An optional error message, indicating that the JQ query failed processing","title":"Output Configuration"},{"location":"reference-docs/json-jq-transform-task/#example","text":"Here is an example of a JSON_JQ_TRANSFORM task. The inputParameters attribute is expected to have a value object that has the following A list of key value pair objects denoted key1/value1, key2/value2 in the example below. Note the key1/value1 are arbitrary names used in this example. A key with the name queryExpression , whose value is a JQ expression. The expression will operate on the value of the inputParameters attribute. In the example below, the inputParameters has 2 inner objects named by attributes key1 and key2 , each of which has an object that is named value1 and value2 . They have an associated array of strings as values, \"a\", \"b\" and \"c\", \"d\" . The expression key3: (.key1.value1 + .key2.value2) concat's the 2 string arrays into a single array against an attribute named key3 { \"name\": \"jq_example_task\", \"taskReferenceName\": \"my_jq_example_task\", \"type\": \"JSON_JQ_TRANSFORM\", \"inputParameters\": { \"key1\": { \"value1\": [ \"a\", \"b\" ] }, \"key2\": { \"value2\": [ \"c\", \"d\" ] }, \"queryExpression\": \"{ key3: (.key1.value1 + .key2.value2) }\" } } The execution of this example task above will provide the following output. The resultList attribute stores the full list of the queryExpression result. The result attribute stores the first element of the resultList. An optional error attribute along with a string message will be returned if there was an error processing the query expression. { \"result\": { \"key3\": [ \"a\", \"b\", \"c\", \"d\" ] }, \"resultList\": [ { \"key3\": [ \"a\", \"b\", \"c\", \"d\" ] } ] }","title":"Example"},{"location":"reference-docs/kafka-publish-task/","text":"Kafka Publish Task \u00b6 \"type\" : \"KAFKA_PUBLISH\" Introduction \u00b6 A Kafka Publish task is used to push messages to another microservice via Kafka. Configuration \u00b6 The task expects an input parameter named kafka_request as part of the task's input with the following details: name description bootStrapServers bootStrapServers for connecting to given kafka. key Key to be published keySerializer Serializer used for serializing the key published to kafka. One of the following can be set : 1. org.apache.kafka.common.serialization.IntegerSerializer 2. org.apache.kafka.common.serialization.LongSerializer 3. org.apache.kafka.common.serialization.StringSerializer. Default is String serializer value Value published to kafka requestTimeoutMs Request timeout while publishing to kafka. If this value is not given the value is read from the property kafka.publish.request.timeout.ms . If the property is not set the value defaults to 100 ms maxBlockMs maxBlockMs while publishing to kafka. If this value is not given the value is read from the property kafka.publish.max.block.ms . If the property is not set the value defaults to 500 ms headers A map of additional kafka headers to be sent along with the request. topic Topic to publish Examples \u00b6 Sample Task { \"name\": \"call_kafka\", \"taskReferenceName\": \"call_kafka\", \"inputParameters\": { \"kafka_request\": { \"topic\": \"userTopic\", \"value\": \"Message to publish\", \"bootStrapServers\": \"localhost:9092\", \"headers\": { \"x-Auth\":\"Auth-key\" }, \"key\": \"123\", \"keySerializer\": \"org.apache.kafka.common.serialization.IntegerSerializer\" } }, \"type\": \"KAFKA_PUBLISH\" } The task expects an input parameter named \"kafka_request\" as part of the task's input with the following details: \"bootStrapServers\" - bootStrapServers for connecting to given kafka. \"key\" - Key to be published. \"keySerializer\" - Serializer used for serializing the key published to kafka. One of the following can be set : a. org.apache.kafka.common.serialization.IntegerSerializer b. org.apache.kafka.common.serialization.LongSerializer c. org.apache.kafka.common.serialization.StringSerializer. Default is String serializer. \"value\" - Value published to kafka \"requestTimeoutMs\" - Request timeout while publishing to kafka. If this value is not given the value is read from the property kafka.publish.request.timeout.ms. If the property is not set the value defaults to 100 ms. \"maxBlockMs\" - maxBlockMs while publishing to kafka. If this value is not given the value is read from the property kafka.publish.max.block.ms. If the property is not set the value defaults to 500 ms. \"headers\" - A map of additional kafka headers to be sent along with the request. \"topic\" - Topic to publish. The producer created in the kafka task is cached. By default the cache size is 10 and expiry time is 120000 ms. To change the defaults following can be modified kafka.publish.producer.cache.size, kafka.publish.producer.cache.time.ms respectively. Kafka Task Output \u00b6 Task status transitions to COMPLETED . The task is marked as FAILED if the message could not be published to the Kafka queue.","title":"Kafka Publish Task"},{"location":"reference-docs/kafka-publish-task/#kafka-publish-task","text":"\"type\" : \"KAFKA_PUBLISH\"","title":"Kafka Publish Task"},{"location":"reference-docs/kafka-publish-task/#introduction","text":"A Kafka Publish task is used to push messages to another microservice via Kafka.","title":"Introduction"},{"location":"reference-docs/kafka-publish-task/#configuration","text":"The task expects an input parameter named kafka_request as part of the task's input with the following details: name description bootStrapServers bootStrapServers for connecting to given kafka. key Key to be published keySerializer Serializer used for serializing the key published to kafka. One of the following can be set : 1. org.apache.kafka.common.serialization.IntegerSerializer 2. org.apache.kafka.common.serialization.LongSerializer 3. org.apache.kafka.common.serialization.StringSerializer. Default is String serializer value Value published to kafka requestTimeoutMs Request timeout while publishing to kafka. If this value is not given the value is read from the property kafka.publish.request.timeout.ms . If the property is not set the value defaults to 100 ms maxBlockMs maxBlockMs while publishing to kafka. If this value is not given the value is read from the property kafka.publish.max.block.ms . If the property is not set the value defaults to 500 ms headers A map of additional kafka headers to be sent along with the request. topic Topic to publish","title":"Configuration"},{"location":"reference-docs/kafka-publish-task/#examples","text":"Sample Task { \"name\": \"call_kafka\", \"taskReferenceName\": \"call_kafka\", \"inputParameters\": { \"kafka_request\": { \"topic\": \"userTopic\", \"value\": \"Message to publish\", \"bootStrapServers\": \"localhost:9092\", \"headers\": { \"x-Auth\":\"Auth-key\" }, \"key\": \"123\", \"keySerializer\": \"org.apache.kafka.common.serialization.IntegerSerializer\" } }, \"type\": \"KAFKA_PUBLISH\" } The task expects an input parameter named \"kafka_request\" as part of the task's input with the following details: \"bootStrapServers\" - bootStrapServers for connecting to given kafka. \"key\" - Key to be published. \"keySerializer\" - Serializer used for serializing the key published to kafka. One of the following can be set : a. org.apache.kafka.common.serialization.IntegerSerializer b. org.apache.kafka.common.serialization.LongSerializer c. org.apache.kafka.common.serialization.StringSerializer. Default is String serializer. \"value\" - Value published to kafka \"requestTimeoutMs\" - Request timeout while publishing to kafka. If this value is not given the value is read from the property kafka.publish.request.timeout.ms. If the property is not set the value defaults to 100 ms. \"maxBlockMs\" - maxBlockMs while publishing to kafka. If this value is not given the value is read from the property kafka.publish.max.block.ms. If the property is not set the value defaults to 500 ms. \"headers\" - A map of additional kafka headers to be sent along with the request. \"topic\" - Topic to publish. The producer created in the kafka task is cached. By default the cache size is 10 and expiry time is 120000 ms. To change the defaults following can be modified kafka.publish.producer.cache.size, kafka.publish.producer.cache.time.ms respectively.","title":"Examples"},{"location":"reference-docs/kafka-publish-task/#kafka-task-output","text":"Task status transitions to COMPLETED . The task is marked as FAILED if the message could not be published to the Kafka queue.","title":"Kafka Task Output"},{"location":"reference-docs/sample-layout/","text":"Dynamic Task \u00b6 What is a Dynamic Task? \u00b6 TODO: What is a Dynamic task? How does it work? Common Use Cases \u00b6 TODO: List out some common use cases Configuration / Properties \u00b6 Inputs \u00b6 TODO: Talk about inputs for the task Output \u00b6 TODO: Talk about output of the task, what to expect Examples \u00b6 TODO: Example 1 TODO: Example 2 FAQs \u00b6 TODO: Gotchas and other nuances Question 1 Answer Question 2 Answer","title":"Dynamic Task"},{"location":"reference-docs/sample-layout/#dynamic-task","text":"","title":"Dynamic Task"},{"location":"reference-docs/sample-layout/#what-is-a-dynamic-task","text":"TODO: What is a Dynamic task? How does it work?","title":"What is a Dynamic Task?"},{"location":"reference-docs/sample-layout/#common-use-cases","text":"TODO: List out some common use cases","title":"Common Use Cases"},{"location":"reference-docs/sample-layout/#configuration-properties","text":"","title":"Configuration / Properties"},{"location":"reference-docs/sample-layout/#inputs","text":"TODO: Talk about inputs for the task","title":"Inputs"},{"location":"reference-docs/sample-layout/#output","text":"TODO: Talk about output of the task, what to expect","title":"Output"},{"location":"reference-docs/sample-layout/#examples","text":"TODO: Example 1 TODO: Example 2","title":"Examples"},{"location":"reference-docs/sample-layout/#faqs","text":"TODO: Gotchas and other nuances Question 1 Answer Question 2 Answer","title":"FAQs"},{"location":"reference-docs/set-variable-task/","text":"Set Variable \u00b6 \"type\" : \"SET_VARIABLE\" Introduction \u00b6 Set Variable allows us to set workflow variables by creating or updating them with new values. Use Cases \u00b6 Variables can be initialized in the workflow definition as well as during the workflow run. Once a variable was initialized it can be read or overwritten with a new value by any other task. Configuration \u00b6 Set Variable task is defined directly inside the workflow with type SET_VARIABLE . Examples \u00b6 Suppose in a workflow, we have to store a value in a variable and then later in workflow reuse the value stored in the variable just as we do in programming, in such scenarios Set Variable task can be used. Following is the workflow definition with SET_VARIABLE task. { \"name\": \"Set_Variable_Workflow\", \"description\": \"Set a value to a variable and then reuse it later in the workflow\", \"version\": 1, \"tasks\": [ { \"name\": \"Set_Name\", \"taskReferenceName\": \"Set_Name\", \"type\": \"SET_VARIABLE\", \"inputParameters\": { \"name\": \"Orkes\" } }, { \"name\": \"Read_Name\", \"taskReferenceName\": \"Read_Name\", \"inputParameters\": { \"var_name\" : \"${workflow.variables.name}\" }, \"type\": \"SIMPLE\" } ], \"restartable\": true, \"ownerEmail\":\"abc@example.com\", \"workflowStatusListenerEnabled\": true, \"schemaVersion\": 2 } In the above example, it can be seen that the task Set_Name is a Set Variable Task and the variable name is set to Orkes and later in the workflow it is referenced by \"${workflow.variables.name}\" in another task.","title":"Set Variable"},{"location":"reference-docs/set-variable-task/#set-variable","text":"\"type\" : \"SET_VARIABLE\"","title":"Set Variable"},{"location":"reference-docs/set-variable-task/#introduction","text":"Set Variable allows us to set workflow variables by creating or updating them with new values.","title":"Introduction"},{"location":"reference-docs/set-variable-task/#use-cases","text":"Variables can be initialized in the workflow definition as well as during the workflow run. Once a variable was initialized it can be read or overwritten with a new value by any other task.","title":"Use Cases"},{"location":"reference-docs/set-variable-task/#configuration","text":"Set Variable task is defined directly inside the workflow with type SET_VARIABLE .","title":"Configuration"},{"location":"reference-docs/set-variable-task/#examples","text":"Suppose in a workflow, we have to store a value in a variable and then later in workflow reuse the value stored in the variable just as we do in programming, in such scenarios Set Variable task can be used. Following is the workflow definition with SET_VARIABLE task. { \"name\": \"Set_Variable_Workflow\", \"description\": \"Set a value to a variable and then reuse it later in the workflow\", \"version\": 1, \"tasks\": [ { \"name\": \"Set_Name\", \"taskReferenceName\": \"Set_Name\", \"type\": \"SET_VARIABLE\", \"inputParameters\": { \"name\": \"Orkes\" } }, { \"name\": \"Read_Name\", \"taskReferenceName\": \"Read_Name\", \"inputParameters\": { \"var_name\" : \"${workflow.variables.name}\" }, \"type\": \"SIMPLE\" } ], \"restartable\": true, \"ownerEmail\":\"abc@example.com\", \"workflowStatusListenerEnabled\": true, \"schemaVersion\": 2 } In the above example, it can be seen that the task Set_Name is a Set Variable Task and the variable name is set to Orkes and later in the workflow it is referenced by \"${workflow.variables.name}\" in another task.","title":"Examples"},{"location":"reference-docs/start-workflow-task/","text":"Start Workflow \u00b6 \"type\" : \"START_WORKFLOW\" Introduction \u00b6 Start Workflow starts another workflow. Unlike SUB_WORKFLOW , START_WORKFLOW does not create a relationship between starter and the started workflow. It also does not wait for the started workflow to complete. A START_WORKFLOW is considered successful once the requested workflow is started successfully. In other words, START_WORKFLOW is marked as COMPLETED, once the started workflow is in RUNNING state. Use Cases \u00b6 When another workflow needs to be started from a workflow, START_WORKFLOW can be used. Configuration \u00b6 Start Workflow task is defined directly inside the workflow with type START_WORKFLOW . Input \u00b6 Parameters: name type description startWorkflow Map[String, Any] The value of this parameter is Start Workflow Request . Output \u00b6 name type description workflowId String The id of the started workflow","title":"Start Workflow"},{"location":"reference-docs/start-workflow-task/#start-workflow","text":"\"type\" : \"START_WORKFLOW\"","title":"Start Workflow"},{"location":"reference-docs/start-workflow-task/#introduction","text":"Start Workflow starts another workflow. Unlike SUB_WORKFLOW , START_WORKFLOW does not create a relationship between starter and the started workflow. It also does not wait for the started workflow to complete. A START_WORKFLOW is considered successful once the requested workflow is started successfully. In other words, START_WORKFLOW is marked as COMPLETED, once the started workflow is in RUNNING state.","title":"Introduction"},{"location":"reference-docs/start-workflow-task/#use-cases","text":"When another workflow needs to be started from a workflow, START_WORKFLOW can be used.","title":"Use Cases"},{"location":"reference-docs/start-workflow-task/#configuration","text":"Start Workflow task is defined directly inside the workflow with type START_WORKFLOW .","title":"Configuration"},{"location":"reference-docs/start-workflow-task/#input","text":"Parameters: name type description startWorkflow Map[String, Any] The value of this parameter is Start Workflow Request .","title":"Input"},{"location":"reference-docs/start-workflow-task/#output","text":"name type description workflowId String The id of the started workflow","title":"Output"},{"location":"reference-docs/sub-workflow-task/","text":"Sub Workflow \u00b6 \"type\" : \"SUB_WORKFLOW\" Introduction \u00b6 Sub Workflow task allows for nesting a workflow within another workflow. Nested workflows contain a reference to their parent. Use Cases \u00b6 Suppose we want to include another workflow inside our current workflow. In that case, Sub Workflow Task would be used. Configuration \u00b6 Sub Workflow task is defined directly inside the workflow with type SUB_WORKFLOW . Input \u00b6 Parameters: name type description subWorkflowParam Map[String, Any] See below subWorkflowParam name type description name String Name of the workflow to execute version Integer Version of the workflow to execute taskToDomain Map[String, String] Allows scheduling the sub workflow's tasks per given mappings. See Task Domains for instructions to configure taskDomains. workflowDefinition WorkflowDefinition Allows starting a subworkflow with a dynamic workflow definition. Output \u00b6 name type description subWorkflowId String Sub-workflow execution Id generated when running the sub-workflow Examples \u00b6 Imagine we have a workflow that has a fork in it. In the example below, we input one image, but using a fork to create 2 images simultaneously: The left fork will create a JPG, and the right fork a WEBP image. Maintaining this workflow might be difficult, as changes made to one side of the fork do not automatically propagate the other. Rather than using 2 tasks, we can define a image_convert_resize workflow that we can call for both forks as a sub-workflow: {{ \"name\": \"image_convert_resize_subworkflow1\", \"description\": \"Image Processing Workflow\", \"version\": 1, \"tasks\": [{ \"name\": \"image_convert_resize_multipleformat_fork\", \"taskReferenceName\": \"image_convert_resize_multipleformat_ref\", \"inputParameters\": {}, \"type\": \"FORK_JOIN\", \"decisionCases\": {}, \"defaultCase\": [], \"forkTasks\": [ [{ \"name\": \"image_convert_resize_sub\", \"taskReferenceName\": \"subworkflow_jpg_ref\", \"inputParameters\": { \"fileLocation\": \"${workflow.input.fileLocation}\", \"recipeParameters\": { \"outputSize\": { \"width\": \"${workflow.input.recipeParameters.outputSize.width}\", \"height\": \"${workflow.input.recipeParameters.outputSize.height}\" }, \"outputFormat\": \"jpg\" } }, \"type\": \"SUB_WORKFLOW\", \"subWorkflowParam\": { \"name\": \"image_convert_resize\", \"version\": 1 } }], [{ \"name\": \"image_convert_resize_sub\", \"taskReferenceName\": \"subworkflow_webp_ref\", \"inputParameters\": { \"fileLocation\": \"${workflow.input.fileLocation}\", \"recipeParameters\": { \"outputSize\": { \"width\": \"${workflow.input.recipeParameters.outputSize.width}\", \"height\": \"${workflow.input.recipeParameters.outputSize.height}\" }, \"outputFormat\": \"webp\" } }, \"type\": \"SUB_WORKFLOW\", \"subWorkflowParam\": { \"name\": \"image_convert_resize\", \"version\": 1 } } ] ] }, { \"name\": \"image_convert_resize_multipleformat_join\", \"taskReferenceName\": \"image_convert_resize_multipleformat_join_ref\", \"inputParameters\": {}, \"type\": \"JOIN\", \"decisionCases\": {}, \"defaultCase\": [], \"forkTasks\": [], \"startDelay\": 0, \"joinOn\": [ \"subworkflow_jpg_ref\", \"upload_toS3_webp_ref\" ], \"optional\": false, \"defaultExclusiveJoinTask\": [], \"asyncComplete\": false, \"loopOver\": [] } ], \"inputParameters\": [], \"outputParameters\": { \"fileLocationJpg\": \"${subworkflow_jpg_ref.output.fileLocation}\", \"fileLocationWebp\": \"${subworkflow_webp_ref.output.fileLocation}\" }, \"schemaVersion\": 2, \"restartable\": true, \"workflowStatusListenerEnabled\": true, \"ownerEmail\": \"devrel@orkes.io\", \"timeoutPolicy\": \"ALERT_ONLY\", \"timeoutSeconds\": 0, \"variables\": {}, \"inputTemplate\": {} } Now our diagram will appear as: The inputs to both sides of the workflow are identical before and after - but we've abstracted the tasks into the sub-workflow. Any change to the sub-workflow will automatically occur in bth sides of the fork. Looking at the subworkflow (the WEBP version): { \"name\": \"image_convert_resize_sub\", \"taskReferenceName\": \"subworkflow_webp_ref\", \"inputParameters\": { \"fileLocation\": \"${workflow.input.fileLocation}\", \"recipeParameters\": { \"outputSize\": { \"width\": \"${workflow.input.recipeParameters.outputSize.width}\", \"height\": \"${workflow.input.recipeParameters.outputSize.height}\" }, \"outputFormat\": \"webp\" } }, \"type\": \"SUB_WORKFLOW\", \"subWorkflowParam\": { \"name\": \"image_convert_resize\", \"version\": 1 } } The subWorkflowParam tells conductor which workflow to call. The task is marked as completed upon the completion of the spawned workflow. If the sub-workflow is terminated or fails the task is marked as failure and retried if configured.","title":"Sub Workflow"},{"location":"reference-docs/sub-workflow-task/#sub-workflow","text":"\"type\" : \"SUB_WORKFLOW\"","title":"Sub Workflow"},{"location":"reference-docs/sub-workflow-task/#introduction","text":"Sub Workflow task allows for nesting a workflow within another workflow. Nested workflows contain a reference to their parent.","title":"Introduction"},{"location":"reference-docs/sub-workflow-task/#use-cases","text":"Suppose we want to include another workflow inside our current workflow. In that case, Sub Workflow Task would be used.","title":"Use Cases"},{"location":"reference-docs/sub-workflow-task/#configuration","text":"Sub Workflow task is defined directly inside the workflow with type SUB_WORKFLOW .","title":"Configuration"},{"location":"reference-docs/sub-workflow-task/#input","text":"Parameters: name type description subWorkflowParam Map[String, Any] See below subWorkflowParam name type description name String Name of the workflow to execute version Integer Version of the workflow to execute taskToDomain Map[String, String] Allows scheduling the sub workflow's tasks per given mappings. See Task Domains for instructions to configure taskDomains. workflowDefinition WorkflowDefinition Allows starting a subworkflow with a dynamic workflow definition.","title":"Input"},{"location":"reference-docs/sub-workflow-task/#output","text":"name type description subWorkflowId String Sub-workflow execution Id generated when running the sub-workflow","title":"Output"},{"location":"reference-docs/sub-workflow-task/#examples","text":"Imagine we have a workflow that has a fork in it. In the example below, we input one image, but using a fork to create 2 images simultaneously: The left fork will create a JPG, and the right fork a WEBP image. Maintaining this workflow might be difficult, as changes made to one side of the fork do not automatically propagate the other. Rather than using 2 tasks, we can define a image_convert_resize workflow that we can call for both forks as a sub-workflow: {{ \"name\": \"image_convert_resize_subworkflow1\", \"description\": \"Image Processing Workflow\", \"version\": 1, \"tasks\": [{ \"name\": \"image_convert_resize_multipleformat_fork\", \"taskReferenceName\": \"image_convert_resize_multipleformat_ref\", \"inputParameters\": {}, \"type\": \"FORK_JOIN\", \"decisionCases\": {}, \"defaultCase\": [], \"forkTasks\": [ [{ \"name\": \"image_convert_resize_sub\", \"taskReferenceName\": \"subworkflow_jpg_ref\", \"inputParameters\": { \"fileLocation\": \"${workflow.input.fileLocation}\", \"recipeParameters\": { \"outputSize\": { \"width\": \"${workflow.input.recipeParameters.outputSize.width}\", \"height\": \"${workflow.input.recipeParameters.outputSize.height}\" }, \"outputFormat\": \"jpg\" } }, \"type\": \"SUB_WORKFLOW\", \"subWorkflowParam\": { \"name\": \"image_convert_resize\", \"version\": 1 } }], [{ \"name\": \"image_convert_resize_sub\", \"taskReferenceName\": \"subworkflow_webp_ref\", \"inputParameters\": { \"fileLocation\": \"${workflow.input.fileLocation}\", \"recipeParameters\": { \"outputSize\": { \"width\": \"${workflow.input.recipeParameters.outputSize.width}\", \"height\": \"${workflow.input.recipeParameters.outputSize.height}\" }, \"outputFormat\": \"webp\" } }, \"type\": \"SUB_WORKFLOW\", \"subWorkflowParam\": { \"name\": \"image_convert_resize\", \"version\": 1 } } ] ] }, { \"name\": \"image_convert_resize_multipleformat_join\", \"taskReferenceName\": \"image_convert_resize_multipleformat_join_ref\", \"inputParameters\": {}, \"type\": \"JOIN\", \"decisionCases\": {}, \"defaultCase\": [], \"forkTasks\": [], \"startDelay\": 0, \"joinOn\": [ \"subworkflow_jpg_ref\", \"upload_toS3_webp_ref\" ], \"optional\": false, \"defaultExclusiveJoinTask\": [], \"asyncComplete\": false, \"loopOver\": [] } ], \"inputParameters\": [], \"outputParameters\": { \"fileLocationJpg\": \"${subworkflow_jpg_ref.output.fileLocation}\", \"fileLocationWebp\": \"${subworkflow_webp_ref.output.fileLocation}\" }, \"schemaVersion\": 2, \"restartable\": true, \"workflowStatusListenerEnabled\": true, \"ownerEmail\": \"devrel@orkes.io\", \"timeoutPolicy\": \"ALERT_ONLY\", \"timeoutSeconds\": 0, \"variables\": {}, \"inputTemplate\": {} } Now our diagram will appear as: The inputs to both sides of the workflow are identical before and after - but we've abstracted the tasks into the sub-workflow. Any change to the sub-workflow will automatically occur in bth sides of the fork. Looking at the subworkflow (the WEBP version): { \"name\": \"image_convert_resize_sub\", \"taskReferenceName\": \"subworkflow_webp_ref\", \"inputParameters\": { \"fileLocation\": \"${workflow.input.fileLocation}\", \"recipeParameters\": { \"outputSize\": { \"width\": \"${workflow.input.recipeParameters.outputSize.width}\", \"height\": \"${workflow.input.recipeParameters.outputSize.height}\" }, \"outputFormat\": \"webp\" } }, \"type\": \"SUB_WORKFLOW\", \"subWorkflowParam\": { \"name\": \"image_convert_resize\", \"version\": 1 } } The subWorkflowParam tells conductor which workflow to call. The task is marked as completed upon the completion of the spawned workflow. If the sub-workflow is terminated or fails the task is marked as failure and retried if configured.","title":"Examples"},{"location":"reference-docs/switch-task/","text":"Switch \u00b6 \"type\" : \"SWITCH\" Introduction \u00b6 A switch task is similar to case...switch statement in a programming language. The switch expression, is a configuration on the SWITCH task type. Currently, two evaluators are supported: Configuration \u00b6 Following are the task configuration parameters : name type description evaluatorType String evaluatortType values expression String Depends on the evaluatortType value decisionCases Map[String, List[task]] Map where key is possible values that can result from expression being evaluated by evaluatorType with value being list of tasks to be executed. defaultCase List[task] List of tasks to be executed when no matching value if found in decision case (default condition) Evaluator Types \u00b6 name description expression value-param Use a parameter directly as the value input parameter javascript Evaluate JavaScript expressions and compute value JavaScript expression Use Cases \u00b6 Useful in any situation where we have to execute one of many task options. Output \u00b6 Following is/are output generated by the Switch Task. name type description evaluationResult List[String] A List of string representing the list of cases that matched. Examples \u00b6 In this example workflow, we have to ship a package with the shipping service providers on the basis of input provided while running the workflow. Let's create a Workflow with the following switch task definition that uses value-param evaluatorType: { \"name\": \"switch_task\", \"taskReferenceName\": \"switch_task\", \"inputParameters\": { \"switchCaseValue\": \"${workflow.input.service}\" }, \"type\": \"SWITCH\", \"evaluatorType\": \"value-param\", \"expression\": \"switchCaseValue\", \"defaultCase\": [ { ... } ], \"decisionCases\": { \"fedex\": [ { ... } ], \"ups\": [ { ... } ] } } In the definition above the value of the parameter switch_case_value is used to determine the switch-case. The evaluator type is value-param and the expression is a direct reference to the name of an input parameter. If the value of switch_case_value is fedex then the decision case ship_via_fedex is executed as shown below. In a similar way - if the input was ups , then ship_via_ups will be executed. If none of the cases match then the default option is executed. Here is an example using the javascript evaluator type: { \"name\": \"switch_task\", \"taskReferenceName\": \"switch_task\", \"inputParameters\": { \"inputValue\": \"${workflow.input.service}\" }, \"type\": \"SWITCH\", \"evaluatorType\": \"javascript\", \"expression\": \"$.inputValue == 'fedex' ? 'fedex' : 'ups'\", \"defaultCase\": [ { ... } ], \"decisionCases\": { \"fedex\": [ { ... } ], \"ups\": [ { ... } ] } }","title":"Switch"},{"location":"reference-docs/switch-task/#switch","text":"\"type\" : \"SWITCH\"","title":"Switch"},{"location":"reference-docs/switch-task/#introduction","text":"A switch task is similar to case...switch statement in a programming language. The switch expression, is a configuration on the SWITCH task type. Currently, two evaluators are supported:","title":"Introduction"},{"location":"reference-docs/switch-task/#configuration","text":"Following are the task configuration parameters : name type description evaluatorType String evaluatortType values expression String Depends on the evaluatortType value decisionCases Map[String, List[task]] Map where key is possible values that can result from expression being evaluated by evaluatorType with value being list of tasks to be executed. defaultCase List[task] List of tasks to be executed when no matching value if found in decision case (default condition)","title":"Configuration"},{"location":"reference-docs/switch-task/#evaluator-types","text":"name description expression value-param Use a parameter directly as the value input parameter javascript Evaluate JavaScript expressions and compute value JavaScript expression","title":"Evaluator Types"},{"location":"reference-docs/switch-task/#use-cases","text":"Useful in any situation where we have to execute one of many task options.","title":"Use Cases"},{"location":"reference-docs/switch-task/#output","text":"Following is/are output generated by the Switch Task. name type description evaluationResult List[String] A List of string representing the list of cases that matched.","title":"Output"},{"location":"reference-docs/switch-task/#examples","text":"In this example workflow, we have to ship a package with the shipping service providers on the basis of input provided while running the workflow. Let's create a Workflow with the following switch task definition that uses value-param evaluatorType: { \"name\": \"switch_task\", \"taskReferenceName\": \"switch_task\", \"inputParameters\": { \"switchCaseValue\": \"${workflow.input.service}\" }, \"type\": \"SWITCH\", \"evaluatorType\": \"value-param\", \"expression\": \"switchCaseValue\", \"defaultCase\": [ { ... } ], \"decisionCases\": { \"fedex\": [ { ... } ], \"ups\": [ { ... } ] } } In the definition above the value of the parameter switch_case_value is used to determine the switch-case. The evaluator type is value-param and the expression is a direct reference to the name of an input parameter. If the value of switch_case_value is fedex then the decision case ship_via_fedex is executed as shown below. In a similar way - if the input was ups , then ship_via_ups will be executed. If none of the cases match then the default option is executed. Here is an example using the javascript evaluator type: { \"name\": \"switch_task\", \"taskReferenceName\": \"switch_task\", \"inputParameters\": { \"inputValue\": \"${workflow.input.service}\" }, \"type\": \"SWITCH\", \"evaluatorType\": \"javascript\", \"expression\": \"$.inputValue == 'fedex' ? 'fedex' : 'ups'\", \"defaultCase\": [ { ... } ], \"decisionCases\": { \"fedex\": [ { ... } ], \"ups\": [ { ... } ] } }","title":"Examples"},{"location":"reference-docs/terminate-task/","text":"Terminate \u00b6 \"type\" : \"TERMINATE\" Introduction \u00b6 Task that can terminate a workflow with a given status and modify the workflow's output with a given parameter, it can act as a return statement for conditions where you simply want to terminate your workflow. Use Cases \u00b6 Use it when you want to terminate the workflow without continuing the execution. For example, if you have a decision where the first condition is met, you want to execute some tasks, otherwise you want to finish your workflow. Configuration \u00b6 Terminate task is defined directly inside the workflow with type TERMINATE . { \"name\": \"terminate\", \"taskReferenceName\": \"terminate0\", \"inputParameters\": { \"terminationStatus\": \"COMPLETED\", \"workflowOutput\": \"${task0.output}\" }, \"type\": \"TERMINATE\", \"startDelay\": 0, \"optional\": false } Inputs \u00b6 Parameters: name type description notes terminationStatus String can only accept \"COMPLETED\" or \"FAILED\" task cannot be optional workflowOutput Any Expected workflow output Output \u00b6 Outputs: name type description output Map The content of workflowOutput from the inputParameters. An empty object if workflowOutput is not set. Examples \u00b6 Let's consider the same example we had in Switch Task . Suppose in a workflow, we have to take decision to ship the courier with the shipping service providers on the basis of input provided while running the workflow. If the input provided while running workflow does not match with the available shipping providers then the workflow will fail and return. If input provided matches then it goes ahead. Here is a snippet that shows the defalt switch case terminating the workflow: { \"name\": \"switch_task\", \"taskReferenceName\": \"switch_task\", \"type\": \"SWITCH\", \"defaultCase\": [ { \"name\": \"terminate\", \"taskReferenceName\": \"terminate\", \"type\": \"TERMINATE\", \"inputParameters\": { \"terminationStatus\": \"FAILED\" } } ] } Workflow gets created as shown in the diagram. Best Practices \u00b6 Include termination reason when terminating the workflow with failure status to make it easy to understand the cause. Include any additional details (e.g. output of the tasks, switch case etc) that helps understand the path taken to termination.","title":"Terminate"},{"location":"reference-docs/terminate-task/#terminate","text":"\"type\" : \"TERMINATE\"","title":"Terminate"},{"location":"reference-docs/terminate-task/#introduction","text":"Task that can terminate a workflow with a given status and modify the workflow's output with a given parameter, it can act as a return statement for conditions where you simply want to terminate your workflow.","title":"Introduction"},{"location":"reference-docs/terminate-task/#use-cases","text":"Use it when you want to terminate the workflow without continuing the execution. For example, if you have a decision where the first condition is met, you want to execute some tasks, otherwise you want to finish your workflow.","title":"Use Cases"},{"location":"reference-docs/terminate-task/#configuration","text":"Terminate task is defined directly inside the workflow with type TERMINATE . { \"name\": \"terminate\", \"taskReferenceName\": \"terminate0\", \"inputParameters\": { \"terminationStatus\": \"COMPLETED\", \"workflowOutput\": \"${task0.output}\" }, \"type\": \"TERMINATE\", \"startDelay\": 0, \"optional\": false }","title":"Configuration"},{"location":"reference-docs/terminate-task/#inputs","text":"Parameters: name type description notes terminationStatus String can only accept \"COMPLETED\" or \"FAILED\" task cannot be optional workflowOutput Any Expected workflow output","title":"Inputs"},{"location":"reference-docs/terminate-task/#output","text":"Outputs: name type description output Map The content of workflowOutput from the inputParameters. An empty object if workflowOutput is not set.","title":"Output"},{"location":"reference-docs/terminate-task/#examples","text":"Let's consider the same example we had in Switch Task . Suppose in a workflow, we have to take decision to ship the courier with the shipping service providers on the basis of input provided while running the workflow. If the input provided while running workflow does not match with the available shipping providers then the workflow will fail and return. If input provided matches then it goes ahead. Here is a snippet that shows the defalt switch case terminating the workflow: { \"name\": \"switch_task\", \"taskReferenceName\": \"switch_task\", \"type\": \"SWITCH\", \"defaultCase\": [ { \"name\": \"terminate\", \"taskReferenceName\": \"terminate\", \"type\": \"TERMINATE\", \"inputParameters\": { \"terminationStatus\": \"FAILED\" } } ] } Workflow gets created as shown in the diagram.","title":"Examples"},{"location":"reference-docs/terminate-task/#best-practices","text":"Include termination reason when terminating the workflow with failure status to make it easy to understand the cause. Include any additional details (e.g. output of the tasks, switch case etc) that helps understand the path taken to termination.","title":"Best Practices"},{"location":"reference-docs/wait-task/","text":"Wait \u00b6 \"type\" : \"WAIT\" Introduction \u00b6 WAIT is used when the workflow needs to be paused for an external signal to continue. Use Cases \u00b6 WAIT is used when the workflow needs to wait and pause for an external signal such as a human intervention (like manual approval) or an event coming from external source such as Kafka, SQS or Conductor's internal queueing mechanism. Some use cases where WAIT task is used: 1. To add a human approval task. When the task is approved/rejected by human WAIT task is updated using POST /tasks API to completion. 2. To wait for and external signal coming from an event queue mechanism supported by Conductor Configuration \u00b6 taskType: WAIT There are no other configurations required Ending a WAIT \u00b6 To conclude a WAIT task, there are three endpoints that can be used. You'll need the workflowId , taskRefName or taskId and the task status (generally COMPLETED or FAILED ). POST /api/tasks POST api/queue/update/{workflowId}/{taskRefName}/{status} POST api/queue/update/{workflowId}/task/{taskId}/{status}","title":"Wait"},{"location":"reference-docs/wait-task/#wait","text":"\"type\" : \"WAIT\"","title":"Wait"},{"location":"reference-docs/wait-task/#introduction","text":"WAIT is used when the workflow needs to be paused for an external signal to continue.","title":"Introduction"},{"location":"reference-docs/wait-task/#use-cases","text":"WAIT is used when the workflow needs to wait and pause for an external signal such as a human intervention (like manual approval) or an event coming from external source such as Kafka, SQS or Conductor's internal queueing mechanism. Some use cases where WAIT task is used: 1. To add a human approval task. When the task is approved/rejected by human WAIT task is updated using POST /tasks API to completion. 2. To wait for and external signal coming from an event queue mechanism supported by Conductor","title":"Use Cases"},{"location":"reference-docs/wait-task/#configuration","text":"taskType: WAIT There are no other configurations required","title":"Configuration"},{"location":"reference-docs/wait-task/#ending-a-wait","text":"To conclude a WAIT task, there are three endpoints that can be used. You'll need the workflowId , taskRefName or taskId and the task status (generally COMPLETED or FAILED ). POST /api/tasks POST api/queue/update/{workflowId}/{taskRefName}/{status} POST api/queue/update/{workflowId}/task/{taskId}/{status}","title":"Ending a WAIT"}]}